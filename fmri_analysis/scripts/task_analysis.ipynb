{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180504-09:42:49,331 duecredit ERROR:\n",
      "\t Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from inspect import currentframe, getframeinfo\n",
    "from pathlib import Path\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.algorithms.modelgen import SpecifyModel\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.pipeline.engine import Workflow, Node\n",
    "import os\n",
    "from os.path import join\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments\n",
    "These are not needed for the jupyter notebook, but are used after conversion to a script for production\n",
    "\n",
    "- conversion command:\n",
    "  - jupyter nbconvert --to script --execute task_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Example BIDS App entrypoint script.')\n",
    "parser.add_argument('-derivatives_dir', default='/derivatives')\n",
    "parser.add_argument('-data_dir', default='/data')\n",
    "parser.add_argument('--participant_labels',nargs=\"+\")\n",
    "parser.add_argument('--events_dir', default=None)\n",
    "parser.add_argument('--tasks', nargs=\"+\")\n",
    "parser.add_argument('--ignore_rt', action='store_true')\n",
    "parser.add_argument('--cleanup', action='store_true')\n",
    "if '-derivatives_dir' in sys.argv or '-h' in sys.argv:\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    args = parser.parse_args([])\n",
    "    args.derivatives_dir = '/mnt/OAK/derivatives/'\n",
    "    args.data_dir = '/mnt/OAK'\n",
    "    args.tasks = ['ANT']\n",
    "    args.participant_labels = ['s130']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get current directory to pass to function nodes\n",
    "filename = getframeinfo(currentframe()).filename\n",
    "current_directory = str(Path(filename).resolve().parent)\n",
    "\n",
    "# list of subject identifiers\n",
    "subject_list = args.participant_labels\n",
    "# list of task identifiers\n",
    "if args.tasks is not None:\n",
    "    task_list = args.tasks\n",
    "else:\n",
    "    task_list = ['ANT', 'CCTHot', 'discountFix',\n",
    "               'DPX', 'motorSelectiveStop',\n",
    "               'stopSignal', 'stroop', 'surveyMedley',\n",
    "               'twoByTwo', 'WATT3']\n",
    "\n",
    "regress_rt = not args.ignore_rt\n",
    "#### Experiment Variables\n",
    "derivatives_dir = args.derivatives_dir\n",
    "fmriprep_dir = join(derivatives_dir, 'fmriprep', 'fmriprep')\n",
    "data_dir = args.data_dir\n",
    "first_level_dir = '1stLevel'\n",
    "working_dir = 'workingdir'\n",
    "# TR of functional images\n",
    "TR = .68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Task List: ['ANT']\n",
      ", Subjects: ['s130']\n",
      ", derivatives_dir: /mnt/OAK/derivatives/\n",
      ", data_dir: /mnt/OAK\n",
      "*******************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print('*'*79)\n",
    "print('Task List: %s\\n, Subjects: %s\\n, derivatives_dir: %s\\n, data_dir: %s' % \n",
    "     (task_list, subject_list, derivatives_dir, data_dir))\n",
    "print('*'*79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function to create bunch\n",
    "def getsubjectinfo(data_dir, fmriprep_dir, subject_id, task, regress_rt, utils_path): \n",
    "    from glob import glob\n",
    "    from os.path import join\n",
    "    import pandas as pd\n",
    "    from nipype.interfaces.base import Bunch\n",
    "    import sys\n",
    "    sys.path.append(utils_path)\n",
    "    from utils.event_utils import get_contrasts, parse_EVs, process_confounds\n",
    "    # strip \"sub\" from beginning of subject_id if provided\n",
    "    subject_id = subject_id.replace('sub-','')\n",
    "    ## Get the Confounds File (output of fmriprep)\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    confounds_file = glob(join(fmriprep_dir,\n",
    "                               'sub-%s' % subject_id,\n",
    "                               '*', 'func',\n",
    "                               '*%s*confounds.tsv' % task))[0]\n",
    "    regressors, regressor_names = process_confounds(confounds_file)\n",
    "    ## Get the Events File if it exists\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    event_file = glob(join(data_dir,\n",
    "                           'sub-%s' % subject_id,\n",
    "                           '*', 'func',\n",
    "                           '*%s*events.tsv' % task))\n",
    "    # create base_subject info\n",
    "    base_subjectinfo = Bunch(subject_id=subject_id,\n",
    "                             task=task,\n",
    "                             tmod=None,\n",
    "                             pmod=None,\n",
    "                             regressor_names=regressor_names,\n",
    "                             regressors=regressors.T.tolist())\n",
    "    if len(event_file)>0:\n",
    "        event_file = event_file[0]\n",
    "        events_df = pd.read_csv(event_file,sep = '\\t')\n",
    "        # set up contrasts\n",
    "        EV_dict = parse_EVs(events_df, task, regress_rt)\n",
    "        contrast_subjectinfo = Bunch(subject_id=subject_id,\n",
    "                                     task=task,\n",
    "                                     conditions=EV_dict['conditions'],\n",
    "                                     onsets=EV_dict['onsets'],\n",
    "                                     durations=EV_dict['durations'],\n",
    "                                     amplitudes=EV_dict['amplitudes'],\n",
    "                                     tmod=None,\n",
    "                                     pmod=None,\n",
    "                                     regressor_names=regressor_names,\n",
    "                                     regressors=regressors.T.tolist())\n",
    "        contrasts = get_contrasts(task, regress_rt)\n",
    "        return base_subjectinfo, contrast_subjectinfo,  contrasts \n",
    "    else:\n",
    "        return base_subjectinfo, None, None\n",
    "    \n",
    "def save_subjectinfo(base_directory,base_subjectinfo, contrast_subjectinfo, contrasts=[]):\n",
    "    from os import makedirs\n",
    "    from os.path import join\n",
    "    import pickle\n",
    "    subject_id = base_subjectinfo.subject_id\n",
    "    task = base_subjectinfo.task\n",
    "    task_dir = join(base_directory, 'subject_info', subject_id + '_task_' + task)\n",
    "    makedirs(task_dir, exist_ok=True)\n",
    "    # save base subject info\n",
    "    subjectinfo_path = join(task_dir,'base_subjectinfo.pkl')\n",
    "    pickle.dump(base_subjectinfo, open(subjectinfo_path,'wb'))\n",
    "    # save contrast subject info\n",
    "    if len(contrast_subjectinfo.items()) > 0:\n",
    "        subjectinfo_path = join(task_dir,'contrast_subjectinfo.pkl')\n",
    "        pickle.dump(contrast_subjectinfo, open(subjectinfo_path,'wb'))\n",
    "    if len(contrasts) > 0:\n",
    "        contrast_path = join(task_dir,'contrasts.pkl')\n",
    "        pickle.dump(contrasts, open(contrast_path,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View one events file used in subject info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Input and Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_subjectinfo(name):\n",
    "    # Get Subject Info - get subject specific condition information\n",
    "    subjectinfo = Node(Function(input_names=['data_dir', 'fmriprep_dir','subject_id', \n",
    "                                             'task','regress_rt', 'utils_path'],\n",
    "                                   output_names=['base_subjectinfo', \n",
    "                                                 'contrast_subjectinfo',\n",
    "                                                 'contrasts'],\n",
    "                                   function=getsubjectinfo),\n",
    "                          name=name)\n",
    "    subjectinfo.inputs.fmriprep_dir = fmriprep_dir\n",
    "    subjectinfo.inputs.data_dir = data_dir\n",
    "    subjectinfo.inputs.regress_rt = regress_rt\n",
    "    subjectinfo.inputs.utils_path = current_directory\n",
    "    return subjectinfo\n",
    "\n",
    "def get_savesubjectinfo(name):\n",
    "    # Save python objects that aren't accomodated by datasink nodes\n",
    "    savesubjectinfo = Node(Function(input_names=['base_directory',\n",
    "                                                  'base_subjectinfo',\n",
    "                                                  'contrast_subjectinfo','contrasts'],\n",
    "                                     output_names=['output_path'],\n",
    "                                    function=save_subjectinfo),\n",
    "                           name=name)\n",
    "    savesubjectinfo.inputs.base_directory = join(derivatives_dir,first_level_dir)\n",
    "    return savesubjectinfo\n",
    "\n",
    "def get_selector(name, session=None):\n",
    "    if session is None:\n",
    "        ses = '*'\n",
    "    else:\n",
    "        ses = 'ses-%s' % str(session)\n",
    "    # SelectFiles - to grab the data (alternative to DataGrabber)\n",
    "    templates = {'func': join('*{subject_id}',ses,'func',\n",
    "                             '*{task}*MNI*preproc.nii.gz'),\n",
    "                'mask': join('*{subject_id}',ses,'func',\n",
    "                             '*{task}*MNI*brainmask.nii.gz')}\n",
    "    selectfiles = Node(SelectFiles(templates,\n",
    "                                   base_directory=fmriprep_dir,\n",
    "                                   sort_filelist=True),\n",
    "                       name=name)\n",
    "    return selectfiles\n",
    "\n",
    "def get_masker(name):\n",
    "    # mask and blur\n",
    "    return Node(fsl.maths.ApplyMask(),name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_GLM_wf(name='wf'):\n",
    "    # Datasink - creates output folder for important outputs\n",
    "    datasink = Node(DataSink(base_directory=derivatives_dir,\n",
    "                             container=first_level_dir),\n",
    "                    name=\"datasink\")\n",
    "    # Use the following DataSink output substitutions\n",
    "    substitutions = [('_subject_id_', ''),\n",
    "                    ('fstat', 'FSTST'),\n",
    "                    ('run0.mat', 'designfile.mat')]\n",
    "    datasink.inputs.substitutions = substitutions\n",
    "    \n",
    "    # SpecifyModel - Generates FSL-specific Model\n",
    "    modelspec = Node(SpecifyModel(input_units='secs',\n",
    "                                  time_repetition=TR,\n",
    "                                  high_pass_filter_cutoff=80),\n",
    "                     name=\"modelspec\")\n",
    "    # Level1Design - Creates FSL config file \n",
    "    level1design = Node(fsl.Level1Design(bases={'dgamma':{'derivs': True}},\n",
    "                                         interscan_interval=TR,\n",
    "                                         model_serial_correlations=True),\n",
    "                            name=\"level1design\")\n",
    "    # FEATmodel generates an FSL design matrix\n",
    "    level1model = Node(fsl.FEATModel(), name=\"FEATModel\")\n",
    "\n",
    "    # FILMGLs\n",
    "    # smooth_autocorr, check default, use FSL default\n",
    "    filmgls = Node(fsl.FILMGLS(), name=\"GLS\")\n",
    "\n",
    "    wf = Workflow(name=name)\n",
    "    wf.connect([(modelspec, level1design, [('session_info','session_info')]),\n",
    "              (level1design, level1model, [('ev_files', 'ev_files'),\n",
    "                                             ('fsf_files','fsf_file')]),\n",
    "              (level1model, filmgls, [('design_file', 'design_file'),\n",
    "                                        ('con_file', 'tcon_file'),\n",
    "                                        ('fcon_file', 'fcon_file')]),\n",
    "              (level1model, datasink, [('design_file', '%s.@design_file' % name)]),\n",
    "              (filmgls, datasink, [('copes', '%s.@copes' % name),\n",
    "                                    ('zstats', '%s.@Z' % name),\n",
    "                                    ('fstats', '%s.@F' % name),\n",
    "                                    ('tstats','%s.@T' % name),\n",
    "                                    ('param_estimates','%s.@param_estimates' % name),\n",
    "                                    ('residual4d', '%s.@residual4d' % name),\n",
    "                                    ('sigmasquareds', '%s.@sigmasquareds' % name)])])\n",
    "    return wf\n",
    "\n",
    "\n",
    "def init_common_wf(workflow, task):\n",
    "        # Infosource - a function free node to iterate over the list of subject names\n",
    "    infosource = Node(IdentityInterface(fields=['subject_id',\n",
    "                                                'task']),\n",
    "                      name=\"%s_infosource\" % task)\n",
    "    infosource.iterables = [('subject_id', subject_list)]\n",
    "    infosource.inputs.task = task\n",
    "    \n",
    "    # initiate basic nodes\n",
    "    subjectinfo = get_subjectinfo('%s_subjectinfo' % task)\n",
    "    savesubjectinfo = get_savesubjectinfo('%s_savesubjectinfo' % task)\n",
    "    masker = get_masker('%s_masker' % task)\n",
    "    selectfiles = get_selector('%s_selectFiles' % task)\n",
    "    \n",
    "    # Connect up the 1st-level analysis components\n",
    "    workflow.connect([(infosource, selectfiles, [('subject_id', 'subject_id'),\n",
    "                                                   ('task', 'task')]),\n",
    "                        (infosource, subjectinfo, [('subject_id','subject_id'),\n",
    "                                                     ('task', 'task')]),\n",
    "                        (subjectinfo, savesubjectinfo, [('base_subjectinfo','base_subjectinfo'),\n",
    "                                                         ('contrast_subjectinfo','contrast_subjectinfo'),\n",
    "                                                         ('contrasts','contrasts')]),\n",
    "                        (selectfiles, masker, [('func','in_file'),\n",
    "                                               ('mask', 'mask_file')])\n",
    "                        ])\n",
    "\n",
    "def get_task_wfs(task):\n",
    "    # set up workflow lookup\n",
    "    wf_dict = {'rest': [(init_GLM_wf, {'name': '%s_base_wf' % task})]}\n",
    "    \n",
    "    default_wf = [(init_GLM_wf, {'name': '%s_contrast_wf' % task}), \n",
    "                  (init_GLM_wf, {'name': '%s_base_wf' % task})]\n",
    "    \n",
    "    # get workflow\n",
    "    workflows = []\n",
    "    for func, kwargs in wf_dict.get(task, default_wf):\n",
    "        workflows.append(func(**kwargs))\n",
    "    return workflows\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiation of the 1st-level analysis workflow\n",
    "l1analysis = Workflow(name='l1analysis')\n",
    "l1analysis.base_dir = join(derivatives_dir, working_dir)\n",
    "\n",
    "for task in task_list:\n",
    "    init_common_wf(l1analysis, task)\n",
    "    task_workflows = get_task_wfs(task)\n",
    "    # get nodes to pass\n",
    "    subjectinfo = l1analysis.get_node('%s_subjectinfo' % task)\n",
    "    masker = l1analysis.get_node('%s_masker' % task)\n",
    "    for wf in task_workflows:\n",
    "        l1analysis.connect([\n",
    "                            (subjectinfo, wf, [('contrast_subjectinfo','modelspec.subject_info')]),\n",
    "                            (masker, wf, [('out_file', 'modelspec.functional_runs')]),\n",
    "                            (masker, wf, [('out_file','GLS.in_file')])\n",
    "                            ])\n",
    "        \n",
    "        if 'contrast' in wf.name:\n",
    "            l1analysis.connect([(subjectinfo, wf, [('contrasts','level1design.contrasts')])])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180504-09:42:49,870 workflow INFO:\n",
      "\t Workflow l1analysis settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "180504-09:42:50,121 workflow INFO:\n",
      "\t Running serially.\n",
      "180504-09:42:50,124 workflow INFO:\n",
      "\t Executing node l1analysis.ANT_subjectinfo in dir: /mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130/ANT_subjectinfo\n",
      "180504-09:42:50,448 workflow INFO:\n",
      "\t Running node \"ANT_subjectinfo\" (\"nipype.interfaces.utility.wrappers.Function\").\n",
      "180504-09:42:51,710 workflow INFO:\n",
      "\t Executing node l1analysis.ANT_savesubjectinfo in dir: /mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130/ANT_savesubjectinfo\n",
      "180504-09:42:52,520 workflow INFO:\n",
      "\t Running node \"ANT_savesubjectinfo\" (\"nipype.interfaces.utility.wrappers.Function\").\n",
      "180504-09:42:53,28 workflow INFO:\n",
      "\t Executing node l1analysis.ANT_selectFiles in dir: /mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130/ANT_selectFiles\n",
      "180504-09:42:53,333 workflow INFO:\n",
      "\t Running node \"ANT_selectFiles\" (\"nipype.interfaces.io.SelectFiles\").\n",
      "180504-09:42:53,561 workflow INFO:\n",
      "\t Executing node l1analysis.ANT_masker in dir: /mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130/ANT_masker\n",
      "180504-09:42:53,595 workflow INFO:\n",
      "\t Collecting precomputed outputs\n",
      "180504-09:42:53,789 workflow INFO:\n",
      "\t Executing node l1analysis.ANT_contrast_wf.modelspec in dir: /mnt/OAK/derivatives/workingdir/l1analysis/ANT_contrast_wf/_subject_id_s130/modelspec\n",
      "180504-09:42:54,509 workflow INFO:\n",
      "\t Running node \"modelspec\" (\"nipype.algorithms.modelgen.SpecifyModel\").\n",
      "180504-09:42:55,856 workflow INFO:\n",
      "\t Executing node l1analysis.ANT_contrast_wf.level1design in dir: /mnt/OAK/derivatives/workingdir/l1analysis/ANT_contrast_wf/_subject_id_s130/level1design\n",
      "180504-09:42:56,567 workflow INFO:\n",
      "\t Running node \"level1design\" (\"nipype.interfaces.fsl.model.Level1Design\").\n",
      "180504-09:42:58,102 workflow INFO:\n",
      "\t Executing node l1analysis.ANT_contrast_wf.FEATModel in dir: /mnt/OAK/derivatives/workingdir/l1analysis/ANT_contrast_wf/_subject_id_s130/FEATModel\n",
      "180504-09:42:59,433 workflow INFO:\n",
      "\t Running node \"FEATModel\" (\"nipype.interfaces.fsl.model.FEATModel\"), a CommandLine Interface with command:\n",
      "feat_model run0 .\n",
      "180504-09:43:08,596 workflow INFO:\n",
      "\t Executing node l1analysis.ANT_contrast_wf.GLS in dir: /mnt/OAK/derivatives/workingdir/l1analysis/ANT_contrast_wf/_subject_id_s130/GLS\n",
      "180504-09:43:08,859 workflow INFO:\n",
      "\t Running node \"GLS\" (\"nipype.interfaces.fsl.model.FILMGLS\"), a CommandLine Interface with command:\n",
      "film_gls --rn=results --con=/mnt/OAK/derivatives/workingdir/l1analysis/ANT_contrast_wf/_subject_id_s130/FEATModel/run0.con --in=/mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130/ANT_masker/sub-s130_ses-2_task-ANT_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz --pd=/mnt/OAK/derivatives/workingdir/l1analysis/ANT_contrast_wf/_subject_id_s130/FEATModel/run0.mat --thr=0.000000.\n",
      "180504-09:43:08,985 interface INFO:\n",
      "\t stdout 2018-05-04T09:43:08.985551:Log directory is: results\n",
      "180504-09:43:48,469 workflow ERROR:\n",
      "\t Node GLS.a0 failed to run on host ian-System-Product-Name.\n",
      "180504-09:43:48,472 workflow ERROR:\n",
      "\t Saving crash info to /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180504-094348-ian-GLS.a0-8b8c2bf5-8061-4ca4-af1f-65624d14297f.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/linear.py\", line 43, in run\n",
      "    node.run(updatehash=updatehash)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1089, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1693, in _run_interface\n",
      "    runtime = run_command(runtime, output=self.terminal_output)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1417, in run_command\n",
      "    _process()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1404, in _process\n",
      "    res = select.select(streams, [], [], timeout)\n",
      "KeyboardInterrupt\n",
      "\n",
      "180504-09:43:48,548 workflow INFO:\n",
      "\t Executing node l1analysis.ANT_base_wf.modelspec in dir: /mnt/OAK/derivatives/workingdir/l1analysis/ANT_base_wf/_subject_id_s130/modelspec\n",
      "180504-09:43:49,227 workflow INFO:\n",
      "\t Running node \"modelspec\" (\"nipype.algorithms.modelgen.SpecifyModel\").\n",
      "180504-09:43:50,525 workflow INFO:\n",
      "\t Executing node l1analysis.ANT_base_wf.level1design in dir: /mnt/OAK/derivatives/workingdir/l1analysis/ANT_base_wf/_subject_id_s130/level1design\n",
      "180504-09:43:51,282 workflow INFO:\n",
      "\t Running node \"level1design\" (\"nipype.interfaces.fsl.model.Level1Design\").\n",
      "180504-09:43:52,719 workflow INFO:\n",
      "\t Executing node l1analysis.ANT_base_wf.FEATModel in dir: /mnt/OAK/derivatives/workingdir/l1analysis/ANT_base_wf/_subject_id_s130/FEATModel\n",
      "180504-09:43:54,9 workflow INFO:\n",
      "\t Running node \"FEATModel\" (\"nipype.interfaces.fsl.model.FEATModel\"), a CommandLine Interface with command:\n",
      "feat_model run0 .\n"
     ]
    }
   ],
   "source": [
    "l1analysis.run()\n",
    "#l1analysis.run('MultiProc', plugin_args={'n_procs': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "243px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
