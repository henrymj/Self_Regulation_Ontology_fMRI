{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from inspect import currentframe, getframeinfo\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.algorithms.modelgen import SpecifyModel\n",
    "from nipype.interfaces.base import Bunch\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.pipeline.engine import Workflow, Node\n",
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "from utils.event_utils import get_beta_series, get_contrasts, parse_EVs, process_confounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments\n",
    "These are not needed for the jupyter notebook, but are used after conversion to a script for production\n",
    "\n",
    "- conversion command:\n",
    "  - jupyter nbconvert --to script --execute task_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Example BIDS App entrypoint script.')\n",
    "parser.add_argument('-derivatives_dir', default='/derivatives')\n",
    "parser.add_argument('-data_dir', default='/data')\n",
    "parser.add_argument('--participant_label')\n",
    "parser.add_argument('--tasks', nargs=\"+\")\n",
    "parser.add_argument('--cleanup', action='store_true')\n",
    "parser.add_argument('--n_procs', default=16)\n",
    "if '-derivatives_dir' in sys.argv or '-h' in sys.argv:\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    args = parser.parse_args([])\n",
    "    args.derivatives_dir = '/mnt/OAK/derivatives'\n",
    "    args.data_dir = '/mnt/OAK'\n",
    "    args.tasks = ['stroop']\n",
    "    args.participant_label = 's130'\n",
    "    args.n_procs=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get current directory to pass to function nodes\n",
    "filename = getframeinfo(currentframe()).filename\n",
    "current_directory = str(Path(filename).resolve().parent)\n",
    "\n",
    "# list of subject identifiers\n",
    "subject_id = args.participant_label\n",
    "# list of task identifiers\n",
    "if args.tasks is not None:\n",
    "    task_list = args.tasks\n",
    "else:\n",
    "    task_list = ['ANT', 'CCTHot', 'discountFix',\n",
    "               'DPX', 'motorSelectiveStop',\n",
    "               'stopSignal', 'stroop', 'surveyMedley',\n",
    "               'twoByTwo', 'WATT3']\n",
    "\n",
    "#### Experiment Variables\n",
    "derivatives_dir = args.derivatives_dir\n",
    "fmriprep_dir = join(derivatives_dir, 'fmriprep', 'fmriprep')\n",
    "data_dir = args.data_dir\n",
    "first_level_dir = join(derivatives_dir,'1stLevel')\n",
    "working_dir = 'workingdir'\n",
    "n_procs = args.n_procs\n",
    "# TR of functional images\n",
    "TR = .68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print\n",
    "print('*'*79)\n",
    "print('Task List: %s\\n, Subject: %s\\n, derivatives_dir: %s\\n, data_dir: %s' % \n",
    "     (task_list, subject_id, derivatives_dir, data_dir))\n",
    "print('*'*79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_events_regressors(data_dir, fmirprep_dir, subject_id, task):\n",
    "    # strip \"sub\" from beginning of subject_id if provided\n",
    "    subject_id = subject_id.replace('sub-','')\n",
    "    ## Get the Confounds File (output of fmriprep)\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    confounds_file = glob(join(fmriprep_dir,\n",
    "                               'sub-%s' % subject_id,\n",
    "                               '*', 'func',\n",
    "                               '*%s*confounds.tsv' % task))[0]\n",
    "    regressors, regressor_names = process_confounds(confounds_file)\n",
    "    ## Get the Events File if it exists\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    event_file = glob(join(data_dir,\n",
    "                           'sub-%s' % subject_id,\n",
    "                           '*', 'func',\n",
    "                           '*%s*events.tsv' % task))   \n",
    "    if len(event_file)>0:\n",
    "        # set up events file\n",
    "        event_file = event_file[0]\n",
    "        events_df = pd.read_csv(event_file,sep = '\\t')\n",
    "    else:\n",
    "        events_df = None\n",
    "    regressors, regressor_names = process_confounds(confounds_file)\n",
    "    return events_df, regressors, regressor_names\n",
    "\n",
    "# helper function to create bunch\n",
    "def getsubjectinfo(events_dr, regressors, regressor_names, task='beta', regress_rt=True): \n",
    "    EV_dict = parse_EVs(events_df, task, regress_rt)\n",
    "    if task not in ['beta']:\n",
    "        contrasts = get_contrasts(task, regress_rt)\n",
    "    else:\n",
    "        contrasts=None\n",
    "    # create beta series info\n",
    "    subjectinfo = Bunch(conditions=EV_dict['conditions'],\n",
    "                        onsets=EV_dict['onsets'],\n",
    "                        durations=EV_dict['durations'],\n",
    "                        amplitudes=EV_dict['amplitudes'],\n",
    "                        tmod=None,\n",
    "                        pmod=None,\n",
    "                        regressor_names=regressor_names,\n",
    "                        regressors=regressors.T.tolist(),\n",
    "                        contrasts=contrasts)\n",
    "    return subjectinfo\n",
    "    \n",
    "def save_subjectinfo(save_directory, subjectinfo):\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    subjectinfo_path = join(save_directory, 'subjectinfo.pkl')\n",
    "    pickle.dump(subjectinfo, open(subjectinfo_path,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Input and Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_selector(task, subject_id, session=None):\n",
    "    if session is None:\n",
    "        ses = '*'\n",
    "    else:\n",
    "        ses = 'ses-%s' % str(session)\n",
    "    # SelectFiles - to grab the data (alternative to DataGrabber)\n",
    "    templates = {'func': join('sub-{subject_id}',ses,'func',\n",
    "                             '*{task}*MNI*preproc.nii.gz'),\n",
    "                 'mask': join('sub-{subject_id}',ses,'func',\n",
    "                              '*{task}*MNI*brainmask.nii.gz')}\n",
    "    selectfiles = Node(SelectFiles(templates,\n",
    "                                   base_directory=fmriprep_dir,\n",
    "                                   sort_filelist=True),\n",
    "                       name='%s_selectFiles' % task)\n",
    "    selectfiles.inputs.task = task\n",
    "    selectfiles.inputs.subject_id = subject_id\n",
    "    return selectfiles\n",
    "\n",
    "def get_masker(name):\n",
    "    # mask and blur\n",
    "    return Node(fsl.maths.ApplyMask(),name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_common_wf(workflow, task):\n",
    "    # initiate basic nodes\n",
    "    masker = get_masker('%s_masker' % task)\n",
    "    selectfiles = get_selector(task, subject_id)\n",
    "    # Connect up the 1st-level analysis components\n",
    "    workflow.connect([(selectfiles, masker, [('func','in_file'), ('mask', 'mask_file')])])\n",
    "\n",
    "def init_GLM_wf(subject_info, name='wf-standard', contrasts=None):\n",
    "    # Datasink - creates output folder for important outputs\n",
    "    datasink = Node(DataSink(base_directory=first_level_dir,\n",
    "                             container=subject_id), name=\"datasink\")\n",
    "    # Use the following DataSink output substitutions\n",
    "    substitutions = [('_subject_id_', ''),\n",
    "                    ('fstat', 'FSTST'),\n",
    "                    ('run0.mat', 'designfile.mat')]\n",
    "    \n",
    "    datasink.inputs.substitutions = substitutions\n",
    "    # ridiculous regexp substitution to get files just right\n",
    "    # link to ridiculousness: https://regex101.com/r/ljS5zK/2\n",
    "    match_str = \"(?P<sub>s[0-9]+)\\/(?P<task>[a-z_]+)_(?P<model>model-[a-z]+)_(?P<submodel>wf-[a-z_]+)\\/s[0-9]+\"\n",
    "    replace_str = \"\\g<sub>/\\g<task>/\\g<model>/\\g<submodel>\"\n",
    "    regexp_substitutions = [(match_str, replace_str)]\n",
    "    datasink.inputs.regexp_substitutions = regexp_substitutions\n",
    "    \n",
    "    # SpecifyModel - Generates FSL-specific Model\n",
    "    modelspec = Node(SpecifyModel(input_units='secs',\n",
    "                                  time_repetition=TR,\n",
    "                                  high_pass_filter_cutoff=80),\n",
    "                     name=\"modelspec\")\n",
    "    modelspec.inputs.subject_info = subject_info\n",
    "    # Level1Design - Creates FSL config file \n",
    "    level1design = Node(fsl.Level1Design(bases={'dgamma':{'derivs': True}},\n",
    "                                         interscan_interval=TR,\n",
    "                                         model_serial_correlations=True),\n",
    "                            name=\"level1design\")\n",
    "    if subject_info.contrasts is not None:\n",
    "        level1design.inputs.contrasts=subject_info.contrasts\n",
    "    # FEATmodel generates an FSL design matrix\n",
    "    level1model = Node(fsl.FEATModel(), name=\"FEATModel\")\n",
    "\n",
    "    # FILMGLs\n",
    "    # smooth_autocorr, check default, use FSL default\n",
    "    filmgls = Node(fsl.FILMGLS(), name=\"GLS\")\n",
    "\n",
    "    wf = Workflow(name=name)\n",
    "    wf.connect([(modelspec, level1design, [('session_info','session_info')]),\n",
    "                (level1design, level1model, [('ev_files', 'ev_files'),\n",
    "                                             ('fsf_files','fsf_file')]),\n",
    "                (level1model, datasink, [('design_file', '%s.@design_file' % name)]),\n",
    "                (level1model, filmgls, [('design_file', 'design_file'),\n",
    "                                        ('con_file', 'tcon_file'),\n",
    "                                        ('fcon_file', 'fcon_file')]),\n",
    "                (filmgls, datasink, [('copes', '%s.@copes' % name),\n",
    "                                     ('zstats', '%s.@Z' % name),\n",
    "                                     ('fstats', '%s.@F' % name),\n",
    "                                     ('tstats','%s.@T' % name),\n",
    "                                     ('param_estimates','%s.@param_estimates' % name),\n",
    "                                     ('residual4d', '%s.@residual4d' % name),\n",
    "                                     ('sigmasquareds', '%s.@sigmasquareds' % name)])\n",
    "               ])\n",
    "    return wf\n",
    "\n",
    "\n",
    "\n",
    "def get_task_wfs(task, beta_subjectinfo=None, contrast_subjectinfo=None, regress_rt=True):\n",
    "    rt_suffix = 'RT' if regress_rt else 'noRT'\n",
    "    # set up workflow lookup\n",
    "    wf_dict = {'contrast': (init_GLM_wf, {'name': '%s_model-%s_wf-contrast' % (task, rt_suffix)}), \n",
    "                'beta': (init_GLM_wf, {'name': '%s_model-%s_wf-beta' % (task, rt_suffix)})}\n",
    "    \n",
    "    workflows = []\n",
    "    if beta_subjectinfo:\n",
    "        save_directory = join(first_level_dir, subject_id, task, 'model-%s' % rt_suffix, 'wf-beta')\n",
    "        save_subjectinfo(save_directory, beta_subjectinfo)\n",
    "        func, kwargs = wf_dict['beta']\n",
    "        workflows.append(func(beta_subjectinfo, **kwargs))\n",
    "    if contrast_subjectinfo:\n",
    "        save_directory = join(first_level_dir, subject_id, task, 'model-%s' % rt_suffix, 'wf-contrast')\n",
    "        save_subjectinfo(save_directory, contrast_subjectinfo)\n",
    "        func, kwargs = wf_dict['contrast']\n",
    "        workflows.append(func(contrast_subjectinfo, **kwargs))\n",
    "    return workflows\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiation of the 1st-level analysis workflow\n",
    "l1analysis = Workflow(name='l1analysis')\n",
    "l1analysis.base_dir = join(derivatives_dir, working_dir)\n",
    "\n",
    "for task in task_list:\n",
    "    init_common_wf(l1analysis, task)\n",
    "    # get nodes to pass\n",
    "    masker = l1analysis.get_node('%s_masker' % task)\n",
    "    # get info to pass to task workflows\n",
    "    events_df, regressors, regressor_names = get_events_regressors(data_dir, fmriprep_dir,\n",
    "                                                                   subject_id, task)\n",
    "    # perform analyses both by regressing rt and not\n",
    "    regress_rt_conditions = [True, False]\n",
    "    if 'stop' in task:\n",
    "        regress_rt_conditions = ['False']\n",
    "    for regress_rt in regress_rt_conditions:\n",
    "        betainfo = getsubjectinfo(events_df, regressors, regressor_names, task='beta', regress_rt=regress_rt)\n",
    "        contrastinfo = getsubjectinfo(events_df, regressors, regressor_names, task=task, regress_rt=regress_rt)\n",
    "        task_workflows = get_task_wfs(task, betainfo, contrastinfo, regress_rt)\n",
    "        for wf in task_workflows:\n",
    "            l1analysis.connect([\n",
    "                                (masker, wf, [('out_file', 'modelspec.functional_runs')]),\n",
    "                                (masker, wf, [('out_file','GLS.in_file')])\n",
    "                                ])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#l1analysis.run()\n",
    "l1analysis.run('MultiProc', plugin_args={'n_procs': n_procs})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "243px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
