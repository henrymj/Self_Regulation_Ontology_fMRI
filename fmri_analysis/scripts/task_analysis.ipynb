{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180501-15:34:24,18 duecredit ERROR:\n",
      "\t Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from inspect import currentframe, getframeinfo\n",
    "from pathlib import Path\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.algorithms.modelgen import SpecifyModel\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.pipeline.engine import Workflow, Node\n",
    "import os\n",
    "from os.path import join\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments\n",
    "These are not needed for the jupyter notebook, but are used after conversion to a script for production\n",
    "\n",
    "- conversion command:\n",
    "  - jupyter nbconvert --to script --execute task_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Example BIDS App entrypoint script.')\n",
    "parser.add_argument('-derivatives_dir', default='/output')\n",
    "parser.add_argument('-data_dir', default='/data')\n",
    "parser.add_argument('--participant_labels',nargs=\"+\")\n",
    "parser.add_argument('--events_dir', default=None)\n",
    "parser.add_argument('--tasks', nargs=\"+\")\n",
    "parser.add_argument('--use_events', action='store_false')\n",
    "parser.add_argument('--ignore_rt', action='store_true')\n",
    "parser.add_argument('--cleanup', action='store_true')\n",
    "parser.add_argument('--overwrite_event', action='store_true')\n",
    "if '-derivatives_dir' in sys.argv:\n",
    "    args = parser.parse_args()\n",
    "    args.derivatives_dir = '/mnt/OAK/derivatives/'\n",
    "    args.data_dir = '/mnt/OAK'\n",
    "    args.tasks = ['stroop']\n",
    "    args.participant_labels = ['s130']\n",
    "else:\n",
    "    args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get current directory to pass to function nodes\n",
    "filename = getframeinfo(currentframe()).filename\n",
    "current_directory = str(Path(filename).resolve().parent)\n",
    "\n",
    "# list of subject identifiers\n",
    "subject_list = args.participant_labels\n",
    "# list of task identifiers\n",
    "if args.tasks is not None:\n",
    "    task_list = args.tasks\n",
    "else:\n",
    "    task_list = ['ANT', 'CCTHot', 'discountFix',\n",
    "               'DPX', 'motorSelectiveStop',\n",
    "               'stopSignal', 'stroop', 'surveyMedley',\n",
    "               'twoByTwo', 'WATT3']\n",
    "\n",
    "regress_rt = not args.ignore_rt\n",
    "#### Experiment Variables\n",
    "derivatives_dir = args.derivatives_dir\n",
    "fmriprep_dir = join(derivatives_dir, 'fmriprep', 'fmriprep')\n",
    "data_dir = args.data_dir\n",
    "first_level_dir = '1stLevel'\n",
    "working_dir = 'workingdir'\n",
    "# TR of functional images\n",
    "TR = .68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Task List: ['stroop']\n",
      ", Subjects: ['s130']\n",
      ", derivatives_dir: /mnt/OAK/derivatives/\n",
      ", data_dir: /mnt/OAK\n",
      "*******************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print('*'*79)\n",
    "print('Task List: %s\\n, Subjects: %s\\n, derivatives_dir: %s\\n, data_dir: %s' % \n",
    "     (task_list, subject_list, derivatives_dir, data_dir))\n",
    "print('*'*79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function to create bunch\n",
    "def getsubjectinfo(data_dir, fmriprep_dir, subject_id, task, regress_rt, utils_path): \n",
    "    from glob import glob\n",
    "    from os.path import join\n",
    "    import pandas as pd\n",
    "    from nipype.interfaces.base import Bunch\n",
    "    import sys\n",
    "    sys.path.append(utils_path)\n",
    "    from utils.event_utils import get_contrasts, parse_EVs, process_confounds\n",
    "    # strip \"sub\" from beginning of subject_id if provided\n",
    "    subject_id = subject_id.replace('sub-','')\n",
    "    ## Get the Confounds File (output of fmriprep)\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    confounds_file = glob(join(fmriprep_dir,\n",
    "                               'sub-%s' % subject_id,\n",
    "                               '*', 'func',\n",
    "                               '*%s*confounds.tsv' % task))[0]\n",
    "    regressors, regressor_names = process_confounds(confounds_file)\n",
    "    ## Get the Events File if it exists\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    event_file = glob(join(data_dir,\n",
    "                           'sub-%s' % subject_id,\n",
    "                           '*', 'func',\n",
    "                           '*%s*events.tsv' % task))\n",
    "    if len(event_file)>0:\n",
    "        event_file = event_file[0]\n",
    "        events_df = pd.read_csv(event_file,sep = '\\t')\n",
    "        # set up contrasts\n",
    "        EV_dict = parse_EVs(events_df, task, regress_rt)\n",
    "        contrast_subjectinfo = Bunch(subject_id=subject_id,\n",
    "                                      task=task,\n",
    "                                      conditions=EV_dict['conditions'],\n",
    "                                      onsets=EV_dict['onsets'],\n",
    "                                      durations=EV_dict['durations'],\n",
    "                                      amplitudes=EV_dict['amplitudes'],\n",
    "                                      tmod=None,\n",
    "                                      pmod=None,\n",
    "                                      regressor_names=regressor_names,\n",
    "                                      regressors=regressors.T.tolist())\n",
    "        contrasts = get_contrasts(task, regress_rt)\n",
    "    else:\n",
    "        contrast_subjectinfo=Bunch()\n",
    "        contrasts=None\n",
    "    # create base_subject info\n",
    "    base_subjectinfo = Bunch(subject_id=subject_id,\n",
    "                             task=task,\n",
    "                             tmod=None,\n",
    "                             pmod=None,\n",
    "                             regressor_names=regressor_names,\n",
    "                             regressors=regressors.T.tolist())\n",
    "\n",
    "    return contrast_subjectinfo, base_subjectinfo, contrasts # this output will later be returned to infosource\n",
    "\n",
    "def save_subjectinfo(base_directory,base_subjectinfo, contrast_subjectinfo, contrasts=[]):\n",
    "    from os import makedirs\n",
    "    from os.path import join\n",
    "    import pickle\n",
    "    subject_id = base_subjectinfo.subject_id\n",
    "    task = base_subjectinfo.task\n",
    "    task_dir = join(base_directory, 'subject_info', subject_id + '_task_' + task)\n",
    "    makedirs(task_dir, exist_ok=True)\n",
    "    # save base subject info\n",
    "    subjectinfo_path = join(task_dir,'base_subjectinfo.pkl')\n",
    "    pickle.dump(base_subjectinfo, open(subjectinfo_path,'wb'))\n",
    "    # save contrast subject info\n",
    "    if len(contrast_subjectinfo.items()) > 0:\n",
    "        subjectinfo_path = join(task_dir,'contrast_subjectinfo.pkl')\n",
    "        pickle.dump(contrast_subjectinfo, open(subjectinfo_path,'wb'))\n",
    "    if len(contrasts) > 0:\n",
    "        contrast_path = join(task_dir,'contrasts.pkl')\n",
    "        pickle.dump(contrasts, open(contrast_path,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View one events file used in subject info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Input and Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get Subject Info - get subject specific condition information\n",
    "subjectinfo = Node(Function(input_names=['data_dir', 'fmriprep_dir','subject_id', \n",
    "                                         'task','regress_rt', 'utils_path'],\n",
    "                               output_names=['contrast_subjectinfo', \n",
    "                                             'base_subjectinfo',\n",
    "                                             'contrasts'],\n",
    "                               function=getsubjectinfo),\n",
    "                      name='getsubjectinfo')\n",
    "subjectinfo.inputs.fmriprep_dir = fmriprep_dir\n",
    "subjectinfo.inputs.data_dir = data_dir\n",
    "subjectinfo.inputs.regress_rt = regress_rt\n",
    "subjectinfo.inputs.utils_path = current_directory\n",
    "\n",
    "\n",
    "# SelectFiles - to grab the data (alternative to DataGrabber)\n",
    "templates = {'func': join('*{subject_id}','*','func',\n",
    "                         '*{task}*MNI*preproc.nii.gz'),\n",
    "            'mask': join('*{subject_id}','*','func',\n",
    "                         '*{task}*MNI*brainmask.nii.gz')}\n",
    "selectfiles = Node(SelectFiles(templates,\n",
    "                               base_directory=fmriprep_dir,\n",
    "                               sort_filelist=True),\n",
    "                   name=\"selectfiles\")\n",
    "# Datasink - creates output folder for important outputs\n",
    "datasink = Node(DataSink(base_directory=derivatives_dir,\n",
    "                         container=first_level_dir),\n",
    "                name=\"datasink\")\n",
    "# Save python objects that aren't accomodated by datasink nodes\n",
    "save_subjectinfo = Node(Function(input_names=['base_directory',\n",
    "                                              'base_subjectinfo',\n",
    "                                              'contrast_subjectinfo','contrasts'],\n",
    "                                 output_names=['output_path'],\n",
    "                                function=save_subjectinfo),\n",
    "                       name=\"savesubjectinfo\")\n",
    "save_subjectinfo.inputs.base_directory = join(derivatives_dir,first_level_dir)\n",
    "\n",
    "# Use the following DataSink output substitutions\n",
    "substitutions = [('_subject_id_', ''),\n",
    "                ('fstat', 'FSTST'),\n",
    "                ('run0.mat', 'designfile.mat')]\n",
    "datasink.inputs.substitutions = substitutions\n",
    "\n",
    "# mask and blur\n",
    "masker = Node(fsl.maths.ApplyMask(),name='masker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wf(name='wf'):\n",
    "    # SpecifyModel - Generates FSL-specific Model\n",
    "    modelspec = Node(SpecifyModel(input_units='secs',\n",
    "                                  time_repetition=TR,\n",
    "                                  high_pass_filter_cutoff=80),\n",
    "                     name=\"modelspec\")\n",
    "    # Level1Design - Creates FSL config file \n",
    "    level1design = Node(fsl.Level1Design(bases={'dgamma':{'derivs': True}},\n",
    "                                         interscan_interval=TR,\n",
    "                                         model_serial_correlations=True),\n",
    "                            name=\"level1design\")\n",
    "    # FEATmodel generates an FSL design matrix\n",
    "    level1model = Node(fsl.FEATModel(), name=\"FEATModel\")\n",
    "\n",
    "    # FILMGLs\n",
    "    # smooth_autocorr, check default, use FSL default\n",
    "    filmgls = Node(fsl.FILMGLS(), name=\"GLS\")\n",
    "\n",
    "    wf = Workflow(name=name)\n",
    "    wf.connect([(modelspec, level1design, [('session_info','session_info')]),\n",
    "              (level1design, level1model, [('ev_files', 'ev_files'),\n",
    "                                             ('fsf_files','fsf_file')]),\n",
    "              (level1model, filmgls, [('design_file', 'design_file'),\n",
    "                                        ('con_file', 'tcon_file'),\n",
    "                                        ('fcon_file', 'fcon_file')]),\n",
    "              (level1model, datasink, [('design_file', '%s.@design_file' % name)]),\n",
    "              (filmgls, datasink, [('copes', '%s.@copes' % name),\n",
    "                                    ('zstats', '%s.@Z' % name),\n",
    "                                    ('fstats', '%s.@F' % name),\n",
    "                                    ('tstats','%s.@T' % name),\n",
    "                                    ('param_estimates','%s.@param_estimates' % name),\n",
    "                                    ('residual4d', '%s.@residual4d' % name),\n",
    "                                    ('sigmasquareds', '%s.@sigmasquareds' % name)])])\n",
    "    return wf\n",
    "\n",
    "\n",
    "def get_wfs(task):\n",
    "    # set up workflow lookup\n",
    "    wf_dict = {'rest': [(init_wf, {'name': 'base_wf'})]}\n",
    "    \n",
    "    default_wf = [(init_wf, {'name': 'contrast_wf'}), \n",
    "                  (init_wf, {'name': 'base_wf'})]\n",
    "    \n",
    "    # get workflow\n",
    "    workflows = []\n",
    "    for func, kwargs in wf_dict.get(task, default_wf):\n",
    "        workflows.append(func(**kwargs))\n",
    "    return workflows\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiation of the 1st-level analysis workflow\n",
    "l1analysis = Workflow(name='l1analysis')\n",
    "l1analysis.base_dir = join(derivatives_dir, working_dir)\n",
    "\n",
    "for task in task_list:\n",
    "    # Infosource - a function free node to iterate over the list of subject names\n",
    "    infosource = Node(IdentityInterface(fields=['subject_id',\n",
    "                                                'task']),\n",
    "                      name=\"%s_infosource\" % task)\n",
    "    infosource.iterables = [('subject_id', subject_list)]\n",
    "    infosource.inputs.task = task\n",
    "    \n",
    "    # Connect up the 1st-level analysis components\n",
    "    l1analysis.connect([(infosource, selectfiles, [('subject_id', 'subject_id'),\n",
    "                                                   ('task', 'task')]),\n",
    "                        (infosource, subjectinfo, [('subject_id','subject_id'),\n",
    "                                                     ('task', 'task')]),\n",
    "                        (subjectinfo, save_subjectinfo, [('base_subjectinfo','base_subjectinfo'),\n",
    "                                                         ('contrast_subjectinfo','contrast_subjectinfo'),\n",
    "                                                         ('contrasts','contrasts')]),\n",
    "                        (selectfiles, masker, [('func','in_file'),\n",
    "                                               ('mask', 'mask_file')])\n",
    "                        ])\n",
    "    \n",
    "    workflows = get_wfs(task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow for calculating contrasts\n",
    "wf1 = init_wf(name='contrast_wf')\n",
    "# workflow for only removing nuisance variables\n",
    "wf2 = init_wf(name='base_wf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add on contrast wf\n",
    "l1analysis.connect([\n",
    "                    (subjectinfo, wf1, [('contrast_subjectinfo','modelspec.subject_info')]),\n",
    "                    (masker, wf1, [('out_file', 'modelspec.functional_runs')]),\n",
    "                    (subjectinfo, wf1, [('contrasts','level1design.contrasts')]),\n",
    "                    (masker, wf1, [('out_file','GLS.in_file')])\n",
    "                    ])\n",
    "# add on residual wf\n",
    "l1analysis.connect([\n",
    "                    (subjectinfo, wf2, [('base_subjectinfo','modelspec.subject_info')]),\n",
    "                    (masker, wf2, [('out_file', 'modelspec.functional_runs')]),\n",
    "                    (masker, wf2, [('out_file','GLS.in_file')])\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180501-15:44:23,933 workflow INFO:\n",
      "\t Workflow l1analysis settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "180501-15:44:24,44 workflow INFO:\n",
      "\t Running serially.\n",
      "180501-15:44:24,45 workflow INFO:\n",
      "\t Executing node l1analysis.getsubjectinfo in dir: /mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130_task_stroop/getsubjectinfo\n",
      "180501-15:44:24,140 workflow INFO:\n",
      "\t Running node \"getsubjectinfo\" (\"nipype.interfaces.utility.wrappers.Function\").\n",
      "180501-15:44:30,73 workflow INFO:\n",
      "\t Executing node l1analysis.savesubjectinfo in dir: /mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130_task_stroop/savesubjectinfo\n",
      "180501-15:44:30,302 workflow INFO:\n",
      "\t Running node \"savesubjectinfo\" (\"nipype.interfaces.utility.wrappers.Function\").\n",
      "180501-15:44:30,650 workflow INFO:\n",
      "\t Executing node l1analysis.selectfiles in dir: /mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130_task_stroop/selectfiles\n",
      "180501-15:44:30,694 workflow INFO:\n",
      "\t Running node \"selectfiles\" (\"nipype.interfaces.io.SelectFiles\").\n",
      "180501-15:44:34,664 workflow INFO:\n",
      "\t Executing node l1analysis.masker in dir: /mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130_task_stroop/masker\n",
      "180501-15:44:34,777 workflow INFO:\n",
      "\t Running node \"masker\" (\"nipype.interfaces.fsl.maths.ApplyMask\"), a CommandLine Interface with command:\n",
      "fslmaths /mnt/OAK/derivatives/fmriprep/fmriprep/sub-s130/ses-1/func/sub-s130_ses-1_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_preproc.nii.gz -mas /mnt/OAK/derivatives/fmriprep/fmriprep/sub-s130/ses-1/func/sub-s130_ses-1_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_brainmask.nii.gz /mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130_task_stroop/masker/sub-s130_ses-1_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz.\n",
      "180501-15:45:29,528 workflow INFO:\n",
      "\t Executing node l1analysis.contrast_wf.modelspec in dir: /mnt/OAK/derivatives/workingdir/l1analysis/contrast_wf/_subject_id_s130_task_stroop/modelspec\n",
      "180501-15:45:29,763 workflow INFO:\n",
      "\t Running node \"modelspec\" (\"nipype.algorithms.modelgen.SpecifyModel\").\n",
      "180501-15:45:30,163 workflow INFO:\n",
      "\t Executing node l1analysis.contrast_wf.level1design in dir: /mnt/OAK/derivatives/workingdir/l1analysis/contrast_wf/_subject_id_s130_task_stroop/level1design\n",
      "180501-15:45:30,485 workflow INFO:\n",
      "\t Running node \"level1design\" (\"nipype.interfaces.fsl.model.Level1Design\").\n",
      "180501-15:45:31,109 workflow INFO:\n",
      "\t Executing node l1analysis.contrast_wf.FEATModel in dir: /mnt/OAK/derivatives/workingdir/l1analysis/contrast_wf/_subject_id_s130_task_stroop/FEATModel\n",
      "180501-15:45:31,353 workflow INFO:\n",
      "\t Running node \"FEATModel\" (\"nipype.interfaces.fsl.model.FEATModel\"), a CommandLine Interface with command:\n",
      "feat_model run0 .\n",
      "180501-15:45:33,830 workflow INFO:\n",
      "\t Executing node l1analysis.contrast_wf.GLS in dir: /mnt/OAK/derivatives/workingdir/l1analysis/contrast_wf/_subject_id_s130_task_stroop/GLS\n",
      "180501-15:45:33,909 workflow INFO:\n",
      "\t Running node \"GLS\" (\"nipype.interfaces.fsl.model.FILMGLS\"), a CommandLine Interface with command:\n",
      "film_gls --rn=results --con=/mnt/OAK/derivatives/workingdir/l1analysis/contrast_wf/_subject_id_s130_task_stroop/FEATModel/run0.con --in=/mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130_task_stroop/masker/sub-s130_ses-1_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz --pd=/mnt/OAK/derivatives/workingdir/l1analysis/contrast_wf/_subject_id_s130_task_stroop/FEATModel/run0.mat --thr=0.000000.\n",
      "180501-15:45:34,272 interface INFO:\n",
      "\t stdout 2018-05-01T15:45:34.272722:Log directory is: results\n",
      "180501-15:45:51,850 interface INFO:\n",
      "\t stdout 2018-05-01T15:45:51.850647:paradigm.getDesignMatrix().Nrows()=339\n",
      "180501-15:45:51,853 interface INFO:\n",
      "\t stdout 2018-05-01T15:45:51.850647:paradigm.getDesignMatrix().Ncols()=26\n",
      "180501-15:45:51,855 interface INFO:\n",
      "\t stdout 2018-05-01T15:45:51.850647:sizeTS=339\n",
      "180501-15:45:51,856 interface INFO:\n",
      "\t stdout 2018-05-01T15:45:51.850647:numTS=199215\n",
      "180501-15:45:52,296 interface INFO:\n",
      "\t stdout 2018-05-01T15:45:52.296083:Calculating residuals...\n",
      "180501-16:01:43,320 workflow ERROR:\n",
      "\t Node GLS.a0 failed to run on host ian-ThinkPad-T450s.\n",
      "180501-16:01:43,326 workflow ERROR:\n",
      "\t Saving crash info to /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180501-160143-ian-GLS.a0-26cca272-32af-49bd-8f25-861cba380df1.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/linear.py\", line 43, in run\n",
      "    node.run(updatehash=updatehash)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1089, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1693, in _run_interface\n",
      "    runtime = run_command(runtime, output=self.terminal_output)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1417, in run_command\n",
      "    _process()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1404, in _process\n",
      "    res = select.select(streams, [], [], timeout)\n",
      "KeyboardInterrupt\n",
      "\n",
      "180501-16:01:43,331 workflow INFO:\n",
      "\t ***********************************\n",
      "180501-16:01:43,339 workflow ERROR:\n",
      "\t could not run node: l1analysis.contrast_wf.GLS.a0\n",
      "180501-16:01:43,342 workflow INFO:\n",
      "\t crashfile: /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180501-160143-ian-GLS.a0-26cca272-32af-49bd-8f25-861cba380df1.pklz\n",
      "180501-16:01:43,346 workflow INFO:\n",
      "\t ***********************************\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Workflow did not execute cleanly. Check log for details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9d08c61eb7a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml1analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#l1analysis.run('MultiProc', plugin_args={'n_procs': 8})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/workflows.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, plugin, plugin_args, updatehash)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'create_report'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_report_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdatehash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdatehash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mdatestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%dT%H%M%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'write_provenance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/linear.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, graph, config, updatehash)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_callback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exception'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mreport_nodes_not_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/tools.py\u001b[0m in \u001b[0;36mreport_nodes_not_run\u001b[0;34m(notrun)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***********************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         raise RuntimeError(('Workflow did not execute cleanly. '\n\u001b[0m\u001b[1;32m     80\u001b[0m                             'Check log for details'))\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Workflow did not execute cleanly. Check log for details"
     ]
    }
   ],
   "source": [
    "l1analysis.run()\n",
    "#l1analysis.run('MultiProc', plugin_args={'n_procs': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "243px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
