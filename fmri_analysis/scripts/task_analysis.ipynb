{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "180430-20:57:41,573 duecredit ERROR:\n",
      "\t Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from inspect import currentframe, getframeinfo\n",
    "from pathlib import Path\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.algorithms.modelgen import SpecifyModel\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.pipeline.engine import Workflow, Node\n",
    "import os\n",
    "from os.path import join\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments\n",
    "These are not needed for the jupyter notebook, but are used after conversion to a script for production\n",
    "\n",
    "- conversion command:\n",
    "  - jupyter nbconvert --to script --execute task_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Example BIDS App entrypoint script.')\n",
    "parser.add_argument('-derivatives_dir', default='/output')\n",
    "parser.add_argument('-data_dir', default='/data')\n",
    "parser.add_argument('--participant_labels',nargs=\"+\")\n",
    "parser.add_argument('--events_dir', default=None)\n",
    "parser.add_argument('--tasks', nargs=\"+\")\n",
    "parser.add_argument('--use_events', action='store_false')\n",
    "parser.add_argument('--ignore_rt', action='store_true')\n",
    "parser.add_argument('--cleanup', action='store_true')\n",
    "parser.add_argument('--overwrite_event', action='store_true')\n",
    "if '-derivatives_dir' in sys.argv:\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get current directory to pass to function nodes\n",
    "filename = getframeinfo(currentframe()).filename\n",
    "current_directory = str(Path(filename).resolve().parent)\n",
    "\n",
    "# list of subject identifiers\n",
    "subject_list = args.participant_labels\n",
    "# list of task identifiers\n",
    "if args.tasks is not None:\n",
    "    task_list = args.tasks\n",
    "else:\n",
    "    task_list = ['ANT', 'CCTHot', 'discountFix',\n",
    "               'DPX', 'motorSelectiveStop',\n",
    "               'stopSignal', 'stroop', 'surveyMedley',\n",
    "               'twoByTwo', 'WATT3']\n",
    "\n",
    "regress_rt = not args.ignore_rt\n",
    "#### Experiment Variables\n",
    "derivatives_dir = args.derivatives_dir\n",
    "fmriprep_dir = join(derivatives_dir, 'fmriprep', 'fmriprep')\n",
    "data_dir = args.data_dir\n",
    "first_level_dir = '1stLevel'\n",
    "working_dir = 'workingdir'\n",
    "# TR of functional images\n",
    "TR = .68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print\n",
    "print('*'*79)\n",
    "print('Task List: %s\\n, Subjects: %s\\n, derivatives_dir: %s\\n, data_dir: %s' % \n",
    "     (task_list, subject_list, derivatives_dir, data_dir))\n",
    "print('*'*79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function to create bunch\n",
    "def getsubjectinfo(data_dir, fmriprep_dir, subject_id, task, regress_rt, utils_path): \n",
    "    from glob import glob\n",
    "    from os.path import join\n",
    "    import pandas as pd\n",
    "    from nipype.interfaces.base import Bunch\n",
    "    import sys\n",
    "    sys.path.append(utils_path)\n",
    "    from utils.event_utils import get_contrasts, parse_EVs, process_confounds\n",
    "    # strip \"sub\" from beginning of subject_id if provided\n",
    "    subject_id = subject_id.replace('sub-','')\n",
    "    ## Get the Confounds File (output of fmriprep)\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    confounds_file = glob(join(fmriprep_dir,\n",
    "                               'sub-%s' % subject_id,\n",
    "                               '*', 'func',\n",
    "                               '*%s*confounds.tsv' % task))[0]\n",
    "    regressors, regressor_names = process_confounds(confounds_file)\n",
    "    ## Get the Events File if it exists\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    event_file = glob(join(data_dir,\n",
    "                           'sub-%s' % subject_id,\n",
    "                           '*', 'func',\n",
    "                           '*%s*events.tsv' % task))\n",
    "    if len(event_file)>0:\n",
    "        event_file = event_file[0]\n",
    "        events_df = pd.read_csv(event_file,sep = '\\t')\n",
    "        # set up contrasts\n",
    "        EV_dict = parse_EVs(events_df, task, regress_rt)\n",
    "        contrast_subjectinfo = Bunch(subject_id=subject_id,\n",
    "                                      task=task,\n",
    "                                      conditions=EV_dict['conditions'],\n",
    "                                      onsets=EV_dict['onsets'],\n",
    "                                      durations=EV_dict['durations'],\n",
    "                                      amplitudes=EV_dict['amplitudes'],\n",
    "                                      tmod=None,\n",
    "                                      pmod=None,\n",
    "                                      regressor_names=regressor_names,\n",
    "                                      regressors=regressors.T.tolist())\n",
    "        contrasts = get_contrasts(task, regress_rt)\n",
    "    else:\n",
    "        contrast_subjectinfo=Bunch()\n",
    "        contrasts=None\n",
    "    # create base_subject info\n",
    "    base_subjectinfo = Bunch(subject_id=subject_id,\n",
    "                             task=task,\n",
    "                             tmod=None,\n",
    "                             pmod=None,\n",
    "                             regressor_names=regressor_names,\n",
    "                             regressors=regressors.T.tolist())\n",
    "\n",
    "    return contrast_subjectinfo, base_subjectinfo, contrasts # this output will later be returned to infosource\n",
    "\n",
    "def save_subjectinfo(base_directory,base_subjectinfo, contrast_subjectinfo, contrasts=[]):\n",
    "    from os import makedirs\n",
    "    from os.path import join\n",
    "    import pickle\n",
    "    subject_id = base_subjectinfo.subject_id\n",
    "    task = base_subjectinfo.task\n",
    "    task_dir = join(base_directory, 'subject_info', subject_id + '_task_' + task)\n",
    "    makedirs(task_dir, exist_ok=True)\n",
    "    # save base subject info\n",
    "    subjectinfo_path = join(task_dir,'base_subjectinfo.pkl')\n",
    "    pickle.dump(base_subjectinfo, open(subjectinfo_path,'wb'))\n",
    "    # save contrast subject info\n",
    "    if len(contrast_subjectinfo.items()) > 0:\n",
    "        subjectinfo_path = join(task_dir,'contrast_subjectinfo.pkl')\n",
    "        pickle.dump(contrast_subjectinfo, open(subjectinfo_path,'wb'))\n",
    "    if len(contrasts) > 0:\n",
    "        contrast_path = join(task_dir,'contrasts.pkl')\n",
    "        pickle.dump(contrasts, open(contrast_path,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View one events file used in subject info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Input and Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get Subject Info - get subject specific condition information\n",
    "subjectinfo = Node(Function(input_names=['data_dir', 'fmriprep_dir','subject_id', \n",
    "                                         'task','regress_rt', 'utils_path'],\n",
    "                               output_names=['contrast_subjectinfo', \n",
    "                                             'base_subjectinfo',\n",
    "                                             'contrasts'],\n",
    "                               function=getsubjectinfo),\n",
    "                      name='getsubjectinfo')\n",
    "subjectinfo.inputs.fmriprep_dir = fmriprep_dir\n",
    "subjectinfo.inputs.data_dir = data_dir\n",
    "subjectinfo.inputs.regress_rt = regress_rt\n",
    "subjectinfo.inputs.utils_path = current_directory\n",
    "\n",
    "# Infosource - a function free node to iterate over the list of subject names\n",
    "infosource = Node(IdentityInterface(fields=['subject_id',\n",
    "                                            'task']),\n",
    "                  name=\"infosource\")\n",
    "infosource.iterables = [('subject_id', subject_list),\n",
    "                        ('task', task_list)]\n",
    "# SelectFiles - to grab the data (alternative to DataGrabber)\n",
    "templates = {'func': join('*{subject_id}','*','func',\n",
    "                         '*{task}*MNI*preproc.nii.gz'),\n",
    "            'mask': join('*{subject_id}','*','func',\n",
    "                         '*{task}*MNI*brainmask.nii.gz')}\n",
    "selectfiles = Node(SelectFiles(templates,\n",
    "                               base_directory=fmriprep_dir,\n",
    "                               sort_filelist=True),\n",
    "                   name=\"selectfiles\")\n",
    "# Datasink - creates output folder for important outputs\n",
    "datasink = Node(DataSink(base_directory=derivatives_dir,\n",
    "                         container=first_level_dir),\n",
    "                name=\"datasink\")\n",
    "# Save python objects that aren't accomodated by datasink nodes\n",
    "save_subjectinfo = Node(Function(input_names=['base_directory',\n",
    "                                              'base_subjectinfo',\n",
    "                                              'contrast_subjectinfo','contrasts'],\n",
    "                                 output_names=['output_path'],\n",
    "                                function=save_subjectinfo),\n",
    "                       name=\"savesubjectinfo\")\n",
    "save_subjectinfo.inputs.base_directory = join(derivatives_dir,first_level_dir)\n",
    "\n",
    "# Use the following DataSink output substitutions\n",
    "substitutions = [('_subject_id_', ''),\n",
    "                ('fstat', 'FSTST'),\n",
    "                ('run0.mat', 'designfile.mat')]\n",
    "datasink.inputs.substitutions = substitutions\n",
    "\n",
    "# mask and blur\n",
    "masker = Node(fsl.maths.ApplyMask(),name='masker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wf(name='wf'):\n",
    "    # SpecifyModel - Generates FSL-specific Model\n",
    "    modelspec = Node(SpecifyModel(input_units='secs',\n",
    "                                  time_repetition=TR,\n",
    "                                  high_pass_filter_cutoff=80),\n",
    "                     name=\"modelspec\")\n",
    "    # Level1Design - Creates FSL config file \n",
    "    level1design = Node(fsl.Level1Design(bases={'dgamma':{'derivs': True}},\n",
    "                                         interscan_interval=TR,\n",
    "                                         model_serial_correlations=True),\n",
    "                            name=\"level1design\")\n",
    "    # FEATmodel generates an FSL design matrix\n",
    "    level1model = Node(fsl.FEATModel(), name=\"FEATModel\")\n",
    "\n",
    "    # FILMGLs\n",
    "    # smooth_autocorr, check default, use FSL default\n",
    "    filmgls = Node(fsl.FILMGLS(), name=\"GLS\")\n",
    "\n",
    "    wf = Workflow(name=name)\n",
    "    wf.connect([(modelspec, level1design, [('session_info','session_info')]),\n",
    "              (level1design, level1model, [('ev_files', 'ev_files'),\n",
    "                                             ('fsf_files','fsf_file')]),\n",
    "              (level1model, filmgls, [('design_file', 'design_file'),\n",
    "                                        ('con_file', 'tcon_file'),\n",
    "                                        ('fcon_file', 'fcon_file')]),\n",
    "              (level1model, datasink, [('design_file', '%s.@design_file' % name)]),\n",
    "              (filmgls, datasink, [('copes', '%s.@copes' % name),\n",
    "                                    ('zstats', '%s.@Z' % name),\n",
    "                                    ('fstats', '%s.@F' % name),\n",
    "                                    ('tstats','%s.@T' % name),\n",
    "                                    ('param_estimates','%s.@param_estimates' % name),\n",
    "                                    ('residual4d', '%s.@residual4d' % name),\n",
    "                                    ('sigmasquareds', '%s.@sigmasquareds' % name)])])\n",
    "    return wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow for calculating contrasts\n",
    "wf1 = init_wf(name='contrast_wf')\n",
    "# workflow for only removing nuisance variables\n",
    "wf2 = init_wf(name='base_wf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiation of the 1st-level analysis workflow\n",
    "l1analysis = Workflow(name='l1analysis')\n",
    "l1analysis.base_dir = join(derivatives_dir, working_dir)\n",
    "# Connect up the 1st-level analysis components\n",
    "l1analysis.connect([(infosource, selectfiles, [('subject_id', 'subject_id'),\n",
    "                                               ('task', 'task')]),\n",
    "                    (infosource, subjectinfo, [('subject_id','subject_id'),\n",
    "                                                 ('task', 'task')]),\n",
    "                    (subjectinfo, save_subjectinfo, [('base_subjectinfo','base_subjectinfo'),\n",
    "                                                     ('contrast_subjectinfo','contrast_subjectinfo'),\n",
    "                                                     ('contrasts','contrasts')]),\n",
    "                    (selectfiles, masker, [('func','in_file'),\n",
    "                                           ('mask', 'mask_file')])\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add on contrast wf\n",
    "l1analysis.connect([\n",
    "                    (subjectinfo, wf1, [('contrast_subjectinfo','modelspec.subject_info')]),\n",
    "                    (masker, wf1, [('out_file', 'modelspec.functional_runs')]),\n",
    "                    (subjectinfo, wf1, [('contrasts','level1design.contrasts')]),\n",
    "                    (masker, wf1, [('out_file','GLS.in_file')])\n",
    "                    ])\n",
    "# # add on residual wf\n",
    "# l1analysis.connect([\n",
    "#                     (subjectinfo, wf2, [('base_subjectinfo','modelspec.subject_info')]),\n",
    "#                     (masker, wf2, [('out_file', 'modelspec.functional_runs')]),\n",
    "#                     (masker, wf2, [('out_file','GLS.in_file')])\n",
    "#                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180430-20:57:42,150 workflow INFO:\n",
      "\t Workflow l1analysis settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "180430-20:57:42,161 workflow INFO:\n",
      "\t Running serially.\n",
      "180430-20:57:42,162 workflow INFO:\n",
      "\t Executing node l1analysis.getsubjectinfo in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130_task_motorSelectiveStop/getsubjectinfo\n",
      "180430-20:57:42,166 workflow INFO:\n",
      "\t Running node \"getsubjectinfo\" (\"nipype.interfaces.utility.wrappers.Function\").\n",
      "180430-20:57:42,538 workflow INFO:\n",
      "\t Executing node l1analysis.savesubjectinfo in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130_task_motorSelectiveStop/savesubjectinfo\n",
      "180430-20:57:42,864 workflow INFO:\n",
      "\t Running node \"savesubjectinfo\" (\"nipype.interfaces.utility.wrappers.Function\").\n",
      "180430-20:57:43,10 workflow INFO:\n",
      "\t Executing node l1analysis.selectfiles in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130_task_motorSelectiveStop/selectfiles\n",
      "180430-20:57:43,13 workflow INFO:\n",
      "\t Running node \"selectfiles\" (\"nipype.interfaces.io.SelectFiles\").\n",
      "180430-20:57:43,19 workflow INFO:\n",
      "\t Executing node l1analysis.masker in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130_task_motorSelectiveStop/masker\n",
      "180430-20:57:43,24 workflow INFO:\n",
      "\t Running node \"masker\" (\"nipype.interfaces.fsl.maths.ApplyMask\"), a CommandLine Interface with command:\n",
      "fslmaths /home/ian/tmp/fmri/derivatives/fmriprep/fmriprep/sub-s130/ses-1/func/sub-s130_ses-1_task-motorSelectiveStop_run-1_bold_space-MNI152NLin2009cAsym_preproc.nii.gz -mas /home/ian/tmp/fmri/derivatives/fmriprep/fmriprep/sub-s130/ses-1/func/sub-s130_ses-1_task-motorSelectiveStop_run-1_bold_space-MNI152NLin2009cAsym_brainmask.nii.gz /media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130_task_motorSelectiveStop/masker/sub-s130_ses-1_task-motorSelectiveStop_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz.\n",
      "180430-20:58:54,894 workflow INFO:\n",
      "\t Executing node l1analysis.contrast_wf.modelspec in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/contrast_wf/_subject_id_s130_task_motorSelectiveStop/modelspec\n",
      "180430-20:58:55,57 workflow INFO:\n",
      "\t Running node \"modelspec\" (\"nipype.algorithms.modelgen.SpecifyModel\").\n",
      "180430-20:58:55,409 workflow INFO:\n",
      "\t Executing node l1analysis.contrast_wf.level1design in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/contrast_wf/_subject_id_s130_task_motorSelectiveStop/level1design\n",
      "180430-20:58:55,703 workflow INFO:\n",
      "\t Running node \"level1design\" (\"nipype.interfaces.fsl.model.Level1Design\").\n",
      "180430-20:58:55,873 workflow INFO:\n",
      "\t Executing node l1analysis.contrast_wf.FEATModel in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/contrast_wf/_subject_id_s130_task_motorSelectiveStop/FEATModel\n",
      "180430-20:58:55,890 workflow INFO:\n",
      "\t Running node \"FEATModel\" (\"nipype.interfaces.fsl.model.FEATModel\"), a CommandLine Interface with command:\n",
      "feat_model run0 .\n",
      "180430-20:58:58,853 workflow INFO:\n",
      "\t Executing node l1analysis.contrast_wf.GLS in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/contrast_wf/_subject_id_s130_task_motorSelectiveStop/GLS\n",
      "180430-20:58:58,857 workflow INFO:\n",
      "\t Running node \"GLS\" (\"nipype.interfaces.fsl.model.FILMGLS\"), a CommandLine Interface with command:\n",
      "film_gls --rn=results --con=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/contrast_wf/_subject_id_s130_task_motorSelectiveStop/FEATModel/run0.con --in=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130_task_motorSelectiveStop/masker/sub-s130_ses-1_task-motorSelectiveStop_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz --pd=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/contrast_wf/_subject_id_s130_task_motorSelectiveStop/FEATModel/run0.mat --thr=0.000000.\n",
      "180430-20:58:58,907 interface INFO:\n",
      "\t stdout 2018-04-30T20:58:58.907767:Log directory is: results\n",
      "180430-20:59:27,260 interface INFO:\n",
      "\t stdout 2018-04-30T20:59:27.260111:paradigm.getDesignMatrix().Nrows()=984\n",
      "180430-20:59:27,260 interface INFO:\n",
      "\t stdout 2018-04-30T20:59:27.260111:paradigm.getDesignMatrix().Ncols()=32\n",
      "180430-20:59:27,261 interface INFO:\n",
      "\t stdout 2018-04-30T20:59:27.260111:sizeTS=984\n",
      "180430-20:59:27,262 interface INFO:\n",
      "\t stdout 2018-04-30T20:59:27.262211:numTS=199250\n",
      "180430-20:59:27,635 interface INFO:\n",
      "\t stdout 2018-04-30T20:59:27.635479:Calculating residuals...\n",
      "180430-22:15:21,456 workflow ERROR:\n",
      "\t Node GLS.a0 failed to run on host ian-System-Product-Name.\n",
      "180430-22:15:21,459 workflow ERROR:\n",
      "\t Saving crash info to /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180430-221521-ian-GLS.a0-9781781c-3542-4c92-88fd-0eebe7d78c2d.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/linear.py\", line 43, in run\n",
      "    node.run(updatehash=updatehash)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1089, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1693, in _run_interface\n",
      "    runtime = run_command(runtime, output=self.terminal_output)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1417, in run_command\n",
      "    _process()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1404, in _process\n",
      "    res = select.select(streams, [], [], timeout)\n",
      "KeyboardInterrupt\n",
      "\n",
      "180430-22:15:21,461 workflow INFO:\n",
      "\t ***********************************\n",
      "180430-22:15:21,462 workflow ERROR:\n",
      "\t could not run node: l1analysis.contrast_wf.GLS.a0\n",
      "180430-22:15:21,463 workflow INFO:\n",
      "\t crashfile: /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180430-221521-ian-GLS.a0-9781781c-3542-4c92-88fd-0eebe7d78c2d.pklz\n",
      "180430-22:15:21,464 workflow INFO:\n",
      "\t ***********************************\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Workflow did not execute cleanly. Check log for details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-335945f57108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#l1analysis.run('MultiProc', plugin_args={'n_procs': 8})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ml1analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/workflows.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, plugin, plugin_args, updatehash)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'create_report'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_report_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdatehash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdatehash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mdatestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%dT%H%M%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'write_provenance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/linear.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, graph, config, updatehash)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_callback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exception'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mreport_nodes_not_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/tools.py\u001b[0m in \u001b[0;36mreport_nodes_not_run\u001b[0;34m(notrun)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***********************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         raise RuntimeError(('Workflow did not execute cleanly. '\n\u001b[0m\u001b[1;32m     80\u001b[0m                             'Check log for details'))\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Workflow did not execute cleanly. Check log for details"
     ]
    }
   ],
   "source": [
    "l1analysis.run('MultiProc', plugin_args={'n_procs': 8})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "243px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
