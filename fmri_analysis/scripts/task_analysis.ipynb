{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180509-11:39:16,640 duecredit ERROR:\n",
      "\t Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from inspect import currentframe, getframeinfo\n",
    "from pathlib import Path\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.algorithms.modelgen import SpecifyModel\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.pipeline.engine import Workflow, Node\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "from utils.event_utils import get_contrasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments\n",
    "These are not needed for the jupyter notebook, but are used after conversion to a script for production\n",
    "\n",
    "- conversion command:\n",
    "  - jupyter nbconvert --to script --execute task_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Example BIDS App entrypoint script.')\n",
    "parser.add_argument('-derivatives_dir', default='/derivatives')\n",
    "parser.add_argument('-data_dir', default='/data')\n",
    "parser.add_argument('--participant_labels',nargs=\"+\")\n",
    "parser.add_argument('--tasks', nargs=\"+\")\n",
    "parser.add_argument('--ignore_rt', action='store_true')\n",
    "parser.add_argument('--cleanup', action='store_true')\n",
    "if '-derivatives_dir' in sys.argv or '-h' in sys.argv:\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    args = parser.parse_args([])\n",
    "    args.derivatives_dir = '/home/ian/tmp/fmri/derivatives/'\n",
    "    args.data_dir = '/mnt/OAK'\n",
    "    args.tasks = ['stroop']\n",
    "    args.participant_labels = ['s130']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get current directory to pass to function nodes\n",
    "filename = getframeinfo(currentframe()).filename\n",
    "current_directory = str(Path(filename).resolve().parent)\n",
    "\n",
    "# list of subject identifiers\n",
    "subject_list = args.participant_labels\n",
    "# list of task identifiers\n",
    "if args.tasks is not None:\n",
    "    task_list = args.tasks\n",
    "else:\n",
    "    task_list = ['ANT', 'CCTHot', 'discountFix',\n",
    "               'DPX', 'motorSelectiveStop',\n",
    "               'stopSignal', 'stroop', 'surveyMedley',\n",
    "               'twoByTwo', 'WATT3']\n",
    "\n",
    "regress_rt = not args.ignore_rt\n",
    "#### Experiment Variables\n",
    "derivatives_dir = args.derivatives_dir\n",
    "fmriprep_dir = join(derivatives_dir, 'fmriprep', 'fmriprep')\n",
    "data_dir = args.data_dir\n",
    "first_level_dir = join(derivatives_dir,'1stLevel')\n",
    "working_dir = 'workingdir'\n",
    "# TR of functional images\n",
    "TR = .68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Task List: ['stroop']\n",
      ", Subjects: ['s130']\n",
      ", derivatives_dir: /home/ian/tmp/fmri/derivatives/\n",
      ", data_dir: /mnt/OAK\n",
      "*******************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print('*'*79)\n",
    "print('Task List: %s\\n, Subjects: %s\\n, derivatives_dir: %s\\n, data_dir: %s' % \n",
    "     (task_list, subject_list, derivatives_dir, data_dir))\n",
    "print('*'*79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function to create bunch\n",
    "def getsubjectinfo(data_dir, fmriprep_dir, subject_id, task, regress_rt, utils_path): \n",
    "    from glob import glob\n",
    "    from os.path import join\n",
    "    import pandas as pd\n",
    "    from nipype.interfaces.base import Bunch\n",
    "    import sys\n",
    "    sys.path.append(utils_path)\n",
    "    from utils.event_utils import get_beta_series, parse_EVs, process_confounds\n",
    "    # strip \"sub\" from beginning of subject_id if provided\n",
    "    subject_id = subject_id.replace('sub-','')\n",
    "    ## Get the Confounds File (output of fmriprep)\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    confounds_file = glob(join(fmriprep_dir,\n",
    "                               'sub-%s' % subject_id,\n",
    "                               '*', 'func',\n",
    "                               '*%s*confounds.tsv' % task))[0]\n",
    "    regressors, regressor_names = process_confounds(confounds_file)\n",
    "    ## Get the Events File if it exists\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    event_file = glob(join(data_dir,\n",
    "                           'sub-%s' % subject_id,\n",
    "                           '*', 'func',\n",
    "                           '*%s*events.tsv' % task))\n",
    "    beta_subjectinfo = None\n",
    "    contrast_subjectinfo = None\n",
    "    contrast = None\n",
    "    if len(event_file)>0:\n",
    "        # set up events file\n",
    "        event_file = event_file[0]\n",
    "        events_df = pd.read_csv(event_file,sep = '\\t')\n",
    "        EV_dict, beta_dict = parse_EVs(events_df, task, regress_rt, beta=True)\n",
    "        # create beta series info\n",
    "        beta_subjectinfo = Bunch(subject_id=subject_id,\n",
    "                                 task=task,\n",
    "                                 conditions=beta_dict['conditions'],\n",
    "                                 onsets=beta_dict['onsets'],\n",
    "                                 durations=beta_dict['durations'],\n",
    "                                 amplitudes=beta_dict['amplitudes'],\n",
    "                                 tmod=None,\n",
    "                                 pmod=None,\n",
    "                                 regressor_names=regressor_names,\n",
    "                                 regressors=regressors.T.tolist())\n",
    "        # set up contrasts\n",
    "        contrast_subjectinfo = Bunch(subject_id=subject_id,\n",
    "                                     task=task,\n",
    "                                     conditions=EV_dict['conditions'],\n",
    "                                     onsets=EV_dict['onsets'],\n",
    "                                     durations=EV_dict['durations'],\n",
    "                                     amplitudes=EV_dict['amplitudes'],\n",
    "                                     tmod=None,\n",
    "                                     pmod=None,\n",
    "                                     regressor_names=regressor_names,\n",
    "                                     regressors=regressors.T.tolist())\n",
    "    return beta_subjectinfo, contrast_subjectinfo\n",
    "    \n",
    "def save_subjectinfo(save_directory, beta_subjectinfo, contrast_subjectinfo, contrasts=[]):\n",
    "    from os import makedirs\n",
    "    from os.path import join\n",
    "    import pickle\n",
    "    subject_id = beta_subjectinfo.subject_id\n",
    "    task = beta_subjectinfo.task\n",
    "    subjectinfo_dir = join(save_directory, subject_id, '%s_subject_info' % task)\n",
    "    makedirs(subjectinfo_dir, exist_ok=True)\n",
    "    # save beta subject info\n",
    "    beta_path = join(subjectinfo_dir,'beta_subjectinfo.pkl')\n",
    "    pickle.dump(beta_subjectinfo, open(beta_path,'wb'))\n",
    "    # save contrast subject info\n",
    "    if len(contrast_subjectinfo.items()) > 0:\n",
    "        contrast_path = join(subjectinfo_dir,'contrast_subjectinfo.pkl')\n",
    "        pickle.dump(contrast_subjectinfo, open(contrast_path,'wb'))\n",
    "    # save contrast list\n",
    "    if len(contrasts) > 0:\n",
    "        contrastlist_path = join(subjectinfo_dir,'contrasts.pkl')\n",
    "        pickle.dump(contrasts, open(contrastlist_path,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View one events file used in subject info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Input and Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_subjectinfo(name):\n",
    "    # Get Subject Info - get subject specific condition information\n",
    "    subjectinfo = Node(Function(input_names=['data_dir', 'fmriprep_dir','subject_id', \n",
    "                                             'task','regress_rt', 'utils_path'],\n",
    "                                   output_names=['beta_subjectinfo', \n",
    "                                                 'contrast_subjectinfo'],\n",
    "                                   function=getsubjectinfo),\n",
    "                          name=name)\n",
    "    subjectinfo.inputs.fmriprep_dir = fmriprep_dir\n",
    "    subjectinfo.inputs.data_dir = data_dir\n",
    "    subjectinfo.inputs.regress_rt = regress_rt\n",
    "    subjectinfo.inputs.utils_path = current_directory\n",
    "    return subjectinfo\n",
    "\n",
    "def get_savesubjectinfo(name):\n",
    "    # Save python objects that aren't accomodated by datasink nodes\n",
    "    savesubjectinfo = Node(Function(input_names=['save_directory',\n",
    "                                                  'beta_subjectinfo',\n",
    "                                                  'contrast_subjectinfo','contrasts'],\n",
    "                                    function=save_subjectinfo),\n",
    "                           name=name)\n",
    "    savesubjectinfo.inputs.save_directory = first_level_dir\n",
    "    return savesubjectinfo\n",
    "\n",
    "def get_selector(name, session=None):\n",
    "    if session is None:\n",
    "        ses = '*'\n",
    "    else:\n",
    "        ses = 'ses-%s' % str(session)\n",
    "    # SelectFiles - to grab the data (alternative to DataGrabber)\n",
    "    templates = {'func': join('sub-{subject_id}',ses,'func',\n",
    "                             '*{task}*MNI*preproc.nii.gz'),\n",
    "                 'mask': join('sub-{subject_id}',ses,'func',\n",
    "                              '*{task}*MNI*brainmask.nii.gz')}\n",
    "    selectfiles = Node(SelectFiles(templates,\n",
    "                                   base_directory=fmriprep_dir,\n",
    "                                   sort_filelist=True),\n",
    "                       name=name)\n",
    "    return selectfiles\n",
    "\n",
    "def get_masker(name):\n",
    "    # mask and blur\n",
    "    return Node(fsl.maths.ApplyMask(),name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_common_wf(workflow, task):\n",
    "        # Infosource - a function free node to iterate over the list of subject names\n",
    "    infosource = Node(IdentityInterface(fields=['subject_id',\n",
    "                                                'task',\n",
    "                                                'contrasts']),\n",
    "                      name=\"%s_infosource\" % task)\n",
    "    infosource.iterables = [('subject_id', subject_list)]\n",
    "    infosource.inputs.task = task\n",
    "    infosource.inputs.contrasts = get_contrasts(task, regress_rt)\n",
    "    \n",
    "    # initiate basic nodes\n",
    "    subjectinfo = get_subjectinfo('%s_subjectinfo' % task)\n",
    "    savesubjectinfo = get_savesubjectinfo('%s_savesubjectinfo' % task)\n",
    "    masker = get_masker('%s_masker' % task)\n",
    "    selectfiles = get_selector('%s_selectFiles' % task)\n",
    "    \n",
    "    # Connect up the 1st-level analysis components\n",
    "    workflow.connect([(infosource, selectfiles, [('subject_id', 'subject_id'), ('task', 'task')]),\n",
    "                      (infosource, subjectinfo, [('subject_id','subject_id'), ('task', 'task')]),\n",
    "                      (infosource, savesubjectinfo, [('contrasts','contrasts')]),\n",
    "                      (subjectinfo, savesubjectinfo, [('beta_subjectinfo','beta_subjectinfo'),\n",
    "                                                      ('contrast_subjectinfo','contrast_subjectinfo')]),\n",
    "                      (selectfiles, masker, [('func','in_file'),\n",
    "                                             ('mask', 'mask_file')])\n",
    "                        ])\n",
    "\n",
    "def init_GLM_wf(name='wf'):\n",
    "    # Datasink - creates output folder for important outputs\n",
    "    datasink = Node(DataSink(base_directory=first_level_dir), name=\"datasink\")\n",
    "    # Use the following DataSink output substitutions\n",
    "    substitutions = [('_subject_id_', ''),\n",
    "                    ('fstat', 'FSTST'),\n",
    "                    ('run0.mat', 'designfile.mat')]\n",
    "    datasink.inputs.substitutions = substitutions\n",
    "    \n",
    "    # SpecifyModel - Generates FSL-specific Model\n",
    "    modelspec = Node(SpecifyModel(input_units='secs',\n",
    "                                  time_repetition=TR,\n",
    "                                  high_pass_filter_cutoff=80),\n",
    "                     name=\"modelspec\")\n",
    "    # Level1Design - Creates FSL config file \n",
    "    level1design = Node(fsl.Level1Design(bases={'dgamma':{'derivs': True}},\n",
    "                                         interscan_interval=TR,\n",
    "                                         model_serial_correlations=True),\n",
    "                            name=\"level1design\")\n",
    "    # FEATmodel generates an FSL design matrix\n",
    "    level1model = Node(fsl.FEATModel(), name=\"FEATModel\")\n",
    "\n",
    "    # FILMGLs\n",
    "    # smooth_autocorr, check default, use FSL default\n",
    "    filmgls = Node(fsl.FILMGLS(), name=\"GLS\")\n",
    "\n",
    "    wf = Workflow(name=name)\n",
    "    wf.connect([(modelspec, level1design, [('session_info','session_info')]),\n",
    "              (level1design, level1model, [('ev_files', 'ev_files'),\n",
    "                                             ('fsf_files','fsf_file')]),\n",
    "              (level1model, filmgls, [('design_file', 'design_file'),\n",
    "                                      ('con_file', 'tcon_file'),\n",
    "                                      ('fcon_file', 'fcon_file')]),\n",
    "              (level1model, datasink, [('design_file', '%s.@design_file' % name)]),\n",
    "              (filmgls, datasink, [('copes', '%s.@copes' % name),\n",
    "                                    ('zstats', '%s.@Z' % name),\n",
    "                                    ('fstats', '%s.@F' % name),\n",
    "                                    ('tstats','%s.@T' % name),\n",
    "                                    ('param_estimates','%s.@param_estimates' % name),\n",
    "                                    ('residual4d', '%s.@residual4d' % name),\n",
    "                                    ('sigmasquareds', '%s.@sigmasquareds' % name)])\n",
    "               ])\n",
    "    return wf\n",
    "\n",
    "\n",
    "\n",
    "def get_task_wfs(task):\n",
    "    wf_dict = {}\n",
    "    # set up workflow lookup\n",
    "    default_wf = [(init_GLM_wf, {'name': '%s_contrast_wf' % task}), \n",
    "                  (init_GLM_wf, {'name': '%s_beta_wf' % task})]\n",
    "    \n",
    "    # get workflow\n",
    "    workflows = []\n",
    "    for func, kwargs in wf_dict.get(task, default_wf):\n",
    "        workflows.append(func(**kwargs))\n",
    "    return workflows\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiation of the 1st-level analysis workflow\n",
    "l1analysis = Workflow(name='l1analysis')\n",
    "l1analysis.base_dir = join(derivatives_dir, working_dir)\n",
    "\n",
    "for task in task_list:\n",
    "    init_common_wf(l1analysis, task)\n",
    "    task_workflows = get_task_wfs(task)\n",
    "    # get nodes to pass\n",
    "    infosource = l1analysis.get_node('%s_infosource' % task)\n",
    "    subjectinfo = l1analysis.get_node('%s_subjectinfo' % task)\n",
    "    masker = l1analysis.get_node('%s_masker' % task)\n",
    "    for wf in task_workflows:\n",
    "        l1analysis.connect([\n",
    "                            (infosource, wf, [('subject_id','datasink.container')]),\n",
    "                            (subjectinfo, wf, [('contrast_subjectinfo','modelspec.subject_info')]),\n",
    "                            (masker, wf, [('out_file', 'modelspec.functional_runs')]),\n",
    "                            (masker, wf, [('out_file','GLS.in_file')])\n",
    "                            ])\n",
    "        \n",
    "        if 'contrast' in wf.name:\n",
    "            l1analysis.connect([(infosource, wf, [('contrasts','level1design.contrasts')])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180509-11:39:17,436 workflow INFO:\n",
      "\t Workflow l1analysis settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "180509-11:39:17,453 workflow INFO:\n",
      "\t Running in parallel.\n",
      "180509-11:39:17,456 workflow INFO:\n",
      "\t Currently running 0 tasks, and 2 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 8/8\n",
      "180509-11:39:17,464 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_subjectinfo.a0 jobid: 0\n",
      "180509-11:39:17,468 workflow INFO:\n",
      "\t Executing node l1analysis.stroop_selectFiles in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130/stroop_selectFiles\n",
      "180509-11:39:17,474 workflow INFO:\n",
      "\t Running node \"stroop_selectFiles\" (\"nipype.interfaces.io.SelectFiles\").\n",
      "180509-11:39:19,471 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_selectFiles.a0 jobid: 2\n",
      "180509-11:39:19,478 workflow INFO:\n",
      "\t Currently running 0 tasks, and 2 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 8/8\n",
      "180509-11:39:19,514 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_savesubjectinfo.a0 jobid: 1\n",
      "180509-11:39:19,518 workflow INFO:\n",
      "\t Executing node l1analysis.stroop_masker in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130/stroop_masker\n",
      "180509-11:39:19,528 workflow INFO:\n",
      "\t Running node \"stroop_masker\" (\"nipype.interfaces.fsl.maths.ApplyMask\"), a CommandLine Interface with command:\n",
      "fslmaths /home/ian/tmp/fmri/derivatives/fmriprep/fmriprep/sub-s130/ses-1/func/sub-s130_ses-1_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_preproc.nii.gz -mas /home/ian/tmp/fmri/derivatives/fmriprep/fmriprep/sub-s130/ses-1/func/sub-s130_ses-1_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_brainmask.nii.gz /media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130/stroop_masker/sub-s130_ses-1_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz.\n",
      "180509-11:39:21,520 workflow INFO:\n",
      "\t Currently running 1 tasks, and 0 jobs ready. Free memory (GB): 28.03/28.23, Free processors: 7/8\n",
      "180509-11:39:45,556 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_masker.a0 jobid: 3\n",
      "180509-11:39:45,563 workflow INFO:\n",
      "\t Currently running 0 tasks, and 2 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 8/8\n",
      "180509-11:39:45,592 workflow INFO:\n",
      "\t Executing node l1analysis.stroop_contrast_wf.modelspec in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/stroop_contrast_wf/_subject_id_s130/modelspec\n",
      "180509-11:39:45,614 workflow INFO:\n",
      "\t Executing node l1analysis.stroop_beta_wf.modelspec in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/stroop_beta_wf/_subject_id_s130/modelspec\n",
      "180509-11:39:45,654 workflow INFO:\n",
      "\t Running node \"modelspec\" (\"nipype.algorithms.modelgen.SpecifyModel\").\n",
      "180509-11:39:45,676 workflow INFO:\n",
      "\t Running node \"modelspec\" (\"nipype.algorithms.modelgen.SpecifyModel\").\n",
      "180509-11:39:47,617 workflow INFO:\n",
      "\t [Job finished] jobname: modelspec.a0 jobid: 4\n",
      "180509-11:39:47,622 workflow INFO:\n",
      "\t [Job finished] jobname: modelspec.a0 jobid: 9\n",
      "180509-11:39:47,629 workflow INFO:\n",
      "\t Currently running 0 tasks, and 2 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 8/8\n",
      "180509-11:39:47,703 workflow INFO:\n",
      "\t Executing node l1analysis.stroop_contrast_wf.level1design in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/stroop_contrast_wf/_subject_id_s130/level1design\n",
      "180509-11:39:47,806 workflow INFO:\n",
      "\t Executing node l1analysis.stroop_beta_wf.level1design in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/stroop_beta_wf/_subject_id_s130/level1design\n",
      "180509-11:39:47,870 workflow INFO:\n",
      "\t Running node \"level1design\" (\"nipype.interfaces.fsl.model.Level1Design\").\n",
      "180509-11:39:47,931 workflow INFO:\n",
      "\t Running node \"level1design\" (\"nipype.interfaces.fsl.model.Level1Design\").\n",
      "180509-11:39:49,807 workflow INFO:\n",
      "\t [Job finished] jobname: level1design.a0 jobid: 5\n",
      "180509-11:39:49,812 workflow INFO:\n",
      "\t [Job finished] jobname: level1design.a0 jobid: 10\n",
      "180509-11:39:49,814 workflow INFO:\n",
      "\t Currently running 0 tasks, and 2 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 8/8\n",
      "180509-11:39:49,824 workflow INFO:\n",
      "\t Executing node l1analysis.stroop_contrast_wf.FEATModel in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/stroop_contrast_wf/_subject_id_s130/FEATModel\n",
      "180509-11:39:49,837 workflow INFO:\n",
      "\t Executing node l1analysis.stroop_beta_wf.FEATModel in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/stroop_beta_wf/_subject_id_s130/FEATModel\n",
      "180509-11:39:49,846 workflow INFO:\n",
      "\t Running node \"FEATModel\" (\"nipype.interfaces.fsl.model.FEATModel\"), a CommandLine Interface with command:\n",
      "feat_model run0 .\n",
      "180509-11:39:49,859 workflow INFO:\n",
      "\t Running node \"FEATModel\" (\"nipype.interfaces.fsl.model.FEATModel\"), a CommandLine Interface with command:\n",
      "feat_model run0 .\n",
      "180509-11:39:51,840 workflow INFO:\n",
      "\t [Job finished] jobname: FEATModel.a0 jobid: 6\n",
      "180509-11:39:51,844 workflow INFO:\n",
      "\t [Job finished] jobname: FEATModel.a0 jobid: 11\n",
      "180509-11:39:51,851 workflow INFO:\n",
      "\t Currently running 0 tasks, and 2 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 8/8\n",
      "180509-11:39:51,856 workflow INFO:\n",
      "\t Executing node l1analysis.stroop_contrast_wf.GLS in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/stroop_contrast_wf/_subject_id_s130/GLS\n",
      "180509-11:39:51,859 workflow INFO:\n",
      "\t Executing node l1analysis.stroop_beta_wf.GLS in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/stroop_beta_wf/_subject_id_s130/GLS\n",
      "180509-11:39:51,861 workflow INFO:\n",
      "\t Running node \"GLS\" (\"nipype.interfaces.fsl.model.FILMGLS\"), a CommandLine Interface with command:\n",
      "film_gls --rn=results --con=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/stroop_contrast_wf/_subject_id_s130/FEATModel/run0.con --in=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130/stroop_masker/sub-s130_ses-1_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz --pd=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/stroop_contrast_wf/_subject_id_s130/FEATModel/run0.mat --thr=0.000000.180509-11:39:51,865 workflow INFO:\n",
      "\t Running node \"GLS\" (\"nipype.interfaces.fsl.model.FILMGLS\"), a CommandLine Interface with command:\n",
      "film_gls --rn=results --con=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/stroop_beta_wf/_subject_id_s130/FEATModel/run0.con --in=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130/stroop_masker/sub-s130_ses-1_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz --pd=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/stroop_beta_wf/_subject_id_s130/FEATModel/run0.mat --thr=0.000000.\n",
      "\n",
      "180509-11:39:53,861 workflow INFO:\n",
      "\t Currently running 2 tasks, and 0 jobs ready. Free memory (GB): 27.83/28.23, Free processors: 6/8\n"
     ]
    }
   ],
   "source": [
    "#l1analysis.run()\n",
    "l1analysis.run('MultiProc', plugin_args={'n_procs': 8})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "243px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
