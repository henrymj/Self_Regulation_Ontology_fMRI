{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180503-22:40:42,923 duecredit ERROR:\n",
      "\t Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from inspect import currentframe, getframeinfo\n",
    "from pathlib import Path\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.algorithms.modelgen import SpecifyModel\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.pipeline.engine import Workflow, Node\n",
    "import os\n",
    "from os.path import join\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments\n",
    "These are not needed for the jupyter notebook, but are used after conversion to a script for production\n",
    "\n",
    "- conversion command:\n",
    "  - jupyter nbconvert --to script --execute task_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Example BIDS App entrypoint script.')\n",
    "parser.add_argument('-derivatives_dir', default='/derivatives')\n",
    "parser.add_argument('-data_dir', default='/data')\n",
    "parser.add_argument('--participant_labels',nargs=\"+\")\n",
    "parser.add_argument('--events_dir', default=None)\n",
    "parser.add_argument('--tasks', nargs=\"+\")\n",
    "parser.add_argument('--ignore_rt', action='store_true')\n",
    "parser.add_argument('--cleanup', action='store_true')\n",
    "if '-derivatives_dir' in sys.argv or '-h' in sys.argv:\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    args = parser.parse_args([])\n",
    "    args.derivatives_dir = '/mnt/OAK/derivatives/'\n",
    "    args.data_dir = '/mnt/OAK'\n",
    "    args.tasks = ['stroop']\n",
    "    args.participant_labels = ['s130']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get current directory to pass to function nodes\n",
    "filename = getframeinfo(currentframe()).filename\n",
    "current_directory = str(Path(filename).resolve().parent)\n",
    "\n",
    "# list of subject identifiers\n",
    "subject_list = args.participant_labels\n",
    "# list of task identifiers\n",
    "if args.tasks is not None:\n",
    "    task_list = args.tasks\n",
    "else:\n",
    "    task_list = ['ANT', 'CCTHot', 'discountFix',\n",
    "               'DPX', 'motorSelectiveStop',\n",
    "               'stopSignal', 'stroop', 'surveyMedley',\n",
    "               'twoByTwo', 'WATT3']\n",
    "\n",
    "regress_rt = not args.ignore_rt\n",
    "#### Experiment Variables\n",
    "derivatives_dir = args.derivatives_dir\n",
    "fmriprep_dir = join(derivatives_dir, 'fmriprep', 'fmriprep')\n",
    "data_dir = args.data_dir\n",
    "first_level_dir = '1stLevel'\n",
    "working_dir = 'workingdir'\n",
    "# TR of functional images\n",
    "TR = .68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Task List: ['stroop']\n",
      ", Subjects: ['s130']\n",
      ", derivatives_dir: /mnt/OAK/derivatives/\n",
      ", data_dir: /mnt/OAK\n",
      "*******************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print('*'*79)\n",
    "print('Task List: %s\\n, Subjects: %s\\n, derivatives_dir: %s\\n, data_dir: %s' % \n",
    "     (task_list, subject_list, derivatives_dir, data_dir))\n",
    "print('*'*79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function to create bunch\n",
    "def getsubjectinfo(data_dir, fmriprep_dir, subject_id, task, regress_rt, utils_path): \n",
    "    from glob import glob\n",
    "    from os.path import join\n",
    "    import pandas as pd\n",
    "    from nipype.interfaces.base import Bunch\n",
    "    import sys\n",
    "    sys.path.append(utils_path)\n",
    "    from utils.event_utils import get_contrasts, parse_EVs, process_confounds\n",
    "    # strip \"sub\" from beginning of subject_id if provided\n",
    "    subject_id = subject_id.replace('sub-','')\n",
    "    ## Get the Confounds File (output of fmriprep)\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    confounds_file = glob(join(fmriprep_dir,\n",
    "                               'sub-%s' % subject_id,\n",
    "                               '*', 'func',\n",
    "                               '*%s*confounds.tsv' % task))[0]\n",
    "    regressors, regressor_names = process_confounds(confounds_file)\n",
    "    ## Get the Events File if it exists\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    event_file = glob(join(data_dir,\n",
    "                           'sub-%s' % subject_id,\n",
    "                           '*', 'func',\n",
    "                           '*%s*events.tsv' % task))\n",
    "    # create base_subject info\n",
    "    base_subjectinfo = Bunch(subject_id=subject_id,\n",
    "                             task=task,\n",
    "                             tmod=None,\n",
    "                             pmod=None,\n",
    "                             regressor_names=regressor_names,\n",
    "                             regressors=regressors.T.tolist())\n",
    "    if len(event_file)>0:\n",
    "        event_file = event_file[0]\n",
    "        events_df = pd.read_csv(event_file,sep = '\\t')\n",
    "        # set up contrasts\n",
    "        EV_dict = parse_EVs(events_df, task, regress_rt)\n",
    "        contrast_subjectinfo = Bunch(subject_id=subject_id,\n",
    "                                     task=task,\n",
    "                                     conditions=EV_dict['conditions'],\n",
    "                                     onsets=EV_dict['onsets'],\n",
    "                                     durations=EV_dict['durations'],\n",
    "                                     amplitudes=EV_dict['amplitudes'],\n",
    "                                     tmod=None,\n",
    "                                     pmod=None,\n",
    "                                     regressor_names=regressor_names,\n",
    "                                     regressors=regressors.T.tolist())\n",
    "        contrasts = get_contrasts(task, regress_rt)\n",
    "        return base_subjectinfo, contrast_subjectinfo,  contrasts \n",
    "    else:\n",
    "        return base_subjectinfo, None, None\n",
    "    \n",
    "def save_subjectinfo(base_directory,base_subjectinfo, contrast_subjectinfo, contrasts=[]):\n",
    "    from os import makedirs\n",
    "    from os.path import join\n",
    "    import pickle\n",
    "    subject_id = base_subjectinfo.subject_id\n",
    "    task = base_subjectinfo.task\n",
    "    task_dir = join(base_directory, 'subject_info', subject_id + '_task_' + task)\n",
    "    makedirs(task_dir, exist_ok=True)\n",
    "    # save base subject info\n",
    "    subjectinfo_path = join(task_dir,'base_subjectinfo.pkl')\n",
    "    pickle.dump(base_subjectinfo, open(subjectinfo_path,'wb'))\n",
    "    # save contrast subject info\n",
    "    if len(contrast_subjectinfo.items()) > 0:\n",
    "        subjectinfo_path = join(task_dir,'contrast_subjectinfo.pkl')\n",
    "        pickle.dump(contrast_subjectinfo, open(subjectinfo_path,'wb'))\n",
    "    if len(contrasts) > 0:\n",
    "        contrast_path = join(task_dir,'contrasts.pkl')\n",
    "        pickle.dump(contrasts, open(contrast_path,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View one events file used in subject info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Input and Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_subjectinfo(name):\n",
    "    # Get Subject Info - get subject specific condition information\n",
    "    subjectinfo = Node(Function(input_names=['data_dir', 'fmriprep_dir','subject_id', \n",
    "                                             'task','regress_rt', 'utils_path'],\n",
    "                                   output_names=['base_subjectinfo', \n",
    "                                                 'contrast_subjectinfo',\n",
    "                                                 'contrasts'],\n",
    "                                   function=name),\n",
    "                          name=name)\n",
    "    subjectinfo.inputs.fmriprep_dir = fmriprep_dir\n",
    "    subjectinfo.inputs.data_dir = data_dir\n",
    "    subjectinfo.inputs.regress_rt = regress_rt\n",
    "    subjectinfo.inputs.utils_path = current_directory\n",
    "    return subjectinfo\n",
    "\n",
    "def get_savesubjectinfo(name):\n",
    "    # Save python objects that aren't accomodated by datasink nodes\n",
    "    save_subjectinfo = Node(Function(input_names=['base_directory',\n",
    "                                                  'base_subjectinfo',\n",
    "                                                  'contrast_subjectinfo','contrasts'],\n",
    "                                     output_names=['output_path'],\n",
    "                                    function=name),\n",
    "                           name=name)\n",
    "    save_subjectinfo.inputs.base_directory = join(derivatives_dir,first_level_dir)\n",
    "    return save_subjectinfo\n",
    "\n",
    "def get_selector(name, session=None):\n",
    "    if session is None:\n",
    "        ses = '*'\n",
    "    else:\n",
    "        ses = 'ses-%s' % str(session)\n",
    "    # SelectFiles - to grab the data (alternative to DataGrabber)\n",
    "    templates = {'func': join('*{subject_id}',ses,'func',\n",
    "                             '*{task}*MNI*preproc.nii.gz'),\n",
    "                'mask': join('*{subject_id}',ses,'func',\n",
    "                             '*{task}*MNI*brainmask.nii.gz')}\n",
    "    selectfiles = Node(SelectFiles(templates,\n",
    "                                   base_directory=fmriprep_dir,\n",
    "                                   sort_filelist=True),\n",
    "                       name=name)\n",
    "    return selectfiles\n",
    "\n",
    "def get_masker(name):\n",
    "    # mask and blur\n",
    "    return Node(fsl.maths.ApplyMask(),name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_GLM_wf(name='wf'):\n",
    "    # Datasink - creates output folder for important outputs\n",
    "    datasink = Node(DataSink(base_directory=derivatives_dir,\n",
    "                             container=first_level_dir),\n",
    "                    name=\"datasink\")\n",
    "    # Use the following DataSink output substitutions\n",
    "    substitutions = [('_subject_id_', ''),\n",
    "                    ('fstat', 'FSTST'),\n",
    "                    ('run0.mat', 'designfile.mat')]\n",
    "    datasink.inputs.substitutions = substitutions\n",
    "    \n",
    "    # SpecifyModel - Generates FSL-specific Model\n",
    "    modelspec = Node(SpecifyModel(input_units='secs',\n",
    "                                  time_repetition=TR,\n",
    "                                  high_pass_filter_cutoff=80),\n",
    "                     name=\"modelspec\")\n",
    "    # Level1Design - Creates FSL config file \n",
    "    level1design = Node(fsl.Level1Design(bases={'dgamma':{'derivs': True}},\n",
    "                                         interscan_interval=TR,\n",
    "                                         model_serial_correlations=True),\n",
    "                            name=\"level1design\")\n",
    "    # FEATmodel generates an FSL design matrix\n",
    "    level1model = Node(fsl.FEATModel(), name=\"FEATModel\")\n",
    "\n",
    "    # FILMGLs\n",
    "    # smooth_autocorr, check default, use FSL default\n",
    "    filmgls = Node(fsl.FILMGLS(), name=\"GLS\")\n",
    "\n",
    "    wf = Workflow(name=name)\n",
    "    wf.connect([(modelspec, level1design, [('session_info','session_info')]),\n",
    "              (level1design, level1model, [('ev_files', 'ev_files'),\n",
    "                                             ('fsf_files','fsf_file')]),\n",
    "              (level1model, filmgls, [('design_file', 'design_file'),\n",
    "                                        ('con_file', 'tcon_file'),\n",
    "                                        ('fcon_file', 'fcon_file')]),\n",
    "              (level1model, datasink, [('design_file', '%s.@design_file' % name)]),\n",
    "              (filmgls, datasink, [('copes', '%s.@copes' % name),\n",
    "                                    ('zstats', '%s.@Z' % name),\n",
    "                                    ('fstats', '%s.@F' % name),\n",
    "                                    ('tstats','%s.@T' % name),\n",
    "                                    ('param_estimates','%s.@param_estimates' % name),\n",
    "                                    ('residual4d', '%s.@residual4d' % name),\n",
    "                                    ('sigmasquareds', '%s.@sigmasquareds' % name)])])\n",
    "    return wf\n",
    "\n",
    "\n",
    "def init_common_wf(workflow, task):\n",
    "        # Infosource - a function free node to iterate over the list of subject names\n",
    "    infosource = Node(IdentityInterface(fields=['subject_id',\n",
    "                                                'task']),\n",
    "                      name=\"%s_infosource\" % task)\n",
    "    infosource.iterables = [('subject_id', subject_list)]\n",
    "    infosource.inputs.task = task\n",
    "    \n",
    "    # initiate basic nodes\n",
    "    subjectinfo = get_subjectinfo('%s_subjectinfo' % task)\n",
    "    save_subjectinfo = get_savesubjectinfo('%s_savesubjectinfo' % task)\n",
    "    masker = get_masker('%s_masker' % task)\n",
    "    selectfiles = get_selector('%s_selectFiles' % task)\n",
    "    \n",
    "    # Connect up the 1st-level analysis components\n",
    "    workflow.connect([(infosource, selectfiles, [('subject_id', 'subject_id'),\n",
    "                                                   ('task', 'task')]),\n",
    "                        (infosource, subjectinfo, [('subject_id','subject_id'),\n",
    "                                                     ('task', 'task')]),\n",
    "                        (subjectinfo, save_subjectinfo, [('base_subjectinfo','base_subjectinfo'),\n",
    "                                                         ('contrast_subjectinfo','contrast_subjectinfo'),\n",
    "                                                         ('contrasts','contrasts')]),\n",
    "                        (selectfiles, masker, [('func','in_file'),\n",
    "                                               ('mask', 'mask_file')])\n",
    "                        ])\n",
    "\n",
    "def get_task_wfs(task):\n",
    "    # set up workflow lookup\n",
    "    wf_dict = {'rest': [(init_GLM_wf, {'name': '%s_base_wf' % task})]}\n",
    "    \n",
    "    default_wf = [(init_GLM_wf, {'name': '%s_contrast_wf' % task}), \n",
    "                  (init_GLM_wf, {'name': '%s_base_wf' % task})]\n",
    "    \n",
    "    # get workflow\n",
    "    workflows = []\n",
    "    for func, kwargs in wf_dict.get(task, default_wf):\n",
    "        workflows.append(func(**kwargs))\n",
    "    return workflows\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiation of the 1st-level analysis workflow\n",
    "l1analysis = Workflow(name='l1analysis')\n",
    "l1analysis.base_dir = join(derivatives_dir, working_dir)\n",
    "\n",
    "for task in task_list:\n",
    "    init_common_wf(l1analysis, task)\n",
    "    task_workflows = get_task_wfs(task)\n",
    "    # get nodes to pass\n",
    "    subjectinfo = l1analysis.get_node('%s_subjectinfo' % task)\n",
    "    masker = l1analysis.get_node('%s_masker' % task)\n",
    "    for wf in task_workflows:\n",
    "        l1analysis.connect([\n",
    "                            (subjectinfo, wf, [('contrast_subjectinfo','modelspec.subject_info')]),\n",
    "                            (masker, wf, [('out_file', 'modelspec.functional_runs')]),\n",
    "                            (masker, wf, [('out_file','GLS.in_file')])\n",
    "                            ])\n",
    "        \n",
    "        if 'contrast' in wf.name:\n",
    "            l1analysis.connect([(subjectinfo, wf, [('contrasts','level1design.contrasts')])])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180503-22:36:29,100 workflow INFO:\n",
      "\t Workflow l1analysis settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "180503-22:36:30,30 workflow INFO:\n",
      "\t Running in parallel.\n",
      "180503-22:36:30,38 workflow INFO:\n",
      "\t Currently running 0 tasks, and 2 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 8/8\n",
      "180503-22:36:30,66 workflow INFO:\n",
      "\t Executing node l1analysis.stroop_selectFiles in dir: /mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130/stroop_selectFiles\n",
      "180503-22:36:30,104 workflow INFO:\n",
      "\t Executing node l1analysis.stroop_subjectinfo in dir: /mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130/stroop_subjectinfo\n",
      "180503-22:36:30,359 workflow INFO:\n",
      "\t Running node \"stroop_selectFiles\" (\"nipype.interfaces.io.SelectFiles\").\n",
      "180503-22:36:32,109 workflow INFO:\n",
      "\t Currently running 2 tasks, and 0 jobs ready. Free memory (GB): 27.83/28.23, Free processors: 6/8\n",
      "180503-22:36:32,482 workflow INFO:\n",
      "\t Running node \"stroop_subjectinfo\" (\"nipype.interfaces.utility.wrappers.Function\").\n",
      "180503-22:36:34,114 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_selectFiles.a0 jobid: 0\n",
      "180503-22:36:34,116 workflow INFO:\n",
      "\t Currently running 1 tasks, and 1 jobs ready. Free memory (GB): 28.03/28.23, Free processors: 7/8\n",
      "180503-22:36:34,631 workflow INFO:\n",
      "\t Executing node l1analysis.stroop_masker in dir: /mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130/stroop_masker\n",
      "180503-22:36:35,561 workflow INFO:\n",
      "\t Running node \"stroop_masker\" (\"nipype.interfaces.fsl.maths.ApplyMask\"), a CommandLine Interface with command:\n",
      "fslmaths /mnt/OAK/derivatives/fmriprep/fmriprep/sub-s130/ses-1/func/sub-s130_ses-1_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_preproc.nii.gz -mas /mnt/OAK/derivatives/fmriprep/fmriprep/sub-s130/ses-1/func/sub-s130_ses-1_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_brainmask.nii.gz /mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130/stroop_masker/sub-s130_ses-1_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz.\n",
      "180503-22:36:36,630 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_subjectinfo.a0 jobid: 2\n",
      "180503-22:36:36,636 workflow INFO:\n",
      "\t Currently running 1 tasks, and 1 jobs ready. Free memory (GB): 28.03/28.23, Free processors: 7/8\n",
      "180503-22:36:36,885 workflow INFO:\n",
      "\t Executing node l1analysis.stroop_savesubjectinfo in dir: /mnt/OAK/derivatives/workingdir/l1analysis/_subject_id_s130/stroop_savesubjectinfo\n",
      "180503-22:36:37,773 workflow INFO:\n",
      "\t Running node \"stroop_savesubjectinfo\" (\"nipype.interfaces.utility.wrappers.Function\").\n",
      "180503-22:36:38,888 workflow ERROR:\n",
      "\t Node stroop_savesubjectinfo.a0 failed to run on host ian-System-Product-Name.\n",
      "180503-22:36:38,892 workflow ERROR:\n",
      "\t Saving crash info to /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180503-223638-ian-stroop_savesubjectinfo.a0-6de313fe-293a-40d8-8f85-003d27f5eae1.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/utils/functions.py\", line 35, in create_function_from_source\n",
      "    exec(function_source, ns)\n",
      "  File \"<string>\", line 1, in <module>\n",
      "NameError: name 'stroop_savesubjectinfo' is not defined\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1089, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/utility/wrappers.py\", line 129, in _run_interface\n",
      "    self.imports)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/utils/functions.py\", line 43, in create_function_from_source\n",
      "    raise_from(RuntimeError(msg), e)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/future/utils/__init__.py\", line 398, in raise_from\n",
      "    exec(execstr, myglobals, mylocals)\n",
      "  File \"<string>\", line 1, in <module>\n",
      "RuntimeError: Error executing function\n",
      "stroop_savesubjectinfo\n",
      "Functions in connection strings have to be standalone. They cannot be declared either interactively or inside another function or inline in the connect string. Any imports should be done inside the function.\n",
      "\n",
      "180503-22:36:38,925 workflow INFO:\n",
      "\t Currently running 1 tasks, and 0 jobs ready. Free memory (GB): 28.03/28.23, Free processors: 7/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process NonDaemonPoolWorker-4:\n",
      "Process NonDaemonPoolWorker-8:\n",
      "Process NonDaemonPoolWorker-7:\n",
      "Process NonDaemonPoolWorker-5:\n",
      "Process NonDaemonPoolWorker-1:\n",
      "Process NonDaemonPoolWorker-2:\n",
      "Process NonDaemonPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-04ae5ad48aef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#l1analysis.run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ml1analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MultiProc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'n_procs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/workflows.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, plugin, plugin_args, updatehash)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'create_report'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_report_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdatehash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdatehash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mdatestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%dT%H%M%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'write_provenance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, graph, config, updatehash)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not submitting (max jobs reached)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_sleep_secs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_node_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#l1analysis.run()\n",
    "l1analysis.run('MultiProc', plugin_args={'n_procs': 8})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "243px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
