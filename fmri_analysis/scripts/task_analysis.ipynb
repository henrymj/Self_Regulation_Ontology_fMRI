{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from inspect import currentframe, getframeinfo\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.algorithms.modelgen import SpecifyModel\n",
    "from nipype.interfaces.base import Bunch\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.pipeline.engine import Workflow, Node\n",
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "from utils.event_utils import get_beta_series, get_contrasts, parse_EVs, process_confounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments\n",
    "These are not needed for the jupyter notebook, but are used after conversion to a script for production\n",
    "\n",
    "- conversion command:\n",
    "  - jupyter nbconvert --to script --execute task_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Example BIDS App entrypoint script.')\n",
    "parser.add_argument('-derivatives_dir', default='/derivatives')\n",
    "parser.add_argument('-data_dir', default='/data')\n",
    "parser.add_argument('--participant_label')\n",
    "parser.add_argument('--tasks', nargs=\"+\")\n",
    "parser.add_argument('--skip_beta', action='store_false')\n",
    "parser.add_argument('--skip_contrast', action='store_false')\n",
    "parser.add_argument('--n_procs', default=16)\n",
    "if '-derivatives_dir' in sys.argv or '-h' in sys.argv:\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    args = parser.parse_args([])\n",
    "    args.derivatives_dir = '/mnt/OAK/derivatives'\n",
    "    args.data_dir = '/mnt/OAK'\n",
    "    args.tasks = ['stroop']\n",
    "    args.participant_label = 's611'\n",
    "    args.n_procs=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get current directory to pass to function nodes\n",
    "filename = getframeinfo(currentframe()).filename\n",
    "current_directory = str(Path(filename).resolve().parent)\n",
    "\n",
    "# list of subject identifiers\n",
    "subject_id = args.participant_label\n",
    "# list of task identifiers\n",
    "if args.tasks is not None:\n",
    "    task_list = args.tasks\n",
    "else:\n",
    "    task_list = ['ANT', 'CCTHot', 'discountFix',\n",
    "               'DPX', 'motorSelectiveStop',\n",
    "               'stopSignal', 'stroop', 'surveyMedley',\n",
    "               'twoByTwo', 'WATT3']\n",
    "\n",
    "#### Experiment Variables\n",
    "derivatives_dir = args.derivatives_dir\n",
    "fmriprep_dir = join(derivatives_dir, 'fmriprep', 'fmriprep')\n",
    "data_dir = args.data_dir\n",
    "first_level_dir = join(derivatives_dir,'1stLevel')\n",
    "working_dir = 'workingdir'\n",
    "run_beta = args.skip_beta\n",
    "run_contrast = args.skip_contrast\n",
    "n_procs = args.n_procs\n",
    "# TR of functional images\n",
    "TR = .68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Task List: ['stroop']\n",
      ", Subject: s611\n",
      ", derivatives_dir: /mnt/OAK/derivatives\n",
      ", data_dir: /mnt/OAK\n",
      "Running Contrast?: Yes, Running Beta?: Yes\n",
      "*******************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print('*'*79)\n",
    "print('Task List: %s\\n, Subject: %s\\n, derivatives_dir: %s\\n, data_dir: %s' % \n",
    "     (task_list, subject_id, derivatives_dir, data_dir))\n",
    "print('Running Contrast?: %s, Running Beta?: %s' % \n",
    "     (['No','Yes'][run_contrast], ['No','Yes'][run_beta]))\n",
    "print('*'*79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_events_regressors(data_dir, fmirprep_dir, subject_id, task):\n",
    "    # strip \"sub\" from beginning of subject_id if provided\n",
    "    subject_id = subject_id.replace('sub-','')\n",
    "    ## Get the Confounds File (output of fmriprep)\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    confounds_file = glob(join(fmriprep_dir,\n",
    "                               'sub-%s' % subject_id,\n",
    "                               '*', 'func',\n",
    "                               '*%s*confounds.tsv' % task))[0]\n",
    "    regressors, regressor_names = process_confounds(confounds_file)\n",
    "    ## Get the Events File if it exists\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    event_file = glob(join(data_dir,\n",
    "                           'sub-%s' % subject_id,\n",
    "                           '*', 'func',\n",
    "                           '*%s*events.tsv' % task))   \n",
    "    if len(event_file)>0:\n",
    "        # set up events file\n",
    "        event_file = event_file[0]\n",
    "        events_df = pd.read_csv(event_file,sep = '\\t')\n",
    "    else:\n",
    "        events_df = None\n",
    "    regressors, regressor_names = process_confounds(confounds_file)\n",
    "    return events_df, regressors, regressor_names\n",
    "\n",
    "# helper function to create bunch\n",
    "def getsubjectinfo(events_dr, regressors, regressor_names, task='beta', regress_rt=True): \n",
    "    EV_dict = parse_EVs(events_df, task, regress_rt)\n",
    "    contrasts = []\n",
    "    if task not in ['beta']:\n",
    "        contrasts = get_contrasts(task, regress_rt)\n",
    "    # create beta series info\n",
    "    subjectinfo = Bunch(conditions=EV_dict['conditions'],\n",
    "                        onsets=EV_dict['onsets'],\n",
    "                        durations=EV_dict['durations'],\n",
    "                        amplitudes=EV_dict['amplitudes'],\n",
    "                        tmod=None,\n",
    "                        pmod=None,\n",
    "                        regressor_names=regressor_names,\n",
    "                        regressors=regressors.T.tolist(),\n",
    "                        contrasts=contrasts)\n",
    "    return subjectinfo\n",
    "    \n",
    "def save_subjectinfo(save_directory, subjectinfo):\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    subjectinfo_path = join(save_directory, 'subjectinfo.pkl')\n",
    "    pickle.dump(subjectinfo, open(subjectinfo_path,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Input and Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_selector(task, subject_id, session=None):\n",
    "    if session is None:\n",
    "        ses = '*'\n",
    "    else:\n",
    "        ses = 'ses-%s' % str(session)\n",
    "    # SelectFiles - to grab the data (alternative to DataGrabber)\n",
    "    templates = {'func': join('sub-{subject_id}',ses,'func',\n",
    "                             '*{task}*MNI*preproc.nii.gz'),\n",
    "                 'mask': join('sub-{subject_id}',ses,'func',\n",
    "                              '*{task}*MNI*brainmask.nii.gz')}\n",
    "    selectfiles = Node(SelectFiles(templates,\n",
    "                                   base_directory=fmriprep_dir,\n",
    "                                   sort_filelist=True),\n",
    "                       name='%s_selectFiles' % task)\n",
    "    selectfiles.inputs.task = task\n",
    "    selectfiles.inputs.subject_id = subject_id\n",
    "    return selectfiles\n",
    "\n",
    "def get_masker(name):\n",
    "    # mask and blur\n",
    "    return Node(fsl.maths.ApplyMask(),name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level1model = Node(fsl.FEATModel(), name=\"FEATModel\")\n",
    "level1model.inputs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_common_wf(workflow, task):\n",
    "    # initiate basic nodes\n",
    "    masker = get_masker('%s_masker' % task)\n",
    "    selectfiles = get_selector(task, subject_id)\n",
    "    # Connect up the 1st-level analysis components\n",
    "    workflow.connect([(selectfiles, masker, [('func','in_file'), ('mask', 'mask_file')])])\n",
    "\n",
    "def init_GLM_wf(subject_info, name='wf-standard', contrasts=None):\n",
    "    # Datasink - creates output folder for important outputs\n",
    "    datasink = Node(DataSink(base_directory=first_level_dir,\n",
    "                             container=subject_id), name=\"datasink\")\n",
    "    # Use the following DataSink output substitutions\n",
    "    substitutions = [('_subject_id_', ''),\n",
    "                    ('fstat', 'FSTST'),\n",
    "                    ('run0.mat', 'designfile.mat')]\n",
    "    \n",
    "    datasink.inputs.substitutions = substitutions\n",
    "    # ridiculous regexp substitution to get files just right\n",
    "    # link to ridiculousness: https://regex101.com/r/ljS5zK/3\n",
    "    match_str = \"(?P<sub>s[0-9]+)\\/(?P<task>[A-Za-z_]+)_(?P<model>model-[a-z]+)_(?P<submodel>wf-[a-z]+)\\/(s[0-9]+/|)\"\n",
    "    replace_str = \"\\g<sub>/\\g<task>/\\g<model>/\\g<submodel>/\"\n",
    "    regexp_substitutions = [(match_str, replace_str)]\n",
    "    datasink.inputs.regexp_substitutions = regexp_substitutions\n",
    "    \n",
    "    # SpecifyModel - Generates FSL-specific Model\n",
    "    modelspec = Node(SpecifyModel(input_units='secs',\n",
    "                                  time_repetition=TR,\n",
    "                                  high_pass_filter_cutoff=80),\n",
    "                     name=\"modelspec\")\n",
    "    modelspec.inputs.subject_info = subject_info\n",
    "    # Level1Design - Creates FSL config file \n",
    "    level1design = Node(fsl.Level1Design(bases={'dgamma':{'derivs': True}},\n",
    "                                         interscan_interval=TR,\n",
    "                                         model_serial_correlations=True),\n",
    "                            name=\"level1design\")\n",
    "    level1design.inputs.contrasts=subject_info.contrasts\n",
    "    # FEATmodel generates an FSL design matrix\n",
    "    level1model = Node(fsl.FEATModel(), name=\"FEATModel\")\n",
    "\n",
    "    # FILMGLs\n",
    "    # smooth_autocorr, check default, use FSL default\n",
    "    filmgls = Node(fsl.FILMGLS(), name=\"GLS\")\n",
    "\n",
    "    wf = Workflow(name=name)\n",
    "    wf.connect([(modelspec, level1design, [('session_info','session_info')]),\n",
    "                (level1design, level1model, [('ev_files', 'ev_files'),\n",
    "                                             ('fsf_files','fsf_file')]),\n",
    "                (level1model, datasink, [('design_file', '%s.@design_file' % name)]),\n",
    "                (level1model, filmgls, [('design_file', 'design_file'),\n",
    "                                        ('con_file', 'tcon_file'),\n",
    "                                        ('fcon_file', 'fcon_file')]),\n",
    "                (filmgls, datasink, [('copes', '%s.@copes' % name),\n",
    "                                     ('zstats', '%s.@Z' % name),\n",
    "                                     ('fstats', '%s.@F' % name),\n",
    "                                     ('tstats','%s.@T' % name),\n",
    "                                     ('param_estimates','%s.@param_estimates' % name),\n",
    "                                     ('residual4d', '%s.@residual4d' % name),\n",
    "                                     ('sigmasquareds', '%s.@sigmasquareds' % name)])\n",
    "               ])\n",
    "    return wf\n",
    "\n",
    "\n",
    "\n",
    "def get_task_wfs(task, beta_subjectinfo=None, contrast_subjectinfo=None, regress_rt=True):\n",
    "    rt_suffix = 'rt' if regress_rt==True else 'nort'\n",
    "    # set up workflow lookup\n",
    "    wf_dict = {'contrast': (init_GLM_wf, {'name': '%s_model-%s_wf-contrast' % (task, rt_suffix)}), \n",
    "               'beta': (init_GLM_wf, {'name': '%s_model-%s_wf-beta' % (task, rt_suffix)})}\n",
    "    \n",
    "    workflows = []\n",
    "    if beta_subjectinfo:\n",
    "        save_directory = join(first_level_dir, subject_id, task, 'model-%s' % rt_suffix, 'wf-beta')\n",
    "        save_subjectinfo(save_directory, beta_subjectinfo)\n",
    "        func, kwargs = wf_dict['beta']\n",
    "        workflows.append(func(beta_subjectinfo, **kwargs))\n",
    "    if contrast_subjectinfo:\n",
    "        save_directory = join(first_level_dir, subject_id, task, 'model-%s' % rt_suffix, 'wf-contrast')\n",
    "        save_subjectinfo(save_directory, contrast_subjectinfo)\n",
    "        func, kwargs = wf_dict['contrast']\n",
    "        workflows.append(func(contrast_subjectinfo, **kwargs))\n",
    "    return workflows\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiation of the 1st-level analysis workflow\n",
    "l1analysis = Workflow(name='%s_l1analysis' % subject_id)\n",
    "l1analysis.base_dir = join(derivatives_dir, working_dir)\n",
    "\n",
    "for task in task_list:\n",
    "    init_common_wf(l1analysis, task)\n",
    "    # get nodes to pass\n",
    "    masker = l1analysis.get_node('%s_masker' % task)\n",
    "    # get info to pass to task workflows\n",
    "    events_df, regressors, regressor_names = get_events_regressors(data_dir, fmriprep_dir,\n",
    "                                                                   subject_id, task)\n",
    "    # perform analyses both by regressing rt and not\n",
    "    regress_rt_conditions = [True, False]\n",
    "    if 'stop' in task:\n",
    "        regress_rt_conditions = [False]\n",
    "    betainfo = None; contrastinfo = None\n",
    "    for regress_rt in regress_rt_conditions:\n",
    "        if run_beta:\n",
    "            betainfo = getsubjectinfo(events_df, regressors, regressor_names, task='beta', regress_rt=regress_rt)\n",
    "        if run_contrast:\n",
    "            contrastinfo = getsubjectinfo(events_df, regressors, regressor_names, task=task, regress_rt=regress_rt)\n",
    "        task_workflows = get_task_wfs(task, betainfo, contrastinfo, regress_rt)\n",
    "        for wf in task_workflows:\n",
    "            l1analysis.connect([\n",
    "                                (masker, wf, [('out_file', 'modelspec.functional_runs')]),\n",
    "                                (masker, wf, [('out_file','GLS.in_file')])\n",
    "                                ])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180510-17:31:35,142 workflow INFO:\n",
      "\t Workflow s611_l1analysis settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "180510-17:31:35,294 workflow INFO:\n",
      "\t Running in parallel.\n",
      "180510-17:31:35,298 workflow INFO:\n",
      "\t Currently running 0 tasks, and 1 jobs ready. Free memory (GB): 10.45/10.45, Free processors: 4/4\n",
      "180510-17:31:35,308 workflow INFO:\n",
      "\t Executing node s611_l1analysis.stroop_selectFiles in dir: /mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_selectFiles\n",
      "180510-17:31:35,437 workflow INFO:\n",
      "\t Running node \"stroop_selectFiles\" (\"nipype.interfaces.io.SelectFiles\").\n",
      "180510-17:31:37,310 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_selectFiles jobid: 0\n",
      "180510-17:31:37,313 workflow INFO:\n",
      "\t Currently running 0 tasks, and 1 jobs ready. Free memory (GB): 10.45/10.45, Free processors: 4/4\n",
      "180510-17:31:37,337 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_masker jobid: 1\n",
      "180510-17:31:39,344 workflow INFO:\n",
      "\t Currently running 0 tasks, and 4 jobs ready. Free memory (GB): 10.45/10.45, Free processors: 4/4\n",
      "180510-17:31:39,362 workflow INFO:\n",
      "\t [Job finished] jobname: modelspec jobid: 2\n",
      "180510-17:31:39,384 workflow INFO:\n",
      "\t [Job finished] jobname: modelspec jobid: 7\n",
      "180510-17:31:39,412 workflow INFO:\n",
      "\t [Job finished] jobname: modelspec jobid: 12\n",
      "180510-17:31:39,441 workflow INFO:\n",
      "\t [Job finished] jobname: modelspec jobid: 17\n",
      "180510-17:31:43,636 workflow INFO:\n",
      "\t [Job finished] jobname: level1design jobid: 3\n",
      "180510-17:31:43,746 workflow INFO:\n",
      "\t [Job finished] jobname: level1design jobid: 8\n",
      "180510-17:31:43,861 workflow INFO:\n",
      "\t [Job finished] jobname: level1design jobid: 13\n",
      "180510-17:31:43,965 workflow INFO:\n",
      "\t [Job finished] jobname: level1design jobid: 18\n",
      "180510-17:31:46,0 workflow INFO:\n",
      "\t Executing node s611_l1analysis.stroop_model-rt_wf-beta.FEATModel in dir: /mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-rt_wf-beta/FEATModel\n",
      "180510-17:31:46,19 workflow INFO:\n",
      "\t Executing node s611_l1analysis.stroop_model-rt_wf-contrast.FEATModel in dir: /mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-rt_wf-contrast/FEATModel\n",
      "180510-17:31:46,81 workflow INFO:\n",
      "\t Executing node s611_l1analysis.stroop_model-nort_wf-beta.FEATModel in dir: /mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-nort_wf-beta/FEATModel\n",
      "180510-17:31:46,162 workflow INFO:\n",
      "\t Executing node s611_l1analysis.stroop_model-nort_wf-contrast.FEATModel in dir: /mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-nort_wf-contrast/FEATModel\n",
      "180510-17:31:46,421 workflow INFO:\n",
      "\t Running node \"FEATModel\" (\"nipype.interfaces.fsl.model.FEATModel\"), a CommandLine Interface with command:\n",
      "feat_model run0 .\n",
      "180510-17:31:46,480 workflow INFO:\n",
      "\t Running node \"FEATModel\" (\"nipype.interfaces.fsl.model.FEATModel\"), a CommandLine Interface with command:\n",
      "feat_model run0 .\n",
      "180510-17:31:47,176 workflow INFO:\n",
      "\t Running node \"FEATModel\" (\"nipype.interfaces.fsl.model.FEATModel\"), a CommandLine Interface with command:\n",
      "feat_model run0 .\n",
      "180510-17:31:47,272 workflow INFO:\n",
      "\t Running node \"FEATModel\" (\"nipype.interfaces.fsl.model.FEATModel\"), a CommandLine Interface with command:\n",
      "feat_model run0 .\n",
      "180510-17:31:48,164 workflow INFO:\n",
      "\t Currently running 4 tasks, and 0 jobs ready. Free memory (GB): 9.65/10.45, Free processors: 0/4\n",
      "180510-17:31:50,168 workflow INFO:\n",
      "\t [Job finished] jobname: FEATModel jobid: 9\n",
      "180510-17:31:50,170 workflow INFO:\n",
      "\t [Job finished] jobname: FEATModel jobid: 19\n",
      "180510-17:31:50,181 workflow INFO:\n",
      "\t Currently running 2 tasks, and 2 jobs ready. Free memory (GB): 10.05/10.45, Free processors: 2/4\n",
      "180510-17:31:50,220 workflow INFO:\n",
      "\t Executing node s611_l1analysis.stroop_model-rt_wf-contrast.GLS in dir: /mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-rt_wf-contrast/GLS\n",
      "180510-17:31:50,268 workflow INFO:\n",
      "\t Executing node s611_l1analysis.stroop_model-nort_wf-contrast.GLS in dir: /mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-nort_wf-contrast/GLS\n",
      "180510-17:31:50,323 workflow INFO:\n",
      "\t Running node \"GLS\" (\"nipype.interfaces.fsl.model.FILMGLS\"), a CommandLine Interface with command:\n",
      "film_gls --rn=results --con=/mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-rt_wf-contrast/FEATModel/run0.con --in=/mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_masker/sub-s611_ses-2_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz --pd=/mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-rt_wf-contrast/FEATModel/run0.mat --thr=0.000000.\n",
      "180510-17:31:50,388 workflow INFO:\n",
      "\t Running node \"GLS\" (\"nipype.interfaces.fsl.model.FILMGLS\"), a CommandLine Interface with command:\n",
      "film_gls --rn=results --con=/mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-nort_wf-contrast/FEATModel/run0.con --in=/mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_masker/sub-s611_ses-2_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz --pd=/mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-nort_wf-contrast/FEATModel/run0.mat --thr=0.000000.\n",
      "180510-17:31:52,270 workflow INFO:\n",
      "\t Currently running 4 tasks, and 0 jobs ready. Free memory (GB): 9.65/10.45, Free processors: 0/4\n",
      "180510-17:35:30,744 workflow INFO:\n",
      "\t [Job finished] jobname: FEATModel jobid: 4\n",
      "180510-17:35:30,756 workflow INFO:\n",
      "\t [Job finished] jobname: FEATModel jobid: 14\n",
      "180510-17:35:30,761 workflow INFO:\n",
      "\t Currently running 2 tasks, and 2 jobs ready. Free memory (GB): 10.05/10.45, Free processors: 2/4\n",
      "180510-17:35:30,786 workflow INFO:\n",
      "\t Executing node s611_l1analysis.stroop_model-rt_wf-beta.GLS in dir: /mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-rt_wf-beta/GLS\n",
      "180510-17:35:30,818 workflow INFO:\n",
      "\t Executing node s611_l1analysis.stroop_model-nort_wf-beta.GLS in dir: /mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-nort_wf-beta/GLS\n",
      "180510-17:35:30,907 workflow INFO:\n",
      "\t Running node \"GLS\" (\"nipype.interfaces.fsl.model.FILMGLS\"), a CommandLine Interface with command:\n",
      "film_gls --rn=results --con=/mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-rt_wf-beta/FEATModel/run0.con --in=/mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_masker/sub-s611_ses-2_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz --pd=/mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-rt_wf-beta/FEATModel/run0.mat --thr=0.000000.\n",
      "180510-17:35:30,936 workflow INFO:\n",
      "\t Running node \"GLS\" (\"nipype.interfaces.fsl.model.FILMGLS\"), a CommandLine Interface with command:\n",
      "film_gls --rn=results --con=/mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-nort_wf-beta/FEATModel/run0.con --in=/mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_masker/sub-s611_ses-2_task-stroop_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz --pd=/mnt/OAK/derivatives/workingdir/s611_l1analysis/stroop_model-nort_wf-beta/FEATModel/run0.mat --thr=0.000000.\n",
      "180510-17:35:32,821 workflow INFO:\n",
      "\t Currently running 4 tasks, and 0 jobs ready. Free memory (GB): 9.65/10.45, Free processors: 0/4\n",
      "180510-17:35:53,251 root ERROR:\n",
      "\t Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-29-e17d545c6a1d>\", line 2, in <module>\n",
      "    l1analysis.run('MultiProc', plugin_args={'n_procs': n_procs})\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/workflows.py\", line 592, in run\n",
      "    runner.run(execgraph, updatehash=updatehash, config=self.config)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/base.py\", line 186, in run\n",
      "    sleep(poll_sleep_secs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1821, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process NonDaemonPoolWorker-9:\n",
      "Process NonDaemonPoolWorker-13:\n",
      "Process NonDaemonPoolWorker-10:\n",
      "Process NonDaemonPoolWorker-11:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process NonDaemonPoolWorker-12:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process NonDaemonPoolWorker-16:\n",
      "Process NonDaemonPoolWorker-21:\n",
      "Process NonDaemonPoolWorker-18:\n",
      "Process NonDaemonPoolWorker-15:\n",
      "Process NonDaemonPoolWorker-20:\n",
      "Process NonDaemonPoolWorker-17:\n",
      "Traceback (most recent call last):\n",
      "Process NonDaemonPoolWorker-14:\n",
      "Process NonDaemonPoolWorker-19:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180510-17:35:53,251 root ERROR:\n",
      "\t Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#l1analysis.run()\n",
    "l1analysis.run('MultiProc', plugin_args={'n_procs': n_procs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "243px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
