{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "180427-09:41:13,671 duecredit ERROR:\n",
      "\t Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import argparse\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.algorithms.modelgen import SpecifyModel\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.pipeline.engine import Workflow, Node\n",
    "from os.path import join\n",
    "from utils.event_utils import move_EVs\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments\n",
    "These are not needed for the jupyter notebook, but are used after conversion to a script for production\n",
    "\n",
    "- conversion command:\n",
    "  - jupyter nbconvert --to script --execute task_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Example BIDS App entrypoint script.')\n",
    "parser.add_argument('-derivatives_dir', default='/output')\n",
    "parser.add_argument('-data_dir', default='/data')\n",
    "parser.add_argument('--participant_label',nargs=\"+\")\n",
    "parser.add_argument('--events_dir', default=None)\n",
    "parser.add_argument('--tasks', nargs=\"+\")\n",
    "parser.add_argument('--use_events', action='store_false')\n",
    "parser.add_argument('--ignore_rt', action='store_true')\n",
    "parser.add_argument('--cleanup', action='store_true')\n",
    "parser.add_argument('--overwrite_event', action='store_true')\n",
    "if '-output_dir' in sys.argv:\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.derivatives_dir = '/home/ian/tmp/fmri/derivatives/'\n",
    "args.data_dir = '/home/ian/tmp/fmri/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of subject identifiers\n",
    "subject_list = args.participant_label\n",
    "# list of task identifiers\n",
    "if args.tasks:\n",
    "    task_list = args.tasks\n",
    "else:\n",
    "  task_list = ['ANT', 'CCTHot', 'discountFix',\n",
    "               'DPX', 'motorSelectiveStop',\n",
    "               'stopSignal', 'stroop', 'surveyMedley',\n",
    "               'twoByTwo', 'WATT3']\n",
    "\n",
    "regress_rt = not args.ignore_rt\n",
    "use_events = args.use_events\n",
    "#### Experiment Variables\n",
    "output_dir = args.output_dir\n",
    "events_dir = args.events_dir\n",
    "data_dir = args.data_dir\n",
    "first_level_dir = '1stLevel'\n",
    "working_dir = 'workingdir'\n",
    "# TR of functional images\n",
    "TR = .68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function to create bunch\n",
    "def getsubjectinfo(data_dir, fmriprep_dir, subject_id, task, regress_rt=True): \n",
    "    from glob import glob\n",
    "    from os.path import join\n",
    "    import pandas as pd\n",
    "    from nipype.interfaces.base import Bunch\n",
    "    from utils.event_utils import get_contrasts, parse_EVs, process_confounds\n",
    "    # strip \"sub\" from beginning of subject_id if provided\n",
    "    subject_id = subject_id.replace('sub-','')\n",
    "    ## Get the Confounds File (output of fmriprep)\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    confounds_file = glob(join(fmriprep_dir,\n",
    "                               'sub-%s' % subject_id,\n",
    "                               '*', 'func',\n",
    "                               '*%s*confounds.tsv' % task))[0]\n",
    "    regressors, regressor_names = process_confounds(confounds_file)\n",
    "    ## Get the Events File if it exists\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    event_file = glob(join(data_dir,\n",
    "                           'sub-%s' % subject_id,\n",
    "                           '*', 'func',\n",
    "                           '*%s*events.tsv' % task))\n",
    "    if len(event_file)>0:\n",
    "        event_file = event_file[0]\n",
    "        events_df = pd.read_csv(event_file,sep = '\\t')\n",
    "        # set up contrasts\n",
    "        EV_dict = parse_EVs(events_df, task, regress_rt)\n",
    "        contrast_subjectinfo = Bunch(subject_id=subject_id,\n",
    "                                      task=task,\n",
    "                                      conditions=EV_dict['conditions'],\n",
    "                                      onsets=EV_dict['onsets'],\n",
    "                                      durations=EV_dict['durations'],\n",
    "                                      amplitudes=EV_dict['amplitudes'],\n",
    "                                      tmod=None,\n",
    "                                      pmod=None,\n",
    "                                      regressor_names=regressor_names,\n",
    "                                      regressors=regressors.T.tolist())\n",
    "        contrasts = get_contrasts(task, regress_rt)\n",
    "    else:\n",
    "        contrast_subjectinfo = Bunch()\n",
    "        contrasts=[]\n",
    "    # create base_subject info\n",
    "    base_subjectinfo = Bunch(subject_id=subject_id,\n",
    "                             task=task,\n",
    "                             tmod=None,\n",
    "                             pmod=None,\n",
    "                             regressor_names=regressor_names,\n",
    "                             regressors=regressors.T.tolist())\n",
    "\n",
    "    return contrast_subjectinfo, base_subjectinfo, contrasts # this output will later be returned to infosource\n",
    "\n",
    "def save_subjectinfo(base_directory, subject_id, task, subjectinfo, contrasts):\n",
    "    from os import makedirs\n",
    "    from os.path import join\n",
    "    import pickle\n",
    "    task_dir = join(base_directory, subject_id + '_task_' + task)\n",
    "    makedirs(task_dir, exist_ok=True)\n",
    "    subjectinfo_path = join(task_dir,'subjectinfo.pkl')\n",
    "    pickle.dump(subjectinfo, open(subjectinfo_path,'wb'))\n",
    "    if len(contrasts) > 0:\n",
    "        contrast_path = join(task_dir,'contrasts.pkl')\n",
    "        pickle.dump(contrasts, open(contrast_path,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View one events file used in subject info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Input and Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get Subject Info - get subject specific condition information\n",
    "subjectinfo = Node(Function(input_names=['data_dir', 'fmiriprep_dir',\n",
    "                                         'subject_id', 'task','regress_rt'],\n",
    "                               output_names=['contrast_subjectinfo', \n",
    "                                             'base_subjectinfo',\n",
    "                                             'contrasts'],\n",
    "                               function=getsubjectinfo),\n",
    "                      name='getsubjectinfo')\n",
    "subjectinfo.inputs.fmriprep_dir = fmriprep_dir\n",
    "subjectinfo.inputs.data_dir = data_dir\n",
    "subjectinfo.inputs.regress_rt = regress_rt\n",
    "\n",
    "# Infosource - a function free node to iterate over the list of subject names\n",
    "infosource = Node(IdentityInterface(fields=['subject_id',\n",
    "                                            'task']),\n",
    "                  name=\"infosource\")\n",
    "infosource.iterables = [('subject_id', subject_list),\n",
    "                        ('task', task_list)]\n",
    "# SelectFiles - to grab the data (alternative to DataGrabber)\n",
    "templates = {'func': join('*{subject_id}','*','func',\n",
    "                         '*{task}*MNI*preproc.nii.gz'),\n",
    "            'mask': join('*{subject_id}','*','func',\n",
    "                         '*{task}*MNI*brainmask.nii.gz')}\n",
    "selectfiles = Node(SelectFiles(templates,\n",
    "                               base_directory = data_dir,\n",
    "                               sort_filelist=True),\n",
    "                   name=\"selectfiles\")\n",
    "# Datasink - creates output folder for important outputs\n",
    "datasink = Node(DataSink(base_directory = output_dir,\n",
    "                         container=first_level_dir),\n",
    "                name=\"datasink\")\n",
    "# Save python objects that aren't accomodated by datasink nodes\n",
    "save_subjectinfo = Node(Function(input_names=['base_directory','subject_id',\n",
    "                                              'task','subjectinfo','contrasts'],\n",
    "                                 output_names=['output_path'],\n",
    "                                function=save_subjectinfo),\n",
    "                       name=\"savesubjectinfo\")\n",
    "save_subjectinfo.inputs.base_directory = join(output_dir,first_level_dir)\n",
    "\n",
    "# Use the following DataSink output substitutions\n",
    "substitutions = [('_subject_id_', ''),\n",
    "                ('fstat', 'FSTST'),\n",
    "                ('run0.mat', 'designfile.mat')]\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mask and blur\n",
    "masker = Node(fsl.maths.ApplyMask(),name='masker')\n",
    "\n",
    "# SpecifyModel - Generates FSL-specific Model\n",
    "modelspec = Node(SpecifyModel(input_units='secs',\n",
    "                              time_repetition=TR,\n",
    "                              high_pass_filter_cutoff=80),\n",
    "                 name=\"modelspec\")\n",
    "# Level1Design - Creates FSL config file \n",
    "level1design = Node(fsl.Level1Design(bases={'dgamma':{'derivs': True}},\n",
    "                                     interscan_interval=TR,\n",
    "                                     model_serial_correlations=True),\n",
    "                        name=\"level1design\")\n",
    "# FEATmodel generates an FSL design matrix\n",
    "level1model = Node(fsl.FEATModel(), name=\"FEATModel\")\n",
    "\n",
    "# FILMGLs\n",
    "# smooth_autocorr, check default, use FSL default\n",
    "filmgls = Node(fsl.FILMGLS(), name=\"GLS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wf(name='wf'):\n",
    "  wf = Workflow(name=name)\n",
    "  wf.connect([(modelspec, level1design, [('session_info','session_info')]),\n",
    "              (level1design, level1model, [('ev_files', 'ev_files'),\n",
    "                                             ('fsf_files','fsf_file')]),\n",
    "              (level1model, filmgls, [('design_file', 'design_file'),\n",
    "                                        ('con_file', 'tcon_file'),\n",
    "                                        ('fcon_file', 'fcon_file')]),\n",
    "              (level1model, datasink, [('design_file', '@design_file')]),\n",
    "              (filmgls, datasink, [('copes', '%s.@copes' % name),\n",
    "                                    ('zstats', '%s.@Z' % name),\n",
    "                                    ('fstats', '%s.@F' % name),\n",
    "                                    ('tstats','%s.@T' % name),\n",
    "                                    ('param_estimates','%s.@param_estimates' % name),\n",
    "                                    ('residual4d', '%s.@residual4d'),\n",
    "                                    ('sigmasquareds', '%s.@sigmasquareds' % name)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow for calculating contrasts\n",
    "wf1 = init_wf(name='wf1')\n",
    "# workflow for only removing nuisance variables\n",
    "wf2 = init_wf(name='wf2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiation of the 1st-level analysis workflow\n",
    "l1analysis = Workflow(name='l1analysis')\n",
    "l1analysis.base_dir = join(output_dir, working_dir)\n",
    "# Connect up the 1st-level analysis components\n",
    "l1analysis.connect([(infosource, selectfiles, [('subject_id', 'subject_id'),\n",
    "                                               ('task', 'task')]),\n",
    "                    (infosource, subjectinfo, [('subject_id','subject_id'),\n",
    "                                                 ('task', 'task')]),\n",
    "                    (subjectinfo, save_subjectinfo, [('subject_id','subject_id'),\n",
    "                                                        ('task','tasj'),\n",
    "                                                        ('subjectinfo','subjectinfo'),\n",
    "                                                        ('contrasts','contrasts')]),\n",
    "                    (selectfiles, masker, [('func','in_file'),\n",
    "                                           ('mask', 'mask_file')])\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180427-09:42:21,609 workflow INFO:\n",
      "\t Workflow l1analysis settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "180427-09:42:21,615 workflow INFO:\n",
      "\t Running serially.\n",
      "180427-09:42:21,616 workflow INFO:\n",
      "\t Executing node l1analysis.selectfiles in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130_task_stroop/selectfiles\n",
      "180427-09:42:21,620 workflow INFO:\n",
      "\t Running node \"selectfiles\" (\"nipype.interfaces.io.SelectFiles\").\n",
      "180427-09:42:21,627 workflow INFO:\n",
      "\t Executing node l1analysis.getsubjectinfo in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130_task_stroop/getsubjectinfo\n",
      "180427-09:42:21,631 workflow INFO:\n",
      "\t Running node \"getsubjectinfo\" (\"nipype.interfaces.utility.wrappers.Function\").\n",
      "180427-09:42:21,640 workflow ERROR:\n",
      "\t Node getsubjectinfo.a0 failed to run on host ian-System-Product-Name.\n",
      "180427-09:42:21,641 workflow ERROR:\n",
      "\t Saving crash info to /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180427-094221-ian-getsubjectinfo.a0-8327cf08-2f29-4463-9b41-f6a411a7d7a0.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/linear.py\", line 43, in run\n",
      "    node.run(updatehash=updatehash)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1089, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/utility/wrappers.py\", line 137, in _run_interface\n",
      "    out = function_handle(**args)\n",
      "  File \"<string>\", line 21, in getsubjectinfo\n",
      "IndexError: list index out of range\n",
      "\n",
      "180427-09:42:21,643 workflow INFO:\n",
      "\t ***********************************\n",
      "180427-09:42:21,643 workflow ERROR:\n",
      "\t could not run node: l1analysis.getsubjectinfo.a0\n",
      "180427-09:42:21,644 workflow INFO:\n",
      "\t crashfile: /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180427-094221-ian-getsubjectinfo.a0-8327cf08-2f29-4463-9b41-f6a411a7d7a0.pklz\n",
      "180427-09:42:21,644 workflow INFO:\n",
      "\t ***********************************\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Workflow did not execute cleanly. Check log for details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b60ef6d9af71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml1analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/workflows.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, plugin, plugin_args, updatehash)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'create_report'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_report_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdatehash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdatehash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mdatestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%dT%H%M%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'write_provenance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/linear.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, graph, config, updatehash)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_callback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exception'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mreport_nodes_not_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/tools.py\u001b[0m in \u001b[0;36mreport_nodes_not_run\u001b[0;34m(notrun)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***********************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         raise RuntimeError(('Workflow did not execute cleanly. '\n\u001b[0m\u001b[1;32m     80\u001b[0m                             'Check log for details'))\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Workflow did not execute cleanly. Check log for details"
     ]
    }
   ],
   "source": [
    "l1analysis.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add on contrast wf\n",
    "l1analysis.connect([\n",
    "                    (subjectinfo, wf1, [('contrast_subjectinfo','modelspec.subject_info')]),\n",
    "                    (masker, wf1, [('out_file', 'modelspec.functional_runs')]),\n",
    "                    (subjectinfo, wf1, [('contrasts','level1design.contrasts')]),\n",
    "                    (masker, wf1, [('out_file','GLS.in_file')])\n",
    "                    ])\n",
    "# add on residual wf\n",
    "l1analysis.connect([\n",
    "                    (subjectinfo, wf2, [('base_subjectinfo','modelspec.subject_info')]),\n",
    "                    (masker, wf2, [('out_file', 'modelspec.functional_runs')]),\n",
    "                    (masker, wf2, [('out_file','GLS.in_file')])\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1analysis.run('MultiProc', plugin_args={'n_procs': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l1analysis.run('MultiProc', plugin_args={'n_procs': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Workflow"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "source": [
    "# Create 1st-level analysis output graph\n",
    "l1analysis.write_graph(graph2use='colored', format='png', simple_form=True)\n",
    "# Visualize the graph\n",
    "from IPython.display import Image\n",
    "graph_file=join(l1analysis.base_dir, 'l1analysis', 'graph.dot.png')\n",
    "Image(graph_file)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "try:\n",
    "    shutil.move(graph_file, 'l1analysis')\n",
    "except shutil.Error:\n",
    "    pass\n",
    "# remove working directory\n",
    "shutil.rmtree(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Design Matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "!tree datasink/1stLevel/sub*\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "design_file = glob(join(experiment_dir,'datasink','1stLevel','sub-s358_task_CCTHot','designfile.mat'))[0]\n",
    "desmtx=numpy.loadtxt(design_file, skiprows=5)\n",
    "plt.imshow(desmtx,aspect='auto',interpolation='nearest',cmap='gray')\n",
    "plt.title('Design Matrix')\n",
    "plt.show()\n",
    "cc=numpy.corrcoef(desmtx.T)\n",
    "plt.imshow(cc,aspect='auto',interpolation='nearest', cmap=plt.cm.viridis)\n",
    "plt.colorbar()\n",
    "plt.title('Correlation of Regressors')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# show first regressors\n",
    "plt.figure(figsize=[20,8])\n",
    "plt.plot(desmtx[:,0],'b', label=subject_info.conditions[0])\n",
    "plt.plot(desmtx[:,2],'r', label=subject_info.conditions[1])\n",
    "for i in subject_info.onsets[0]:\n",
    "    plt.axvline(i/.68, ymin=0, ymax=.1)\n",
    "for i in subject_info.onsets[1]:\n",
    "    plt.axvline(i/.68, ymin=0, ymax=.1, c='r')\n",
    "plt.legend()\n",
    "plt.title('Congruent/Incongruent Regressors')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "from nilearn.plotting import plot_stat_map\n",
    "from nilearn.image import smooth_img\n",
    "\n",
    "anatimg = glob(join(experiment_dir,'Data','sub-s358','anat','*T1w*MNI*preproc*'))[0]\n",
    "contrast_img = glob(join(experiment_dir,'datasink','1stLevel','sub-s358_task_ANT','zstat3.nii.gz'))[0]\n",
    "plot_stat_map(smooth_img(contrast_img, 8), title='stroop effect',\n",
    "              bg_img=anatimg, threshold=1, display_mode='z', cut_coords=(-30, -15, 0, 15, 30), dim=-1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "source": [
    "import nilearn.plotting\n",
    "import nilearn.image\n",
    "import cPickle\n",
    "\n",
    "for task in task_list:\n",
    "    contrast_file = glob(join(experiment_dir,'datasink','1stLevel','sub-s358_task_' + task,'contrasts.pkl'))[0]\n",
    "    contrasts = cPickle.load(open(contrast_file,'r'))\n",
    "    contrasts = [contrast[0] for contrast in contrasts]\n",
    "    contrast_imgs = sort(glob(join(experiment_dir,'datasink','1stLevel','sub-s358_task_' + task,'zstat*.nii.gz')))\n",
    "    for i, contrast_img in enumerate(contrast_imgs):\n",
    "        nilearn.plotting.plot_glass_brain(nilearn.image.smooth_img(contrast_img, 8),\n",
    "                                          display_mode='lyrz', colorbar=True, plot_abs=False, threshold=0,\n",
    "                                          title=contrasts[i])\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "243px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
