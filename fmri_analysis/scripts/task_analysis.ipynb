{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180509-00:55:47,935 duecredit ERROR:\n",
      "\t Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from inspect import currentframe, getframeinfo\n",
    "from pathlib import Path\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.algorithms.modelgen import SpecifyModel\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.pipeline.engine import Workflow, Node\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "from utils.event_utils import get_contrasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments\n",
    "These are not needed for the jupyter notebook, but are used after conversion to a script for production\n",
    "\n",
    "- conversion command:\n",
    "  - jupyter nbconvert --to script --execute task_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Example BIDS App entrypoint script.')\n",
    "parser.add_argument('-derivatives_dir', default='/derivatives')\n",
    "parser.add_argument('-data_dir', default='/data')\n",
    "parser.add_argument('--participant_labels',nargs=\"+\")\n",
    "parser.add_argument('--tasks', nargs=\"+\")\n",
    "parser.add_argument('--ignore_rt', action='store_true')\n",
    "parser.add_argument('--cleanup', action='store_true')\n",
    "if '-derivatives_dir' in sys.argv or '-h' in sys.argv:\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    args = parser.parse_args([])\n",
    "    args.derivatives_dir = '/home/ian/tmp/fmri/derivatives/'\n",
    "    args.data_dir = '/mnt/OAK'\n",
    "    args.tasks = ['twoByTwo']\n",
    "    args.participant_labels = ['s130']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get current directory to pass to function nodes\n",
    "filename = getframeinfo(currentframe()).filename\n",
    "current_directory = str(Path(filename).resolve().parent)\n",
    "\n",
    "# list of subject identifiers\n",
    "subject_list = args.participant_labels\n",
    "# list of task identifiers\n",
    "if args.tasks is not None:\n",
    "    task_list = args.tasks\n",
    "else:\n",
    "    task_list = ['ANT', 'CCTHot', 'discountFix',\n",
    "               'DPX', 'motorSelectiveStop',\n",
    "               'stopSignal', 'stroop', 'surveyMedley',\n",
    "               'twoByTwo', 'WATT3']\n",
    "\n",
    "regress_rt = not args.ignore_rt\n",
    "#### Experiment Variables\n",
    "derivatives_dir = args.derivatives_dir\n",
    "fmriprep_dir = join(derivatives_dir, 'fmriprep', 'fmriprep')\n",
    "data_dir = args.data_dir\n",
    "first_level_dir = join(derivatives_dir,'1stLevel')\n",
    "working_dir = 'workingdir'\n",
    "# TR of functional images\n",
    "TR = .68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Task List: ['twoByTwo']\n",
      ", Subjects: ['s130']\n",
      ", derivatives_dir: /home/ian/tmp/fmri/derivatives/\n",
      ", data_dir: /mnt/OAK\n",
      "*******************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print('*'*79)\n",
    "print('Task List: %s\\n, Subjects: %s\\n, derivatives_dir: %s\\n, data_dir: %s' % \n",
    "     (task_list, subject_list, derivatives_dir, data_dir))\n",
    "print('*'*79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function to create bunch\n",
    "def getsubjectinfo(data_dir, fmriprep_dir, subject_id, task, regress_rt, utils_path): \n",
    "    from glob import glob\n",
    "    from os.path import join\n",
    "    import pandas as pd\n",
    "    from nipype.interfaces.base import Bunch\n",
    "    import sys\n",
    "    sys.path.append(utils_path)\n",
    "    from utils.event_utils import get_beta_series, parse_EVs, process_confounds\n",
    "    # strip \"sub\" from beginning of subject_id if provided\n",
    "    subject_id = subject_id.replace('sub-','')\n",
    "    ## Get the Confounds File (output of fmriprep)\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    confounds_file = glob(join(fmriprep_dir,\n",
    "                               'sub-%s' % subject_id,\n",
    "                               '*', 'func',\n",
    "                               '*%s*confounds.tsv' % task))[0]\n",
    "    regressors, regressor_names = process_confounds(confounds_file)\n",
    "    ## Get the Events File if it exists\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    event_file = glob(join(data_dir,\n",
    "                           'sub-%s' % subject_id,\n",
    "                           '*', 'func',\n",
    "                           '*%s*events.tsv' % task))\n",
    "    beta_subjectinfo = None\n",
    "    contrast_subjectinfo = None\n",
    "    contrast = None\n",
    "    if len(event_file)>0:\n",
    "        # set up events file\n",
    "        event_file = event_file[0]\n",
    "        events_df = pd.read_csv(event_file,sep = '\\t')\n",
    "        # create beta series info\n",
    "        beta_dict = get_beta_series(events_df, regress_rt)\n",
    "        beta_subjectinfo = Bunch(subject_id=subject_id,\n",
    "                                 task=task,\n",
    "                                 conditions=beta_dict['conditions'],\n",
    "                                 onsets=beta_dict['onsets'],\n",
    "                                 durations=beta_dict['durations'],\n",
    "                                 amplitudes=beta_dict['amplitudes'],\n",
    "                                 tmod=None,\n",
    "                                 pmod=None,\n",
    "                                 regressor_names=regressor_names,\n",
    "                                 regressors=regressors.T.tolist())\n",
    "        # set up contrasts\n",
    "        EV_dict = parse_EVs(events_df, task, regress_rt)\n",
    "        contrast_subjectinfo = Bunch(subject_id=subject_id,\n",
    "                                     task=task,\n",
    "                                     conditions=EV_dict['conditions'],\n",
    "                                     onsets=EV_dict['onsets'],\n",
    "                                     durations=EV_dict['durations'],\n",
    "                                     amplitudes=EV_dict['amplitudes'],\n",
    "                                     tmod=None,\n",
    "                                     pmod=None,\n",
    "                                     regressor_names=regressor_names,\n",
    "                                     regressors=regressors.T.tolist())\n",
    "    return beta_subjectinfo, contrast_subjectinfo\n",
    "    \n",
    "def save_subjectinfo(save_directory, beta_subjectinfo, contrast_subjectinfo, contrasts=[]):\n",
    "    from os import makedirs\n",
    "    from os.path import join\n",
    "    import pickle\n",
    "    pickle.dump('hey', open('/home/ian/tmp/test2.pkl', 'wb'))\n",
    "    pickle.dump([beta_subjectinfo, contrast_subjectinfo], open('/home/ian/tmp/test.pkl', 'wb'))\n",
    "    subject_id = beta_subjectinfo.subject_id\n",
    "    task = beta_subjectinfo.task\n",
    "    subjectinfo_dir = join(save_directory, subject_id, '%s_subject_info' % task)\n",
    "    makedirs(subjectinfo_dir, exist_ok=True)\n",
    "    # save beta subject info\n",
    "    beta_path = join(subjectinfo_dir,'beta_subjectinfo.pkl')\n",
    "    pickle.dump(beta_subjectinfo, open(beta_path,'wb'))\n",
    "    # save contrast subject info\n",
    "    if len(contrast_subjectinfo.items()) > 0:\n",
    "        contrast_path = join(subjectinfo_dir,'contrast_subjectinfo.pkl')\n",
    "        pickle.dump(contrast_subjectinfo, open(contrast_path,'wb'))\n",
    "    # save contrast list\n",
    "    if len(contrasts) > 0:\n",
    "        contrastlist_path = join(subjectinfo_dir,'contrasts.pkl')\n",
    "        pickle.dump(contrasts, open(contrastlist_path,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View one events file used in subject info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Input and Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_subjectinfo(name):\n",
    "    # Get Subject Info - get subject specific condition information\n",
    "    subjectinfo = Node(Function(input_names=['data_dir', 'fmriprep_dir','subject_id', \n",
    "                                             'task','regress_rt', 'utils_path'],\n",
    "                                   output_names=['beta_subjectinfo', \n",
    "                                                 'contrast_subjectinfo'],\n",
    "                                   function=getsubjectinfo),\n",
    "                          name=name)\n",
    "    subjectinfo.inputs.fmriprep_dir = fmriprep_dir\n",
    "    subjectinfo.inputs.data_dir = data_dir\n",
    "    subjectinfo.inputs.regress_rt = regress_rt\n",
    "    subjectinfo.inputs.utils_path = current_directory\n",
    "    return subjectinfo\n",
    "\n",
    "def get_savesubjectinfo(name):\n",
    "    # Save python objects that aren't accomodated by datasink nodes\n",
    "    savesubjectinfo = Node(Function(input_names=['save_directory',\n",
    "                                                  'beta_subjectinfo',\n",
    "                                                  'contrast_subjectinfo','contrasts'],\n",
    "                                    function=save_subjectinfo),\n",
    "                           name=name)\n",
    "    savesubjectinfo.inputs.save_directory = first_level_dir\n",
    "    return savesubjectinfo\n",
    "\n",
    "def get_selector(name, session=None):\n",
    "    if session is None:\n",
    "        ses = '*'\n",
    "    else:\n",
    "        ses = 'ses-%s' % str(session)\n",
    "    # SelectFiles - to grab the data (alternative to DataGrabber)\n",
    "    templates = {'func': join('sub-{subject_id}',ses,'func',\n",
    "                             '*{task}*MNI*preproc.nii.gz'),\n",
    "                 'mask': join('sub-{subject_id}',ses,'func',\n",
    "                              '*{task}*MNI*brainmask.nii.gz')}\n",
    "    selectfiles = Node(SelectFiles(templates,\n",
    "                                   base_directory=fmriprep_dir,\n",
    "                                   sort_filelist=True),\n",
    "                       name=name)\n",
    "    return selectfiles\n",
    "\n",
    "def get_masker(name):\n",
    "    # mask and blur\n",
    "    return Node(fsl.maths.ApplyMask(),name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_common_wf(workflow, task):\n",
    "        # Infosource - a function free node to iterate over the list of subject names\n",
    "    infosource = Node(IdentityInterface(fields=['subject_id',\n",
    "                                                'task',\n",
    "                                                'contrasts']),\n",
    "                      name=\"%s_infosource\" % task)\n",
    "    infosource.iterables = [('subject_id', subject_list)]\n",
    "    infosource.inputs.task = task\n",
    "    infosource.inputs.contrasts = get_contrasts(task, regress_rt)\n",
    "    \n",
    "    # initiate basic nodes\n",
    "    subjectinfo = get_subjectinfo('%s_subjectinfo' % task)\n",
    "    savesubjectinfo = get_savesubjectinfo('%s_savesubjectinfo' % task)\n",
    "    masker = get_masker('%s_masker' % task)\n",
    "    selectfiles = get_selector('%s_selectFiles' % task)\n",
    "    \n",
    "    # Connect up the 1st-level analysis components\n",
    "    workflow.connect([(infosource, selectfiles, [('subject_id', 'subject_id'), ('task', 'task')]),\n",
    "                      (infosource, subjectinfo, [('subject_id','subject_id'), ('task', 'task')]),\n",
    "                      (infosource, savesubjectinfo, [('contrasts','contrasts')]),\n",
    "                      (subjectinfo, savesubjectinfo, [('beta_subjectinfo','beta_subjectinfo'),\n",
    "                                                      ('contrast_subjectinfo','contrast_subjectinfo')]),\n",
    "                      (selectfiles, masker, [('func','in_file'),\n",
    "                                             ('mask', 'mask_file')])\n",
    "                        ])\n",
    "\n",
    "def init_GLM_wf(name='wf'):\n",
    "    # Datasink - creates output folder for important outputs\n",
    "    datasink = Node(DataSink(base_directory=first_level_dir), name=\"datasink\")\n",
    "    # Use the following DataSink output substitutions\n",
    "    substitutions = [('_subject_id_', ''),\n",
    "                    ('fstat', 'FSTST'),\n",
    "                    ('run0.mat', 'designfile.mat')]\n",
    "    datasink.inputs.substitutions = substitutions\n",
    "    \n",
    "    # SpecifyModel - Generates FSL-specific Model\n",
    "    modelspec = Node(SpecifyModel(input_units='secs',\n",
    "                                  time_repetition=TR,\n",
    "                                  high_pass_filter_cutoff=80),\n",
    "                     name=\"modelspec\")\n",
    "    # Level1Design - Creates FSL config file \n",
    "    level1design = Node(fsl.Level1Design(bases={'dgamma':{'derivs': True}},\n",
    "                                         interscan_interval=TR,\n",
    "                                         model_serial_correlations=True),\n",
    "                            name=\"level1design\")\n",
    "    # FEATmodel generates an FSL design matrix\n",
    "    level1model = Node(fsl.FEATModel(), name=\"FEATModel\")\n",
    "\n",
    "    # FILMGLs\n",
    "    # smooth_autocorr, check default, use FSL default\n",
    "    filmgls = Node(fsl.FILMGLS(), name=\"GLS\")\n",
    "\n",
    "    wf = Workflow(name=name)\n",
    "    wf.connect([(modelspec, level1design, [('session_info','session_info')]),\n",
    "              (level1design, level1model, [('ev_files', 'ev_files'),\n",
    "                                             ('fsf_files','fsf_file')]),\n",
    "              (level1model, filmgls, [('design_file', 'design_file'),\n",
    "                                      ('con_file', 'tcon_file'),\n",
    "                                      ('fcon_file', 'fcon_file')]),\n",
    "              (level1model, datasink, [('design_file', '%s.@design_file' % name)]),\n",
    "              (filmgls, datasink, [('copes', '%s.@copes' % name),\n",
    "                                    ('zstats', '%s.@Z' % name),\n",
    "                                    ('fstats', '%s.@F' % name),\n",
    "                                    ('tstats','%s.@T' % name),\n",
    "                                    ('param_estimates','%s.@param_estimates' % name),\n",
    "                                    ('residual4d', '%s.@residual4d' % name),\n",
    "                                    ('sigmasquareds', '%s.@sigmasquareds' % name)])\n",
    "               ])\n",
    "    return wf\n",
    "\n",
    "\n",
    "\n",
    "def get_task_wfs(task):\n",
    "    wf_dict = {}\n",
    "    # set up workflow lookup\n",
    "    default_wf = [(init_GLM_wf, {'name': '%s_contrast_wf' % task}), \n",
    "                  (init_GLM_wf, {'name': '%s_beta_wf' % task})]\n",
    "    \n",
    "    # get workflow\n",
    "    workflows = []\n",
    "    for func, kwargs in wf_dict.get(task, default_wf):\n",
    "        workflows.append(func(**kwargs))\n",
    "    return workflows\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiation of the 1st-level analysis workflow\n",
    "l1analysis = Workflow(name='l1analysis')\n",
    "l1analysis.base_dir = join(derivatives_dir, working_dir)\n",
    "\n",
    "for task in task_list:\n",
    "    init_common_wf(l1analysis, task)\n",
    "    task_workflows = get_task_wfs(task)\n",
    "    # get nodes to pass\n",
    "    infosource = l1analysis.get_node('%s_infosource' % task)\n",
    "    subjectinfo = l1analysis.get_node('%s_subjectinfo' % task)\n",
    "    masker = l1analysis.get_node('%s_masker' % task)\n",
    "    for wf in task_workflows:\n",
    "        l1analysis.connect([\n",
    "                            (infosource, wf, [('subject_id','datasink.container')]),\n",
    "                            (subjectinfo, wf, [('contrast_subjectinfo','modelspec.subject_info')]),\n",
    "                            (masker, wf, [('out_file', 'modelspec.functional_runs')]),\n",
    "                            (masker, wf, [('out_file','GLS.in_file')])\n",
    "                            ])\n",
    "        \n",
    "        if 'contrast' in wf.name:\n",
    "            l1analysis.connect([(infosource, wf, [('contrasts','level1design.contrasts')])])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def init_common_wf(workflow, task):\n",
    "        # Infosource - a function free node to iterate over the list of subject names\n",
    "    infosource = Node(IdentityInterface(fields=['subject_id',\n",
    "                                                'task',\n",
    "                                                'contrasts']),\n",
    "                      name=\"%s_infosource\" % task)\n",
    "    infosource.iterables = [('subject_id', subject_list)]\n",
    "    infosource.inputs.task = task\n",
    "    infosource.inputs.contrasts = get_contrasts(task, regress_rt)\n",
    "    \n",
    "    # initiate basic nodes\n",
    "    subjectinfo = get_subjectinfo('%s_subjectinfo' % task)\n",
    "    savesubjectinfo = get_savesubjectinfo('%s_savesubjectinfo' % task)\n",
    "    masker = get_masker('%s_masker' % task)\n",
    "    selectfiles = get_selector('%s_selectFiles' % task)\n",
    "    \n",
    "    # Connect up the 1st-level analysis components\n",
    "    workflow.connect([(infosource, selectfiles, [('subject_id', 'subject_id'), ('task', 'task')]),\n",
    "                      (infosource, subjectinfo, [('subject_id','subject_id'), ('task', 'task')]),\n",
    "                      (infosource, savesubjectinfo, [('contrasts','contrasts')]),\n",
    "                      (subjectinfo, savesubjectinfo, [('beta_subjectinfo','beta_subjectinfo'),\n",
    "                                                      ('contrast_subjectinfo','contrast_subjectinfo')]),\n",
    "                      (selectfiles, masker, [('func','in_file'),\n",
    "                                             ('mask', 'mask_file')])\n",
    "                        ])\n",
    "\n",
    "def init_GLM_wf(name='wf'):\n",
    "    # Datasink - creates output folder for important outputs\n",
    "    datasink = Node(DataSink(base_directory=first_level_dir), name=\"datasink\")\n",
    "    # Use the following DataSink output substitutions\n",
    "    substitutions = [('_subject_id_', ''),\n",
    "                    ('fstat', 'FSTST'),\n",
    "                    ('run0.mat', 'designfile.mat')]\n",
    "    datasink.inputs.substitutions = substitutions\n",
    "    \n",
    "    # SpecifyModel - Generates FSL-specific Model\n",
    "    modelspec = Node(SpecifyModel(input_units='secs',\n",
    "                                  time_repetition=TR,\n",
    "                                  high_pass_filter_cutoff=80),\n",
    "                     name=\"modelspec\")\n",
    "    # Level1Design - Creates FSL config file \n",
    "    level1design = Node(fsl.Level1Design(bases={'dgamma':{'derivs': True}},\n",
    "                                         interscan_interval=TR,\n",
    "                                         model_serial_correlations=True),\n",
    "                            name=\"level1design\")\n",
    "    # FEATmodel generates an FSL design matrix\n",
    "    level1model = Node(fsl.FEATModel(), name=\"FEATModel\")\n",
    "\n",
    "    # FILMGLs\n",
    "    # smooth_autocorr, check default, use FSL default\n",
    "    filmgls = Node(fsl.FILMGLS(), name=\"GLS\")\n",
    "\n",
    "    wf = Workflow(name=name)\n",
    "    wf.connect([(modelspec, level1design, [('session_info','session_info')])\n",
    "               ])\n",
    "    return wf\n",
    "\n",
    "\n",
    "\n",
    "def get_task_wfs(task):\n",
    "    wf_dict = {}\n",
    "    # set up workflow lookup\n",
    "    default_wf = [(init_GLM_wf, {'name': '%s_contrast_wf' % task}), \n",
    "                  (init_GLM_wf, {'name': '%s_beta_wf' % task})]\n",
    "    \n",
    "    # get workflow\n",
    "    workflows = []\n",
    "    for func, kwargs in wf_dict.get(task, default_wf):\n",
    "        workflows.append(func(**kwargs))\n",
    "    return workflows\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Initiation of the 1st-level analysis workflow\n",
    "l1analysis = Workflow(name='l1analysis')\n",
    "l1analysis.base_dir = join(derivatives_dir, working_dir)\n",
    "\n",
    "for task in task_list:\n",
    "    init_common_wf(l1analysis, task)\n",
    "    task_workflows = get_task_wfs(task)\n",
    "    # get nodes to pass\n",
    "    infosource = l1analysis.get_node('%s_infosource' % task)\n",
    "    subjectinfo = l1analysis.get_node('%s_subjectinfo' % task)\n",
    "    masker = l1analysis.get_node('%s_masker' % task)\n",
    "    for wf in task_workflows:\n",
    "        l1analysis.connect([\n",
    "                            (subjectinfo, wf, [('contrast_subjectinfo','modelspec.subject_info')]),\n",
    "                            (masker, wf, [('out_file', 'modelspec.functional_runs')]),\n",
    "                            ])\n",
    "        \n",
    "        if 'contrast' in wf.name:\n",
    "            l1analysis.connect([(infosource, wf, [('contrasts','level1design.contrasts')])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180509-00:59:35,354 workflow INFO:\n",
      "\t Workflow l1analysis settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "180509-00:59:35,371 workflow INFO:\n",
      "\t Running serially.\n",
      "180509-00:59:35,372 workflow INFO:\n",
      "\t Executing node l1analysis.twoByTwo_selectFiles in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130/twoByTwo_selectFiles\n",
      "180509-00:59:35,377 workflow INFO:\n",
      "\t Running node \"twoByTwo_selectFiles\" (\"nipype.interfaces.io.SelectFiles\").\n",
      "180509-00:59:35,385 workflow INFO:\n",
      "\t Executing node l1analysis.twoByTwo_masker in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130/twoByTwo_masker\n",
      "180509-00:59:35,387 workflow INFO:\n",
      "\t Collecting precomputed outputs\n",
      "180509-00:59:35,388 workflow INFO:\n",
      "\t Executing node l1analysis.twoByTwo_subjectinfo in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130/twoByTwo_subjectinfo\n",
      "180509-00:59:35,396 workflow INFO:\n",
      "\t Collecting precomputed outputs\n",
      "180509-00:59:35,443 workflow INFO:\n",
      "\t Executing node l1analysis.twoByTwo_contrast_wf.modelspec in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/twoByTwo_contrast_wf/_subject_id_s130/modelspec\n",
      "180509-00:59:35,456 workflow INFO:\n",
      "\t Collecting precomputed outputs\n",
      "180509-00:59:35,583 workflow INFO:\n",
      "\t Executing node l1analysis.twoByTwo_contrast_wf.level1design in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/twoByTwo_contrast_wf/_subject_id_s130/level1design\n",
      "180509-00:59:35,688 workflow INFO:\n",
      "\t Collecting precomputed outputs\n",
      "180509-00:59:35,704 workflow INFO:\n",
      "\t Executing node l1analysis.twoByTwo_contrast_wf.FEATModel in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/twoByTwo_contrast_wf/_subject_id_s130/FEATModel\n",
      "180509-00:59:35,707 workflow INFO:\n",
      "\t Collecting precomputed outputs\n",
      "180509-00:59:35,710 workflow INFO:\n",
      "\t Executing node l1analysis.twoByTwo_contrast_wf.GLS in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/twoByTwo_contrast_wf/_subject_id_s130/GLS\n",
      "180509-00:59:35,714 workflow INFO:\n",
      "\t Running node \"GLS\" (\"nipype.interfaces.fsl.model.FILMGLS\"), a CommandLine Interface with command:\n",
      "film_gls --rn=results --con=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/twoByTwo_contrast_wf/_subject_id_s130/FEATModel/run0.con --in=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130/twoByTwo_masker/sub-s130_ses-2_task-twoByTwo_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz --pd=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/twoByTwo_contrast_wf/_subject_id_s130/FEATModel/run0.mat --thr=0.000000.\n",
      "180509-00:59:35,770 interface INFO:\n",
      "\t stdout 2018-05-09T00:59:35.770900:Log directory is: results\n",
      "180509-01:00:05,145 interface INFO:\n",
      "\t stdout 2018-05-09T01:00:05.145843:paradigm.getDesignMatrix().Nrows()=1016\n",
      "180509-01:00:05,146 interface INFO:\n",
      "\t stdout 2018-05-09T01:00:05.145843:paradigm.getDesignMatrix().Ncols()=33\n",
      "180509-01:00:05,146 interface INFO:\n",
      "\t stdout 2018-05-09T01:00:05.145843:sizeTS=1016\n",
      "180509-01:00:05,147 interface INFO:\n",
      "\t stdout 2018-05-09T01:00:05.145843:numTS=200755\n",
      "180509-01:00:05,520 interface INFO:\n",
      "\t stdout 2018-05-09T01:00:05.520182:Calculating residuals...\n",
      "180509-01:02:11,83 workflow ERROR:\n",
      "\t Node GLS.a0 failed to run on host ian-System-Product-Name.\n",
      "180509-01:02:11,84 workflow ERROR:\n",
      "\t Saving crash info to /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180509-010211-ian-GLS.a0-bc58a47a-e58d-4c0a-97d0-f1a3b6929315.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/linear.py\", line 43, in run\n",
      "    node.run(updatehash=updatehash)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1089, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1693, in _run_interface\n",
      "    runtime = run_command(runtime, output=self.terminal_output)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1417, in run_command\n",
      "    _process()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1404, in _process\n",
      "    res = select.select(streams, [], [], timeout)\n",
      "KeyboardInterrupt\n",
      "\n",
      "180509-01:02:11,137 workflow INFO:\n",
      "\t Executing node l1analysis.twoByTwo_beta_wf.modelspec in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/twoByTwo_beta_wf/_subject_id_s130/modelspec\n",
      "180509-01:02:11,157 workflow INFO:\n",
      "\t Collecting precomputed outputs\n",
      "180509-01:02:11,296 workflow INFO:\n",
      "\t Executing node l1analysis.twoByTwo_beta_wf.level1design in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/twoByTwo_beta_wf/_subject_id_s130/level1design\n",
      "180509-01:02:11,407 workflow INFO:\n",
      "\t Collecting precomputed outputs\n",
      "180509-01:02:11,424 workflow INFO:\n",
      "\t Executing node l1analysis.twoByTwo_beta_wf.FEATModel in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/twoByTwo_beta_wf/_subject_id_s130/FEATModel\n",
      "180509-01:02:11,427 workflow INFO:\n",
      "\t Collecting precomputed outputs\n",
      "180509-01:02:11,430 workflow INFO:\n",
      "\t Executing node l1analysis.twoByTwo_beta_wf.GLS in dir: /home/ian/tmp/fmri/derivatives/workingdir/l1analysis/twoByTwo_beta_wf/_subject_id_s130/GLS\n",
      "180509-01:02:11,434 workflow INFO:\n",
      "\t Running node \"GLS\" (\"nipype.interfaces.fsl.model.FILMGLS\"), a CommandLine Interface with command:\n",
      "film_gls --rn=results --con=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/twoByTwo_beta_wf/_subject_id_s130/FEATModel/run0.con --in=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/_subject_id_s130/twoByTwo_masker/sub-s130_ses-2_task-twoByTwo_run-1_bold_space-MNI152NLin2009cAsym_preproc_masked.nii.gz --pd=/media/Data/Ian/Temp/fmri/derivatives/workingdir/l1analysis/twoByTwo_beta_wf/_subject_id_s130/FEATModel/run0.mat --thr=0.000000.\n",
      "180509-01:02:11,485 interface INFO:\n",
      "\t stdout 2018-05-09T01:02:11.485336:Log directory is: results\n"
     ]
    }
   ],
   "source": [
    "l1analysis.run()\n",
    "#l1analysis.run('MultiProc', plugin_args={'n_procs': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "243px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
