{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180512-01:22:09,223 duecredit ERROR:\n",
      "\t Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from inspect import currentframe, getframeinfo\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.algorithms.modelgen import SpecifyModel\n",
    "from nipype.interfaces.base import Bunch\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.pipeline.engine import Workflow, Node\n",
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "from utils.event_utils import get_beta_series, get_contrasts, parse_EVs, process_confounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments\n",
    "These are not needed for the jupyter notebook, but are used after conversion to a script for production\n",
    "\n",
    "- conversion command:\n",
    "  - jupyter nbconvert --to script --execute task_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Example BIDS App entrypoint script.')\n",
    "parser.add_argument('-derivatives_dir', default='/derivatives')\n",
    "parser.add_argument('-data_dir', default='/data')\n",
    "parser.add_argument('--participant_label')\n",
    "parser.add_argument('--tasks', nargs=\"+\")\n",
    "parser.add_argument('--skip_beta', action='store_false')\n",
    "parser.add_argument('--skip_contrast', action='store_false')\n",
    "parser.add_argument('--n_procs', default=16)\n",
    "if '-derivatives_dir' in sys.argv or '-h' in sys.argv:\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    args = parser.parse_args([])\n",
    "    args.derivatives_dir = '/home/ian/tmp/fmri/derivatives/'\n",
    "    args.data_dir = '/mnt/OAK'\n",
    "    args.tasks = ['stroop']\n",
    "    args.participant_label = 's130'\n",
    "    args.n_procs=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get current directory to pass to function nodes\n",
    "filename = getframeinfo(currentframe()).filename\n",
    "current_directory = str(Path(filename).resolve().parent)\n",
    "\n",
    "# list of subject identifiers\n",
    "subject_id = args.participant_label\n",
    "# list of task identifiers\n",
    "if args.tasks is not None:\n",
    "    task_list = args.tasks\n",
    "else:\n",
    "    task_list = ['ANT', 'CCTHot', 'discountFix',\n",
    "               'DPX', 'motorSelectiveStop',\n",
    "               'stopSignal', 'stroop', 'surveyMedley',\n",
    "               'twoByTwo', 'WATT3']\n",
    "\n",
    "#### Experiment Variables\n",
    "derivatives_dir = args.derivatives_dir\n",
    "fmriprep_dir = join(derivatives_dir, 'fmriprep', 'fmriprep')\n",
    "data_dir = args.data_dir\n",
    "first_level_dir = join(derivatives_dir,'1stLevel')\n",
    "working_dir = 'workingdir'\n",
    "run_beta = args.skip_beta\n",
    "run_contrast = args.skip_contrast\n",
    "n_procs = args.n_procs\n",
    "# TR of functional images\n",
    "TR = .68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Task List: ['stroop']\n",
      ", Subject: s130\n",
      ", derivatives_dir: /home/ian/tmp/fmri/derivatives/\n",
      ", data_dir: /mnt/OAK\n",
      "Running Contrast?: Yes, Running Beta?: Yes\n",
      "*******************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print('*'*79)\n",
    "print('Task List: %s\\n, Subject: %s\\n, derivatives_dir: %s\\n, data_dir: %s' % \n",
    "     (task_list, subject_id, derivatives_dir, data_dir))\n",
    "print('Running Contrast?: %s, Running Beta?: %s' % \n",
    "     (['No','Yes'][run_contrast], ['No','Yes'][run_beta]))\n",
    "print('*'*79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_events_regressors(data_dir, fmirprep_dir, subject_id, task):\n",
    "    # strip \"sub\" from beginning of subject_id if provided\n",
    "    subject_id = subject_id.replace('sub-','')\n",
    "    ## Get the Confounds File (output of fmriprep)\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    confounds_file = glob(join(fmriprep_dir,\n",
    "                               'sub-%s' % subject_id,\n",
    "                               '*', 'func',\n",
    "                               '*%s*confounds.tsv' % task))[0]\n",
    "    regressors, regressor_names = process_confounds(confounds_file)\n",
    "    ## Get the Events File if it exists\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    event_file = glob(join(data_dir,\n",
    "                           'sub-%s' % subject_id,\n",
    "                           '*', 'func',\n",
    "                           '*%s*events.tsv' % task))   \n",
    "    if len(event_file)>0:\n",
    "        # set up events file\n",
    "        event_file = event_file[0]\n",
    "        events_df = pd.read_csv(event_file,sep = '\\t')\n",
    "    else:\n",
    "        events_df = None\n",
    "    regressors, regressor_names = process_confounds(confounds_file)\n",
    "    return events_df, regressors, regressor_names\n",
    "\n",
    "# helper function to create bunch\n",
    "def getsubjectinfo(events_dr, regressors, regressor_names, task='beta', regress_rt=True): \n",
    "    EV_dict = parse_EVs(events_df, task, regress_rt)\n",
    "    contrasts = []\n",
    "    if task not in ['beta']:\n",
    "        contrasts = get_contrasts(task, regress_rt)\n",
    "    # create beta series info\n",
    "    subjectinfo = Bunch(conditions=EV_dict['conditions'],\n",
    "                        onsets=EV_dict['onsets'],\n",
    "                        durations=EV_dict['durations'],\n",
    "                        amplitudes=EV_dict['amplitudes'],\n",
    "                        tmod=None,\n",
    "                        pmod=None,\n",
    "                        regressor_names=regressor_names,\n",
    "                        regressors=regressors.T.tolist(),\n",
    "                        contrasts=contrasts)\n",
    "    return subjectinfo\n",
    "    \n",
    "def save_subjectinfo(save_directory, subjectinfo):\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    subjectinfo_path = join(save_directory, 'subjectinfo.pkl')\n",
    "    pickle.dump(subjectinfo, open(subjectinfo_path,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Input and Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_selector(task, subject_id, session=None):\n",
    "    if session is None:\n",
    "        ses = '*'\n",
    "    else:\n",
    "        ses = 'ses-%s' % str(session)\n",
    "    # SelectFiles - to grab the data (alternative to DataGrabber)\n",
    "    templates = {'func': join('sub-{subject_id}',ses,'func',\n",
    "                             '*{task}*MNI*preproc.nii.gz'),\n",
    "                 'mask': join('sub-{subject_id}',ses,'func',\n",
    "                              '*{task}*MNI*brainmask.nii.gz')}\n",
    "    selectfiles = Node(SelectFiles(templates,\n",
    "                                   base_directory=fmriprep_dir,\n",
    "                                   sort_filelist=True),\n",
    "                       name='%s_selectFiles' % task)\n",
    "    selectfiles.inputs.task = task\n",
    "    selectfiles.inputs.subject_id = subject_id\n",
    "    return selectfiles\n",
    "\n",
    "def get_masker(name):\n",
    "    # mask and blur\n",
    "    return Node(fsl.maths.ApplyMask(),name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_common_wf(workflow, task):\n",
    "    # initiate basic nodes\n",
    "    masker = get_masker('%s_masker' % task)\n",
    "    selectfiles = get_selector(task, subject_id)\n",
    "    # Connect up the 1st-level analysis components\n",
    "    workflow.connect([(selectfiles, masker, [('func','in_file'), ('mask', 'mask_file')])])\n",
    "\n",
    "def init_GLM_wf(subject_info, task, wf_label='model-standard_wf-standard', contrasts=None):\n",
    "    name = '%s_%s' % (task, wf_label)\n",
    "    # Datasink - creates output folder for important outputs\n",
    "    datasink = Node(DataSink(base_directory=first_level_dir,\n",
    "                             container=subject_id), name=\"datasink\")\n",
    "    # Use the following DataSink output substitutions\n",
    "    substitutions = [('_subject_id_', ''),\n",
    "                    ('fstat', 'FSTST'),\n",
    "                    ('run0.mat', 'designfile.mat')]\n",
    "    \n",
    "    datasink.inputs.substitutions = substitutions\n",
    "    # ridiculous regexp substitution to get files just right\n",
    "    # link to ridiculousness: https://regex101.com/r/ljS5zK/3\n",
    "    match_str = \"(?P<sub>s[0-9]+)\\/(?P<task>[A-Za-z1-9_]+)_(?P<model>model-[a-z]+)_(?P<submodel>wf-[a-z]+)\\/(s[0-9]+/|)\"\n",
    "    replace_str = \"\\g<sub>/\\g<task>/\\g<model>/\\g<submodel>/\"\n",
    "    regexp_substitutions = [(match_str, replace_str)]\n",
    "    datasink.inputs.regexp_substitutions = regexp_substitutions\n",
    "    \n",
    "    # SpecifyModel - Generates FSL-specific Model\n",
    "    modelspec = Node(SpecifyModel(input_units='secs',\n",
    "                                  time_repetition=TR,\n",
    "                                  high_pass_filter_cutoff=80),\n",
    "                     name=\"%s_modelspec\" % task)\n",
    "    modelspec.inputs.subject_info = subject_info\n",
    "    # Level1Design - Creates FSL config file \n",
    "    level1design = Node(fsl.Level1Design(bases={'dgamma':{'derivs': True}},\n",
    "                                         interscan_interval=TR,\n",
    "                                         model_serial_correlations=True),\n",
    "                            name=\"%s_level1design\" % task)\n",
    "    level1design.inputs.contrasts=subject_info.contrasts\n",
    "    # FEATmodel generates an FSL design matrix\n",
    "    level1model = Node(fsl.FEATModel(), name=\"%s_FEATModel\" % task)\n",
    "\n",
    "    # FILMGLs\n",
    "    # smooth_autocorr, check default, use FSL default\n",
    "    filmgls = Node(fsl.FILMGLS(threshold=1000), name=\"%s_GLS\" % task)\n",
    "\n",
    "    wf = Workflow(name=name)\n",
    "    wf.connect([(modelspec, level1design, [('session_info','session_info')]),\n",
    "                (level1design, level1model, [('ev_files', 'ev_files'),\n",
    "                                             ('fsf_files','fsf_file')]),\n",
    "                (level1model, datasink, [('design_file', '%s.@design_file' % name)]),\n",
    "                (level1model, filmgls, [('design_file', 'design_file'),\n",
    "                                        ('con_file', 'tcon_file'),\n",
    "                                        ('fcon_file', 'fcon_file')]),\n",
    "                (filmgls, datasink, [('copes', '%s.@copes' % name),\n",
    "                                     ('zstats', '%s.@Z' % name),\n",
    "                                     ('fstats', '%s.@F' % name),\n",
    "                                     ('tstats','%s.@T' % name),\n",
    "                                     ('param_estimates','%s.@param_estimates' % name),\n",
    "                                     ('residual4d', '%s.@residual4d' % name),\n",
    "                                     ('sigmasquareds', '%s.@sigmasquareds' % name)])\n",
    "               ])\n",
    "    return wf\n",
    "\n",
    "\n",
    "\n",
    "def get_task_wfs(task, beta_subjectinfo=None, contrast_subjectinfo=None, regress_rt=True):\n",
    "    rt_suffix = 'rt' if regress_rt==True else 'nort'\n",
    "    # set up workflow lookup\n",
    "    wf_dict = {'contrast': (init_GLM_wf, {'wf_label': 'model-%s_wf-contrast' % rt_suffix,\n",
    "                                          'task': task}), \n",
    "               'beta': (init_GLM_wf, {'wf_label': 'model-%s_wf-beta' % rt_suffix,\n",
    "                                      'task': task})}\n",
    "    \n",
    "    workflows = []\n",
    "    if beta_subjectinfo:\n",
    "        save_directory = join(first_level_dir, subject_id, task, 'model-%s' % rt_suffix, 'wf-beta')\n",
    "        save_subjectinfo(save_directory, beta_subjectinfo)\n",
    "        func, kwargs = wf_dict['beta']\n",
    "        workflows.append(func(beta_subjectinfo, **kwargs))\n",
    "    if contrast_subjectinfo:\n",
    "        save_directory = join(first_level_dir, subject_id, task, 'model-%s' % rt_suffix, 'wf-contrast')\n",
    "        save_subjectinfo(save_directory, contrast_subjectinfo)\n",
    "        func, kwargs = wf_dict['contrast']\n",
    "        workflows.append(func(contrast_subjectinfo, **kwargs))\n",
    "    return workflows\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiation of the 1st-level analysis workflow\n",
    "l1analysis = Workflow(name='%s_l1analysis' % subject_id)\n",
    "l1analysis.base_dir = join(derivatives_dir, working_dir)\n",
    "\n",
    "for task in task_list:\n",
    "    init_common_wf(l1analysis, task)\n",
    "    # get nodes to pass\n",
    "    masker = l1analysis.get_node('%s_masker' % task)\n",
    "    # get info to pass to task workflows\n",
    "    events_df, regressors, regressor_names = get_events_regressors(data_dir, fmriprep_dir,\n",
    "                                                                   subject_id, task)\n",
    "    # perform analyses both by regressing rt and not\n",
    "    regress_rt_conditions = [True, False]\n",
    "    if 'stop' in task:\n",
    "        regress_rt_conditions = [False]\n",
    "    betainfo = None; contrastinfo = None\n",
    "    for regress_rt in regress_rt_conditions:\n",
    "        if run_beta:\n",
    "            betainfo = getsubjectinfo(events_df, regressors, regressor_names, task='beta', regress_rt=regress_rt)\n",
    "        if run_contrast:\n",
    "            contrastinfo = getsubjectinfo(events_df, regressors, regressor_names, task=task, regress_rt=regress_rt)\n",
    "        task_workflows = get_task_wfs(task, betainfo, contrastinfo, regress_rt)\n",
    "        for wf in task_workflows:\n",
    "            l1analysis.connect([\n",
    "                                (masker, wf, [('out_file', '%s_modelspec.functional_runs' % task)]),\n",
    "                                (masker, wf, [('out_file','%s_GLS.in_file' % task)])\n",
    "                                ])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180512-01:22:34,499 workflow INFO:\n",
      "\t Workflow s130_l1analysis settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "180512-01:22:34,530 workflow INFO:\n",
      "\t Running in parallel.\n",
      "180512-01:22:34,533 workflow INFO:\n",
      "\t Currently running 0 tasks, and 1 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 4/4\n",
      "180512-01:22:34,536 workflow INFO:\n",
      "\t Executing node s130_l1analysis.stroop_selectFiles in dir: /home/ian/tmp/fmri/derivatives/workingdir/s130_l1analysis/stroop_selectFiles\n",
      "180512-01:22:34,543 workflow INFO:\n",
      "\t Running node \"stroop_selectFiles\" (\"nipype.interfaces.io.SelectFiles\").\n",
      "180512-01:22:36,538 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_selectFiles jobid: 0\n",
      "180512-01:22:36,540 workflow INFO:\n",
      "\t Currently running 0 tasks, and 1 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 4/4\n",
      "180512-01:22:36,543 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_masker jobid: 1\n",
      "180512-01:22:38,551 workflow INFO:\n",
      "\t Currently running 0 tasks, and 4 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 4/4\n",
      "180512-01:22:38,566 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_modelspec jobid: 2\n",
      "180512-01:22:38,572 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_modelspec jobid: 7\n",
      "180512-01:22:38,578 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_modelspec jobid: 12\n",
      "180512-01:22:38,583 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_modelspec jobid: 17\n",
      "180512-01:22:40,688 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_level1design jobid: 3\n",
      "180512-01:22:40,745 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_level1design jobid: 8\n",
      "180512-01:22:40,808 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_level1design jobid: 13\n",
      "180512-01:22:40,862 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_level1design jobid: 18\n",
      "180512-01:22:42,889 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_FEATModel jobid: 4\n",
      "180512-01:22:42,897 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_FEATModel jobid: 9\n",
      "180512-01:22:42,921 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_FEATModel jobid: 14\n",
      "180512-01:22:42,928 workflow INFO:\n",
      "\t [Job finished] jobname: stroop_FEATModel jobid: 19\n",
      "180512-01:22:44,936 workflow INFO:\n",
      "\t Currently running 0 tasks, and 8 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 4/4\n",
      "180512-01:22:44,952 workflow INFO:\n",
      "\t Executing node s130_l1analysis.stroop_model-rt_wf-beta.stroop_GLS in dir: /home/ian/tmp/fmri/derivatives/workingdir/s130_l1analysis/stroop_model-rt_wf-beta/stroop_GLS\n",
      "180512-01:22:44,970 workflow INFO:\n",
      "\t Executing node s130_l1analysis.stroop_model-rt_wf-beta.datasink in dir: /home/ian/tmp/fmri/derivatives/workingdir/s130_l1analysis/stroop_model-rt_wf-beta/datasink\n",
      "180512-01:22:44,975 workflow INFO:\n",
      "\t Executing node s130_l1analysis.stroop_model-rt_wf-contrast.stroop_GLS in dir: /home/ian/tmp/fmri/derivatives/workingdir/s130_l1analysis/stroop_model-rt_wf-contrast/stroop_GLS\n",
      "180512-01:22:44,990 workflow INFO:\n",
      "\t Executing node s130_l1analysis.stroop_model-rt_wf-contrast.datasink in dir: /home/ian/tmp/fmri/derivatives/workingdir/s130_l1analysis/stroop_model-rt_wf-contrast/datasink\n",
      "180512-01:22:44,992 workflow INFO:\n",
      "\t Running node \"datasink\" (\"nipype.interfaces.io.DataSink\").\n",
      "180512-01:22:44,998 interface INFO:\n",
      "\t sub: /home/ian/tmp/fmri/derivatives/1stLevel/s130/stroop_model-rt_wf-beta/run0.mat -> /home/ian/tmp/fmri/derivatives/1stLevel/s130/stroop/model-rt/wf-beta/designfile.mat\n",
      "180512-01:22:45,6 workflow INFO:\n",
      "\t Running node \"datasink\" (\"nipype.interfaces.io.DataSink\").\n",
      "180512-01:22:45,9 interface INFO:\n",
      "\t sub: /home/ian/tmp/fmri/derivatives/1stLevel/s130/stroop_model-rt_wf-contrast/run0.mat -> /home/ian/tmp/fmri/derivatives/1stLevel/s130/stroop/model-rt/wf-contrast/designfile.mat\n",
      "180512-01:22:46,992 workflow ERROR:\n",
      "\t Node stroop_GLS failed to run on host ian-System-Product-Name.\n",
      "180512-01:22:46,996 workflow ERROR:\n",
      "\t Saving crash info to /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180512-012246-ian-stroop_GLS-bf5128e2-68d9-49fa-a266-2fe3048f56fa.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 639, in _run_command\n",
      "    cmd = self._interface.cmdline\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1616, in cmdline\n",
      "    self._check_mandatory_inputs()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 980, in _check_mandatory_inputs\n",
      "    raise ValueError(msg)\n",
      "ValueError: FILMGLS requires a value for input 'in_file'. For a list of required inputs, see FILMGLS.help()\n",
      "\n",
      "180512-01:22:47,1 workflow INFO:\n",
      "\t [Job finished] jobname: datasink jobid: 6\n",
      "180512-01:22:47,3 workflow ERROR:\n",
      "\t Node stroop_GLS failed to run on host ian-System-Product-Name.\n",
      "180512-01:22:47,3 workflow ERROR:\n",
      "\t Saving crash info to /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180512-012246-ian-stroop_GLS-01d4a0b1-2af9-44f8-9aab-8db595925fbf.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 639, in _run_command\n",
      "    cmd = self._interface.cmdline\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1616, in cmdline\n",
      "    self._check_mandatory_inputs()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 980, in _check_mandatory_inputs\n",
      "    raise ValueError(msg)\n",
      "ValueError: FILMGLS requires a value for input 'in_file'. For a list of required inputs, see FILMGLS.help()\n",
      "\n",
      "180512-01:22:47,5 workflow INFO:\n",
      "\t [Job finished] jobname: datasink jobid: 11\n",
      "180512-01:22:47,7 workflow INFO:\n",
      "\t Currently running 0 tasks, and 4 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 4/4\n",
      "180512-01:22:47,10 workflow INFO:\n",
      "\t Executing node s130_l1analysis.stroop_model-nort_wf-beta.stroop_GLS in dir: /home/ian/tmp/fmri/derivatives/workingdir/s130_l1analysis/stroop_model-nort_wf-beta/stroop_GLS\n",
      "180512-01:22:47,13 workflow INFO:\n",
      "\t Executing node s130_l1analysis.stroop_model-nort_wf-beta.datasink in dir: /home/ian/tmp/fmri/derivatives/workingdir/s130_l1analysis/stroop_model-nort_wf-beta/datasink\n",
      "180512-01:22:47,16 workflow INFO:\n",
      "\t Executing node s130_l1analysis.stroop_model-nort_wf-contrast.stroop_GLS in dir: /home/ian/tmp/fmri/derivatives/workingdir/s130_l1analysis/stroop_model-nort_wf-contrast/stroop_GLS\n",
      "180512-01:22:47,21 workflow INFO:\n",
      "\t Running node \"datasink\" (\"nipype.interfaces.io.DataSink\").180512-01:22:47,21 workflow INFO:\n",
      "\t Executing node s130_l1analysis.stroop_model-nort_wf-contrast.datasink in dir: /home/ian/tmp/fmri/derivatives/workingdir/s130_l1analysis/stroop_model-nort_wf-contrast/datasink\n",
      "\n",
      "180512-01:22:47,24 interface INFO:\n",
      "\t sub: /home/ian/tmp/fmri/derivatives/1stLevel/s130/stroop_model-nort_wf-beta/run0.mat -> /home/ian/tmp/fmri/derivatives/1stLevel/s130/stroop/model-nort/wf-beta/designfile.mat\n",
      "180512-01:22:47,31 workflow INFO:\n",
      "\t Running node \"datasink\" (\"nipype.interfaces.io.DataSink\").\n",
      "180512-01:22:47,34 interface INFO:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t sub: /home/ian/tmp/fmri/derivatives/1stLevel/s130/stroop_model-nort_wf-contrast/run0.mat -> /home/ian/tmp/fmri/derivatives/1stLevel/s130/stroop/model-nort/wf-contrast/designfile.mat\n",
      "180512-01:22:49,25 workflow ERROR:\n",
      "\t Node stroop_GLS failed to run on host ian-System-Product-Name.\n",
      "180512-01:22:49,28 workflow ERROR:\n",
      "\t Saving crash info to /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180512-012249-ian-stroop_GLS-1cb3ff5a-c229-4bbf-8527-217459e28dc0.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 639, in _run_command\n",
      "    cmd = self._interface.cmdline\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1616, in cmdline\n",
      "    self._check_mandatory_inputs()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 980, in _check_mandatory_inputs\n",
      "    raise ValueError(msg)\n",
      "ValueError: FILMGLS requires a value for input 'in_file'. For a list of required inputs, see FILMGLS.help()\n",
      "\n",
      "180512-01:22:49,38 workflow INFO:\n",
      "\t [Job finished] jobname: datasink jobid: 16\n",
      "180512-01:22:49,43 workflow ERROR:\n",
      "\t Node stroop_GLS failed to run on host ian-System-Product-Name.\n",
      "180512-01:22:49,46 workflow ERROR:\n",
      "\t Saving crash info to /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180512-012249-ian-stroop_GLS-ba8b31ac-0c13-428e-9a3a-b243997a3521.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 639, in _run_command\n",
      "    cmd = self._interface.cmdline\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1616, in cmdline\n",
      "    self._check_mandatory_inputs()\n",
      "  File \"/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 980, in _check_mandatory_inputs\n",
      "    raise ValueError(msg)\n",
      "ValueError: FILMGLS requires a value for input 'in_file'. For a list of required inputs, see FILMGLS.help()\n",
      "\n",
      "180512-01:22:49,52 workflow INFO:\n",
      "\t [Job finished] jobname: datasink jobid: 21\n",
      "180512-01:22:49,56 workflow INFO:\n",
      "\t Currently running 0 tasks, and 0 jobs ready. Free memory (GB): 28.23/28.23, Free processors: 4/4\n",
      "180512-01:22:51,60 workflow INFO:\n",
      "\t ***********************************\n",
      "180512-01:22:51,62 workflow ERROR:\n",
      "\t could not run node: s130_l1analysis.stroop_model-rt_wf-beta.stroop_GLS\n",
      "180512-01:22:51,64 workflow INFO:\n",
      "\t crashfile: /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180512-012246-ian-stroop_GLS-bf5128e2-68d9-49fa-a266-2fe3048f56fa.pklz\n",
      "180512-01:22:51,65 workflow ERROR:\n",
      "\t could not run node: s130_l1analysis.stroop_model-rt_wf-contrast.stroop_GLS\n",
      "180512-01:22:51,66 workflow INFO:\n",
      "\t crashfile: /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180512-012246-ian-stroop_GLS-01d4a0b1-2af9-44f8-9aab-8db595925fbf.pklz\n",
      "180512-01:22:51,66 workflow ERROR:\n",
      "\t could not run node: s130_l1analysis.stroop_model-nort_wf-beta.stroop_GLS\n",
      "180512-01:22:51,66 workflow INFO:\n",
      "\t crashfile: /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180512-012249-ian-stroop_GLS-1cb3ff5a-c229-4bbf-8527-217459e28dc0.pklz\n",
      "180512-01:22:51,67 workflow ERROR:\n",
      "\t could not run node: s130_l1analysis.stroop_model-nort_wf-contrast.stroop_GLS\n",
      "180512-01:22:51,67 workflow INFO:\n",
      "\t crashfile: /media/Data/Ian/Experiments/expfactory/Self_Regulation_Ontology_fMRI/fmri_analysis/scripts/crash-20180512-012249-ian-stroop_GLS-ba8b31ac-0c13-428e-9a3a-b243997a3521.pklz\n",
      "180512-01:22:51,68 workflow INFO:\n",
      "\t ***********************************\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Workflow did not execute cleanly. Check log for details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b1a9045850c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#l1analysis.run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ml1analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MultiProc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'n_procs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn_procs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/engine/workflows.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, plugin, plugin_args, updatehash)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'create_report'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_report_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdatehash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdatehash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mdatestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%dT%H%M%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'write_provenance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, graph, config, updatehash)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_node_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mreport_nodes_not_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# close any open resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Data/Ian/miniconda/envs/fmri/lib/python3.6/site-packages/nipype/pipeline/plugins/tools.py\u001b[0m in \u001b[0;36mreport_nodes_not_run\u001b[0;34m(notrun)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***********************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         raise RuntimeError(('Workflow did not execute cleanly. '\n\u001b[0m\u001b[1;32m     80\u001b[0m                             'Check log for details'))\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Workflow did not execute cleanly. Check log for details"
     ]
    }
   ],
   "source": [
    "#l1analysis.run()\n",
    "l1analysis.run('MultiProc', plugin_args={'n_procs': n_procs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "243px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
