{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/usr/local/miniconda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/miniconda/lib/python3.7/site-packages/nipype/utils/misc.py:14: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterator\n",
      "/usr/local/miniconda/lib/python3.7/site-packages/nipype/utils/config.py:103: DeprecationWarning: This method will be removed in future versions.  Use 'parser.read_file()' instead.\n",
      "  self._config.readfp(StringIO(default_cfg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200213-17:35:37,648 duecredit ERROR:\n",
      "\t Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from glob import glob\n",
    "from os import makedirs, path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from nistats.second_level_model import SecondLevelModel\n",
    "from nistats.thresholding import map_threshold\n",
    "from nilearn import plotting\n",
    "from utils.firstlevel_utils import (get_first_level_objs, \n",
    "                                    get_first_level_maps, \n",
    "                                    load_first_level_objs, \n",
    "                                    FirstLevel)\n",
    "from utils.secondlevel_utils import create_group_mask, randomise\n",
    "from utils.utils import get_contrasts, get_flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments\n",
    "These are not needed for the jupyter notebook, but are used after conversion to a script for production\n",
    "\n",
    "- conversion command:\n",
    "  - jupyter nbconvert --to script --execute 2ndlevel_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='2nd level Entrypoint Script.')\n",
    "parser.add_argument('-derivatives_dir', default=None)\n",
    "parser.add_argument('--tasks', nargs=\"+\", help=\"Choose from ANT, CCTHot, discountFix, \\\n",
    "                                    DPX, motorSelectiveStop, stopSignal, \\\n",
    "                                    stroop, surveyMedley, twoByTwo, WATT3\")\n",
    "parser.add_argument('--rerun', action='store_true')\n",
    "parser.add_argument('--rt', action='store_true')\n",
    "parser.add_argument('--beta', action='store_true')\n",
    "parser.add_argument('--n_perms', default=1000, type=int)\n",
    "parser.add_argument('--quiet', '-q', action='store_true')\n",
    "\n",
    "if '-derivatives_dir' in sys.argv or '-h' in sys.argv:\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    args = parser.parse_args([])\n",
    "    args.derivatives_dir = '/data/derivatives/'\n",
    "    args.tasks = ['stroop']\n",
    "    args.rt=True\n",
    "    args.n_perms = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.quiet:\n",
    "    def verboseprint(*args, **kwargs):\n",
    "        print(*args, **kwargs)\n",
    "else:\n",
    "    verboseprint = lambda *a, **k: None # do-nothing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Organize paths and set parameters based on arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "first_level_dir = path.join(args.derivatives_dir, '1stlevel')\n",
    "second_level_dir = path.join(args.derivatives_dir,'2ndlevel')\n",
    "fmriprep_dir = path.join(args.derivatives_dir, 'fmriprep')\n",
    "\n",
    "# set tasks\n",
    "if args.tasks is not None:\n",
    "    tasks = args.tasks\n",
    "else:\n",
    "    tasks = ['ANT', 'CCTHot', 'discountFix',\n",
    "            'DPX', 'motorSelectiveStop',\n",
    "            'stopSignal', 'stroop',\n",
    "            'twoByTwo', 'WATT3']\n",
    "    \n",
    "# set other variables\n",
    "regress_rt = args.rt\n",
    "beta_series = args.beta\n",
    "n_perms = args.n_perms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_threshold = .95\n",
    "mask_loc = path.join(second_level_dir, 'group_mask_thresh-%s.nii.gz' % str(mask_threshold))\n",
    "if path.exists(mask_loc) == False or args.rerun:\n",
    "    verboseprint('Making group mask')\n",
    "    group_mask = create_group_mask(fmriprep_dir, mask_threshold)\n",
    "    makedirs(path.dirname(mask_loc), exist_ok=True)\n",
    "    group_mask.to_filename(mask_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create second level objects\n",
    "Gather first level models and create second level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_flag, beta_flag = get_flags(regress_rt, beta_series)\n",
    "for task in tasks:\n",
    "    verboseprint('Running 2nd level for %s' % task)\n",
    "    # load first level models\n",
    "    # create contrast maps\n",
    "    verboseprint('*** Creating maps')\n",
    "    task_contrasts = get_contrasts(task, regress_rt)\n",
    "    maps_dir = path.join(second_level_dir, task, 'secondlevel-%s_%s_maps' % (rt_flag, beta_flag))\n",
    "    makedirs(maps_dir, exist_ok=True)\n",
    "    # run through each contrast\n",
    "    for name, contrast in task_contrasts:\n",
    "        second_level_model = SecondLevelModel(mask=mask_loc, smoothing_fwhm=6)\n",
    "        maps = get_first_level_maps('*', task, first_level_dir, name, regress_rt, beta_series)\n",
    "        N = str(len(maps)).zfill(2)\n",
    "        verboseprint('****** %s, %s files found' % (name, N))\n",
    "        if len(maps) <= 1:\n",
    "            verboseprint('****** No Maps')\n",
    "            continue\n",
    "        design_matrix = pd.DataFrame([1] * len(maps), columns=['intercept'])\n",
    "        second_level_model.fit(maps, design_matrix=design_matrix)\n",
    "        contrast_map = second_level_model.compute_contrast()\n",
    "        # save\n",
    "        contrast_file = path.join(maps_dir, 'contrast-%s.nii.gz' % name)\n",
    "        contrast_map.to_filename(contrast_file)\n",
    "         # write metadata\n",
    "        with open(path.join(maps_dir, 'metadata.txt'), 'a') as f:\n",
    "            f.write('Contrast-%s: %s maps\\n' % (contrast, N))\n",
    "        # save corrected map\n",
    "        if n_perms > 0:\n",
    "            verboseprint('*** Running Randomise')\n",
    "            randomise(maps, maps_dir, mask_loc, n_perms=n_perms)\n",
    "            # write metadata\n",
    "            with open(path.join(maps_dir, 'metadata.txt'), 'a') as f:\n",
    "                f.write('Contrast-%s: Randomise run with %s permutations\\n' % (contrast, str(n_perms)))\n",
    "    verboseprint('Done with %s' % task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Using nistats method of first level objects. Not conducive for randomise.\n",
    "rt_flag, beta_flag = get_flags(regress_rt, beta_series)\n",
    "for task in tasks:\n",
    "    verboseprint('Running 2nd level for %s' % task)\n",
    "    # load first level models\n",
    "    first_levels = load_first_level_objs(task, first_level_dir, regress_rt=regress_rt)\n",
    "    if len(first_levels) == 0:\n",
    "        continue\n",
    "    first_level_models = [subj.fit_model for subj in first_levels]\n",
    "    N = str(len(first_level_models)).zfill(2)\n",
    "\n",
    "    # simple design for one sample test\n",
    "    design_matrix = pd.DataFrame([1] * len(first_level_models), columns=['intercept'])\n",
    "    \n",
    "    # run second level\n",
    "    verboseprint('*** Running model. %s first level files found' % N)\n",
    "    second_level_model = SecondLevelModel(mask=mask_loc, smoothing_fwhm=6).fit(\n",
    "        first_level_models, design_matrix=design_matrix)\n",
    "    makedirs(path.join(second_level_dir, task), exist_ok=True)\n",
    "    f = open(path.join(second_level_dir, task, 'secondlevel_%s_%s.pkl' % (rt_flag, beta_flag)), 'wb')\n",
    "    pickle.dump(second_level_model, f)\n",
    "    f.close()\n",
    "    \n",
    "    # create contrast maps\n",
    "    verboseprint('*** Creating maps')\n",
    "    task_contrasts = get_contrasts(task, regress_rt)\n",
    "    maps_dir = path.join(second_level_dir, task, 'secondlevel_%s_%s_N-%s_maps' % (rt_flag, beta_flag, N))\n",
    "    makedirs(maps_dir, exist_ok=True)\n",
    "    for name, contrast in task_contrasts:\n",
    "        verboseprint('****** %s' % name)\n",
    "        contrast_map = second_level_model.compute_contrast(first_level_contrast=contrast)\n",
    "        contrast_file = path.join(maps_dir, 'contrast-%s.nii.gz' % name)\n",
    "        contrast_map.to_filename(contrast_file)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
